{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqM-T1RTzY6C"
   },
   "source": [
    "This notebook uses the `Llama-3` format for conversation style finetunes. We use [Open Assistant conversations](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style) in ShareGPT style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2eSvM9zX_2d3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install langtorch\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2v_X2fA0Df5"
   },
   "source": [
    "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
    "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
    "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
    "* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.\n",
    "* [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_function(model, tokenizer, dataset_name='hellaswag', split='validation', language='pol_Latn', device='cuda', verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate a language model on a given dataset.\n",
    "    \n",
    "    Args:\n",
    "    - model: The language model to evaluate\n",
    "    - tokenizer: The tokenizer for the model\n",
    "    - dataset_name: 'hellaswag', 'belebele', or 'hellaswag_pl'\n",
    "    - split: The dataset split to use (for hellaswag)\n",
    "    - language: The language to use (for belebele)\n",
    "    - device: The device to run the model on\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: The normalized accuracy of the model on the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_dataset():\n",
    "        if dataset_name == 'hellaswag':\n",
    "            ds = load_dataset(\"Rowan/hellaswag\", split=split)\n",
    "            return ds.map(lambda x: {\"label\": int(x[\"label\"])})\n",
    "        elif dataset_name == 'belebele':\n",
    "            ds = load_dataset(\"facebook/belebele\", language)\n",
    "            # hellaswag format\n",
    "            return ds.map(lambda x: {\n",
    "                \"ctx\": x[\"question\"],\n",
    "                \"label\": int(x[\"correct_answer_num\"]) - 1,\n",
    "                \"endings\": [x[\"mc_answer1\"], x[\"mc_answer2\"], x[\"mc_answer3\"], x[\"mc_answer4\"]]\n",
    "            })[\"test\"]\n",
    "        elif dataset_name == 'hellaswag_pl':\n",
    "            ds = []\n",
    "            with open(\"../../translate_hellaswag/translations.jsonl\", \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    if \"{\\n\" in line:\n",
    "                        jsonl_line = line\n",
    "                    elif \"}\\n\" in line:\n",
    "                        jsonl_line += line\n",
    "                        jsonl_line = jsonl_line.replace(\"\\n\", \"\")\n",
    "                        ds.append(json.loads(jsonl_line))\n",
    "                    else:\n",
    "                        jsonl_line += line\n",
    "                        \n",
    "                for entry in ds:\n",
    "                    entry[\"label\"] = int(entry[\"label\"])\n",
    "            return ds\n",
    "        else:\n",
    "            raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    def render_example(example):\n",
    "        ctx = example[\"ctx\"]\n",
    "        label = example[\"label\"]\n",
    "        endings = example[\"endings\"]\n",
    "        if len(endings)>4:\n",
    "            return None, None, None\n",
    "\n",
    "        ctx_tokens = tokenizer.encode(ctx)\n",
    "        tok_rows = []\n",
    "        mask_rows = []\n",
    "        for end in endings:\n",
    "            end_tokens = tokenizer.encode(\" \" + end)\n",
    "            tok_rows.append(ctx_tokens + end_tokens)\n",
    "            mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))\n",
    "\n",
    "        max_len = max(len(row) for row in tok_rows)\n",
    "        tokens = torch.zeros((4, max_len), dtype=torch.long)\n",
    "        mask = torch.zeros((4, max_len), dtype=torch.long)\n",
    "        for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):\n",
    "            tokens[i, :len(tok_row)] = torch.tensor(tok_row)\n",
    "            mask[i, :len(mask_row)] = torch.tensor(mask_row)\n",
    "\n",
    "        return tokens, mask, label\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    ds = get_dataset()\n",
    "    num_correct_norm = 0\n",
    "    num_total = 0\n",
    "    with torch.no_grad():\n",
    "        for example in ds:\n",
    "            tokens, mask, label = render_example(example)\n",
    "            if tokens is None:\n",
    "                continue\n",
    "            tokens = tokens.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            logits = model(tokens).logits\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_tokens = tokens[..., 1:].contiguous()\n",
    "            shift_mask = mask[..., 1:].contiguous()\n",
    "\n",
    "            flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            flat_shift_tokens = shift_tokens.view(-1)\n",
    "\n",
    "            shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
    "            shift_losses = shift_losses.view(tokens.size(0), -1)\n",
    "\n",
    "            masked_shift_losses = shift_losses * shift_mask\n",
    "            avg_loss = masked_shift_losses.sum(dim=1) / shift_mask.sum(dim=1)\n",
    "            pred_norm = avg_loss.argmin().item()\n",
    "\n",
    "            num_total += 1\n",
    "            num_correct_norm += int(pred_norm == label)\n",
    "\n",
    "            if verbose and num_total % 100 == 0:\n",
    "                print(f\"Processed {num_total} examples. Current accuracy: {num_correct_norm/num_total:.4f}\")\n",
    "\n",
    "    accuracy = num_correct_norm / num_total\n",
    "    print(f\"Final accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# print(eval_function(model, tokenizer, \"belebele\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "b2852520-aef7-467a-deb3-1ddf5c76c4e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "GPU = NVIDIA H100 80GB HBM3. Max memory = 79.216 GB.\n",
      "0.0 GB of memory reserved.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.216 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 9.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0295580ab0e147c681fa80f428f556e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "chekpoint_path = \"unsloth/Meta-Llama-3.1-8B\"#-instruct\" #\"outputs/checkpoint-7000\" \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "  model_name = chekpoint_path, \n",
    "  max_seq_length = max_seq_length,\n",
    "  dtype = dtype,\n",
    "  load_in_4bit = load_in_4bit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.216 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 9.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8ebbc968a2435a910320b51bd3572e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.2844\n",
      "Alpha: 0.5625, Accuracy: 0.2844\n",
      "Final accuracy: 0.2856\n",
      "Alpha: 0.8438, Accuracy: 0.2856\n",
      "Final accuracy: 0.2833\n",
      "Alpha: 0.9844, Accuracy: 0.2833\n",
      "Final accuracy: 0.2822\n",
      "Alpha: 0.9141, Accuracy: 0.2822\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m adapter_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASobieszek/l3.1-wiki-15k-128-wd0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 127\u001b[0m optimal_alpha, best_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_search_optimal_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbelebele\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc_0_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2689\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.2833\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal alpha: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_alpha\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Best accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 93\u001b[0m, in \u001b[0;36mbinary_search_optimal_alpha\u001b[0;34m(model_base, adapter_name, eval_function, dataset_name, split, language, device, tolerance, acc_0_2)\u001b[0m\n\u001b[1;32m     90\u001b[0m scale_adapters(model, (left \u001b[38;5;241m+\u001b[39m right) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, mid)\n\u001b[1;32m     91\u001b[0m mid \u001b[38;5;241m=\u001b[39m (left \u001b[38;5;241m+\u001b[39m right) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 93\u001b[0m accuracy_mid \u001b[38;5;241m=\u001b[39m \u001b[43meval_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlpha: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmid\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_mid\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy_mid \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m, in \u001b[0;36meval_function\u001b[0;34m(model, tokenizer, dataset_name, split, language, device, verbose)\u001b[0m\n\u001b[1;32m     90\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     91\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 93\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     94\u001b[0m shift_logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     95\u001b[0m shift_tokens \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:964\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    953\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    963\u001b[0m ):\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:188\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:883\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 883\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    897\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:750\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 750\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:467\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    466\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 467\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:354\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     cos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb\u001b[38;5;241m.\u001b[39mcos_cached\n\u001b[1;32m    353\u001b[0m     sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb\u001b[38;5;241m.\u001b[39msin_cached\n\u001b[0;32m--> 354\u001b[0m     Q, K \u001b[38;5;241m=\u001b[39m \u001b[43mfast_rope_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(V, seq_len \u001b[38;5;241m=\u001b[39m kv_seq_len)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/kernels/rope_embedding.py:135\u001b[0m, in \u001b[0;36mfast_rope_embedding\u001b[0;34m(Q, K, cos, sin)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfast_rope_embedding\u001b[39m(Q, K, cos, sin):\n\u001b[0;32m--> 135\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[43mFast_RoPE_Embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    136\u001b[0m     K \u001b[38;5;241m=\u001b[39m Fast_RoPE_Embedding\u001b[38;5;241m.\u001b[39mapply(K\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), cos, sin)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Q, K\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/kernels/rope_embedding.py:91\u001b[0m, in \u001b[0;36mFast_RoPE_Embedding.forward\u001b[0;34m(ctx, Q, cos, sin)\u001b[0m\n\u001b[1;32m     88\u001b[0m div, mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(n_heads, ROPE_GROUP_SIZE)\n\u001b[1;32m     89\u001b[0m n_groups \u001b[38;5;241m=\u001b[39m div \u001b[38;5;241m+\u001b[39m (mod \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[43m_rope_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m      \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBACKWARD_PASS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mBLOCK_SIZE \u001b[38;5;241m=\u001b[39m BLOCK_SIZE\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mnum_warps  \u001b[38;5;241m=\u001b[39m num_warps\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/runtime/jit.py:167\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/runtime/autotuner.py:305\u001b[0m, in \u001b[0;36mHeuristics.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, heur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    304\u001b[0m     kwargs[v] \u001b[38;5;241m=\u001b[39m heur({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marg_names, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs})\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/runtime/jit.py:424\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key]\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warmup:\n\u001b[0;32m--> 424\u001b[0m     args \u001b[38;5;241m=\u001b[39m [arg\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mis_constexpr]\n\u001b[1;32m    425\u001b[0m     kernel\u001b[38;5;241m.\u001b[39mrun(grid_0, grid_1, grid_2, kernel\u001b[38;5;241m.\u001b[39mnum_warps, kernel\u001b[38;5;241m.\u001b[39mnum_ctas,  \u001b[38;5;66;03m# number of warps/ctas per instance\u001b[39;00m\n\u001b[1;32m    426\u001b[0m                kernel\u001b[38;5;241m.\u001b[39mcluster_dims[\u001b[38;5;241m0\u001b[39m], kernel\u001b[38;5;241m.\u001b[39mcluster_dims[\u001b[38;5;241m1\u001b[39m], kernel\u001b[38;5;241m.\u001b[39mcluster_dims[\u001b[38;5;241m2\u001b[39m],  \u001b[38;5;66;03m# cluster\u001b[39;00m\n\u001b[1;32m    427\u001b[0m                kernel\u001b[38;5;241m.\u001b[39mshared, stream, kernel\u001b[38;5;241m.\u001b[39mfunction, CompiledKernel\u001b[38;5;241m.\u001b[39mlaunch_enter_hook,\n\u001b[1;32m    428\u001b[0m                CompiledKernel\u001b[38;5;241m.\u001b[39mlaunch_exit_hook, kernel,\n\u001b[1;32m    429\u001b[0m                \u001b[38;5;241m*\u001b[39mdriver\u001b[38;5;241m.\u001b[39massemble_tensormap_to_arg(kernel\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensormaps_info\u001b[39m\u001b[38;5;124m\"\u001b[39m], args))\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kernel\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/triton/runtime/jit.py:424\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    422\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key]\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warmup:\n\u001b[0;32m--> 424\u001b[0m     args \u001b[38;5;241m=\u001b[39m [arg\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mis_constexpr]\n\u001b[1;32m    425\u001b[0m     kernel\u001b[38;5;241m.\u001b[39mrun(grid_0, grid_1, grid_2, kernel\u001b[38;5;241m.\u001b[39mnum_warps, kernel\u001b[38;5;241m.\u001b[39mnum_ctas,  \u001b[38;5;66;03m# number of warps/ctas per instance\u001b[39;00m\n\u001b[1;32m    426\u001b[0m                kernel\u001b[38;5;241m.\u001b[39mcluster_dims[\u001b[38;5;241m0\u001b[39m], kernel\u001b[38;5;241m.\u001b[39mcluster_dims[\u001b[38;5;241m1\u001b[39m], kernel\u001b[38;5;241m.\u001b[39mcluster_dims[\u001b[38;5;241m2\u001b[39m],  \u001b[38;5;66;03m# cluster\u001b[39;00m\n\u001b[1;32m    427\u001b[0m                kernel\u001b[38;5;241m.\u001b[39mshared, stream, kernel\u001b[38;5;241m.\u001b[39mfunction, CompiledKernel\u001b[38;5;241m.\u001b[39mlaunch_enter_hook,\n\u001b[1;32m    428\u001b[0m                CompiledKernel\u001b[38;5;241m.\u001b[39mlaunch_exit_hook, kernel,\n\u001b[1;32m    429\u001b[0m                \u001b[38;5;241m*\u001b[39mdriver\u001b[38;5;241m.\u001b[39massemble_tensormap_to_arg(kernel\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensormaps_info\u001b[39m\u001b[38;5;124m\"\u001b[39m], args))\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kernel\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def scale_adapters(model, scale, previous_scale):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.mul_(scale/previous_scale)\n",
    "\n",
    "def add_scaled_adapters(model_base, adapter_list):\n",
    "    \"\"\"\n",
    "    Load a model with multiple LoRA adapters.\n",
    "    \n",
    "    Args:\n",
    "    - model_name (str): Name of the base model\n",
    "    - adapter_list (list): List of tuples (adapter_name, alpha) or a single tuple\n",
    "    \n",
    "    Returns:\n",
    "    - model: The loaded model with adapters applied\n",
    "    - tokenizer: The tokenizer for the model\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(adapter_list, tuple):\n",
    "        adapter_list = [adapter_list]\n",
    "\n",
    "    model = model_base\n",
    "    for i, (adapter_name, alpha) in enumerate(adapter_list):\n",
    "        model = PeftModel.from_pretrained(model, adapter_name)\n",
    "        \n",
    "    adjusted_alpha = alpha / len(adapter_list)\n",
    "    scale_adapters(model, adjusted_alpha, 1)\n",
    "    return model\n",
    "\n",
    "def binary_search_optimal_alpha(model_base, adapter_name, eval_function, dataset_name='hellaswag', split='validation', language='pol_Latn', device='cuda', tolerance=1e-2, acc_0_2 = None):\n",
    "    \"\"\"\n",
    "    Find the optimal alpha scaling factor for a single adapter using binary search.\n",
    "    \n",
    "    Args:\n",
    "    - model_base (str or CausalLM): The base model or its name\n",
    "    - adapter_name (str): Name of the LoRA adapter\n",
    "    - eval_function (function): Evaluation function that returns accuracy\n",
    "    - dataset_name (str): Name of the dataset to use for evaluation\n",
    "    - split (str): Dataset split to use\n",
    "    - language (str): Language for Belebele dataset\n",
    "    - device (str): Device to run the model on\n",
    "    - tolerance (float): Tolerance for binary search convergence\n",
    "    \n",
    "    Returns:\n",
    "    - optimal_alpha (float): The optimal alpha scaling factor\n",
    "    - best_accuracy (float): The best accuracy achieved\n",
    "    \"\"\"\n",
    "    global tokenizer\n",
    "    if isinstance(model_base, str):\n",
    "        model_base, tokenizer = FastLanguageModel.from_pretrained(\n",
    "          model_name = model_base, \n",
    "          max_seq_length = max_seq_length,\n",
    "          dtype = dtype,\n",
    "          load_in_4bit = load_in_4bit\n",
    "        )\n",
    "    \n",
    "    left, right = 0.0, 1.125\n",
    "    best_alpha = None\n",
    "    best_accuracy = float('-inf')\n",
    "\n",
    "    if acc_0_2 is None:\n",
    "        # Calculate accuracy for alpha = 0 (no adapter)\n",
    "        accuracy_left = eval_function(model_base, tokenizer, dataset_name, split, language, device)\n",
    "        print(f\"Alpha: 0.0000, Accuracy: {accuracy_left:.4f}\")\n",
    "        \n",
    "        # Calculate accuracy for alpha = 2\n",
    "        mid = right\n",
    "        model = add_scaled_adapters(model_base, (adapter_name, right))\n",
    "        accuracy_right = eval_function(model, tokenizer, dataset_name, split, language, device)\n",
    "        print(f\"Alpha: 2.0000, Accuracy: {accuracy_right:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        mid = 1.0\n",
    "        model = add_scaled_adapters(model_base, (adapter_name, mid))\n",
    "        accuracy_left, accuracy_right = acc_0_2\n",
    "\n",
    "    \n",
    "    if accuracy_left > best_accuracy:\n",
    "        best_accuracy = accuracy_left\n",
    "        best_alpha = 0.0\n",
    "    if accuracy_right > best_accuracy:\n",
    "        best_accuracy = accuracy_right\n",
    "        best_alpha = 2.0\n",
    "\n",
    "    while right - left > tolerance:\n",
    "        scale_adapters(model, (left + right) / 2, mid)\n",
    "        mid = (left + right) / 2\n",
    "        \n",
    "        accuracy_mid = eval_function(model, tokenizer, dataset_name, split, language, device)\n",
    "        \n",
    "        print(f\"Alpha: {mid:.4f}, Accuracy: {accuracy_mid:.4f}\")\n",
    "        \n",
    "        if accuracy_mid > best_accuracy:\n",
    "            best_accuracy = accuracy_mid\n",
    "            best_alpha = mid\n",
    "\n",
    "        # Decide which half to continue searching\n",
    "        if accuracy_left < accuracy_mid > accuracy_right:\n",
    "            # Peak is between left and right\n",
    "            if mid - left < right - mid:\n",
    "                right = mid\n",
    "                accuracy_right = accuracy_mid\n",
    "            else:\n",
    "                left = mid\n",
    "                accuracy_left = accuracy_mid\n",
    "        elif accuracy_left > accuracy_mid:\n",
    "            # Peak is on the left side\n",
    "            right = mid\n",
    "            accuracy_right = accuracy_mid\n",
    "        else:\n",
    "            # Peak is on the right side\n",
    "            left = mid\n",
    "            accuracy_left = accuracy_mid\n",
    "\n",
    "    return best_alpha, best_accuracy\n",
    "\n",
    "    \n",
    "    return best_alpha, best_accuracy\n",
    "\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
    "adapter_name = \"ASobieszek/l3.1-wiki-15k-128-wd0.5\"\n",
    "\n",
    "optimal_alpha, best_accuracy = binary_search_optimal_alpha(model_name, adapter_name, eval_function, \"belebele\", acc_0_2 = (0.2689,0.2833))\n",
    "print(f\"Optimal alpha: {optimal_alpha:.4f}, Best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# optimal_model, optimal_tokenizer = load_model_with_adapters(model_name, (adapter_name, optimal_alpha))\n",
    "\n",
    "# final_accuracy = eval_function(optimal_model, optimal_tokenizer, \"belebele\")\n",
    "# print(f\"Final accuracy with optimal alpha: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "GPU = NVIDIA H100 80GB HBM3. Max memory = 79.216 GB.\n",
      "0.0 GB of memory reserved.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.216 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 9.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f09ad0f6aa54c85bda08aa001cc0a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "chekpoint_path = \"unsloth/Meta-Llama-3.1-8B\"#-instruct\" #\"outputs/checkpoint-7000\" \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "  model_name = chekpoint_path, \n",
    "  max_seq_length = max_seq_length,\n",
    "  dtype = dtype,\n",
    "  load_in_4bit = load_in_4bit\n",
    ")\n",
    "\n",
    "# # Call the eval function\n",
    "# print(eval_function(model_base, tokenizer, \"belebele\"))\n",
    "\n",
    "# for adapter_name in [\"ASobieszek/l3.1-wiki-15k-128-wd0.5\", \"ASobieszek/l3.1-wiki-20k-256-wd0.5\", \"ASobieszek/l3.1-wiki-15k-128-wd0.01\"]:\n",
    "#     model = PeftModel.from_pretrained(model_base, adapter_name)\n",
    "#     print(adapter_name.split(\"wiki-\")[1])\n",
    "#     print(eval_function(model, tokenizer, \"belebele\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langtorch import TextModule, TextTensor, ChatTensor\n",
    "from langtorch.tt import Activation\n",
    "from langtorch import Text, Chat\n",
    "from langtorch import Text\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, TrainingArguments, IntervalStrategy\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data():\n",
    "    class LLamaChat(Chat):\n",
    "        def __str__(self):\n",
    "            formatted = \"\"\n",
    "            for role, content in self.items():\n",
    "                formatted += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{Text(content,parse=False)}<|eot_id|>\"\n",
    "            if role == \"user\": # Add assistant prompt for generation\n",
    "                formatted += \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "            return formatted\n",
    "\n",
    "    interview = pd.read_csv(r\"interviews.csv\")\n",
    "    interview[\"doctor_diagnosis\"] = interview[\"doctor_diagnosis\"].apply(lambda x: x.split('code\":\"')[1].split('\"')[0])\n",
    "    interview_tensor = TextTensor(dict((k,interview[k].to_list()) for k in [\"doctor_interview\",\"doctor_diagnosis\"]),parse = False)\n",
    "\n",
    "    answers = TextTensor((\n",
    "        (\"system\", \"JesteÅ› lekarzem, ktÃ³ry stawia do danego wywiadu medycznego diagnozÄ™.\"),\n",
    "        (\"user\",\"doctor_interview\"),\n",
    "        (\"assistant\",\"doctor_diagnosis\"))\n",
    "    )*interview_tensor\n",
    "\n",
    "    answers.ttype = LLamaChat\n",
    "    print(answers.item())\n",
    "    def first_n_words(s, n):\n",
    "        words = s.split()\n",
    "        first_n_words = words[:n]\n",
    "        return ' '.join(first_n_words)\n",
    "\n",
    "\n",
    "    return [str(m) for m in answers.flat]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "46e02a86-6eda-40a6-dff7-bc0ee1190163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 945258 rows (59.54% of the original dataset)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from functools import partial\n",
    "import re\n",
    "\n",
    "def process_wikipedia_dataset(prompt, dataset_name=\"JonaszPotoniec/wikipedia-with-statistics-pl\"):\n",
    "    \"\"\"\n",
    "    Load, filter, and format a Wikipedia dataset using HuggingFace's Dataset class optimizations.\n",
    "    \n",
    "    Args:\n",
    "    - prompt: To format entries\n",
    "    - dataset_name: The name of the dataset to load from HuggingFace\n",
    "\n",
    "    Returns:\n",
    "    - A processed dataset with formatted prompts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define banned strings for filtering\n",
    "    banned_strings = [\n",
    "        \"â€“ wieÅ›\", \"â€“ gmina\", \"â€“ miasto\", \"â€“ jednostka administracyjna\", \"â€“ stacja\", \"â€“ jaskinia\", \"â€“ park\", \"â€“ powiat\", \"â€“ chutor\", \"miejscowoÅ›Ä‡\", \"planetoida\", \"â€“ osada\", \"â€“ supernowa\", \"â€“ galaktyka\", \"â€“ gwiazda\", \"â€“ rzeka\",  \"â€“ osiedle\",  \"â€“ kolonia\",  \"â€“ port\", \"â€“ gaun\", \"â€“ przystanek\", \"â€“ jezioro\", \"â€“ okrÄ™g\", \"â€“ dystrykt\", \"â€“ droga\", \"â€“ ulica\", \"â€“ przysiÃ³Å‚ek\", \"â€“ wyspa\", \"â€“ pomnik\", \"â€“ wzniesienie\", \"â€“ rezerwat\", \"- prowincja\", \"- rejon\", \"- zatoka\", \"hrabstw\", \"- region\", \"- szczyt\", \"- potok\", \n",
    "        \"â€“ gitara\", \"â€“ perkusja\", \"â€“ piosenka\",  \"â€“ singel\", \"- budynek\",\n",
    "        \"â€“ parafia\",  \"â€“ rzymskokatolick\", \"â€“ diecezja\", \"â€“ duchown\", \"â€“ koÅ›ciÃ³Å‚\", \"â€“ biskup\", \"â€“ prawosÅ‚awn\", \"â€“ synagoga\", \"â€“ droga\", \n",
    "        \"â€“ dawn\", \"â€“ zabytkowy\", \"â€“ herb\", \"â€“ oficerowie\", \"â€“ generaÅ‚\", \"- podpuÅ‚kownik\", \"- kasztelan\", \"- sÄ™dzia\", \"- pododdzia\", \"Å¼oÅ‚nierz\", \"- hrabia\", \"- funkcjonariusz\",\n",
    "        \"w Rumunii\", \"struga\",\n",
    "        \"w reÅ¼yserii\", \"debiutancki\", \n",
    "        \"- turniej\", \"- zawody\", \"wioÅ›lar\", \"Å‚yÅ¼wiar\", \"tenis\", \"piÅ‚kar\", \"piÅ‚ce\", \"klub piÅ‚karski\", \"aktor\", \"piosenkar\"\n",
    "    ]\n",
    "    \n",
    "    def format_and_filter1(example):\n",
    "        \"\"\"\n",
    "        Format a single example and filter based on banned strings.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if any banned string is in the text\n",
    "            if not example['text'] or any(s in example['text'] for s in banned_strings):\n",
    "                return {'text': ''}  # This will filter out the example\n",
    "            if example['pageviews'] < 10:\n",
    "                return {'text': 'obscure'}\n",
    "            \n",
    "            def format_text(t):\n",
    "                for end in [\"\\n\\nUwagi\",\"\\n\\nPrzypisy\",\"\\n\\nBibliografia\", \"\\n\\nLinki zewnÄ™trzne\"]:\n",
    "                    t = t.split(end)[0]\n",
    "                return t  \n",
    "            formatted_text = format_text(example['text'])\n",
    "            return example | {'text': formatted_text}\n",
    "        except:\n",
    "            return {'text': ''}\n",
    "\n",
    "    def split_long_entries(examples, max_len = 7_000):\n",
    "        \"\"\"\n",
    "        Split long entries into multiple examples.\n",
    "        \"\"\"\n",
    "        new_examples = []\n",
    "        for example in examples:\n",
    "            if len(example['text']) <= max_len:\n",
    "                new_examples.append({'text': prompt.format(example['title'], example['text'].strip()) + EOS_TOKEN})\n",
    "            else:\n",
    "                text = example['text']\n",
    "                while text:\n",
    "                    # Find the last paragraph break within the first 10k characters\n",
    "                    split_index = -1\n",
    "                    if len(text)<max_len+1000:\n",
    "                        split_index = len(text)\n",
    "                    else:\n",
    "                        while split_index == -1: # tweak this\n",
    "                            split_index = text[:max_len].rfind('\\n\\n')\n",
    "                            if split_index == -1:  # If no paragraph break, just take the first max_len\n",
    "                                split_index = max_len\n",
    "                    \n",
    "                    chunk = {'text': prompt.format(example['title'], text[:split_index].strip()) + EOS_TOKEN}\n",
    "                    new_examples.append(chunk)\n",
    "                    \n",
    "                    text = text[split_index:].strip()\n",
    "                    if len(text)<500:\n",
    "                        break\n",
    "        \n",
    "        return new_examples\n",
    "\n",
    "    def format_and_filter2(example):\n",
    "        try:\n",
    "            formatted_text = prompt.format(example['title'], example['text']) + EOS_TOKEN\n",
    "            return {'text': formatted_text}\n",
    "        except:\n",
    "            return {'text': ''}\n",
    "    \n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(dataset_name)['train']\n",
    "    orig_len = len(dataset)\n",
    "\n",
    "    # Apply initial formatting and filtering\n",
    "    formatted_dataset = dataset.map(\n",
    "        partial(format_and_filter1),\n",
    "        num_proc=32  # Use multiple processes for speedup\n",
    "    )\n",
    "    \n",
    "    # Filter out None values and obscure articles\n",
    "    formatted_dataset = formatted_dataset.filter(lambda x: x['text'] != '' and x['text'] != 'obscure')\n",
    "    diff = orig_len - len(formatted_dataset)\n",
    "    print(f\"Filtered out {diff} rows ({diff/orig_len*100:.2f}% of the original dataset)\")\n",
    "    \n",
    "    # Split long entries\n",
    "    all_examples = formatted_dataset.to_list()\n",
    "    split_examples = split_long_entries(all_examples)\n",
    "    \n",
    "    print(f\"Increased the dataset size by {(len(split_examples)/len(all_examples)-1)*100:.2f}% via splitting\")\n",
    "    formatted_dataset = Dataset.from_list(split_examples)\n",
    "        \n",
    "    print(formatted_dataset)\n",
    "    \n",
    "    # Final formatting\n",
    "    # formatted_dataset = formatted_dataset.map(\n",
    "    #     partial(format_and_filter2),\n",
    "    #     remove_columns=formatted_dataset.column_names,\n",
    "    #     num_proc=32\n",
    "    # )\n",
    "    \n",
    "    # Filter out short entries\n",
    "    formatted_dataset = formatted_dataset.filter(lambda x: len(x['text']) > 250)\n",
    "    final_len = len(formatted_dataset)\n",
    "    print(f\"Final dataset has {final_len} entries ({final_len/orig_len*100:.2f}% of the original dataset)\")\n",
    "    \n",
    "    return formatted_dataset\n",
    "\n",
    "# Usage\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "wikipedia_prompt = \"{}\\n\\n{}\"\n",
    "dataset = process_wikipedia_dataset(wikipedia_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### UAProf\n",
      "User Agent Profile (UAprof) to definicja opisu moÅ¼liwoÅ›ci telefonu komÃ³rkowego, utworzona przez organizacjÄ™ WAP Forum w ramach specyfikacji WAP 2.0, obecnie rozszerzana przez Open Mobile Alliance.\n",
      "\n",
      "Powodem powstania definicji UAprof byÅ‚a ciÄ…gle wzrastajÄ…ca iloÅ›Ä‡ wspomaganych formatÃ³w i serwisÃ³w, poprzez co pole \"Accept\" w nagÅ‚Ã³wku HTTP byÅ‚o coraz dÅ‚uÅ¼sze. DziÄ™ki UAProf nagÅ‚Ã³wek ten musi zawieraÄ‡ tylko jeden URL, a kaÅ¼dy zainteresowany serwer moÅ¼e Å›ciÄ…gnÄ…Ä‡ plik XML z opisem wszystkich moÅ¼liwoÅ›ci telefonu i zainstalowanych na nim programÃ³w. Pole \"Accept\" moÅ¼e byÄ‡ dziÄ™ki temu ograniczone do najwaÅ¼niejszych formatÃ³w.\n",
      "\n",
      "PrzykÅ‚ad \n",
      "(Nokia N73)\n",
      "\n",
      "HTTP-Header (wycinek):\n",
      "<nowiki>\n",
      "Accept: text/javascript, text/ecmascript, application/x-javascript, text/html, application/vnd.wap.xhtml+xml, application/xhtml+xml, text/css, multipart/mixed, text/vnd.wap.wml, application/vnd.wap.wmlc, application/vnd.wap.wmlscriptc, application/java-archive, application/java, application/x-java-archive, text/vnd.sun.j2me.app-descriptor, application/vnd.oma.drm.message, application/vnd.oma.drm.content, application/vnd.wap.mms-message, application/vnd.wap.sic, text/x-co-desc, application/vnd.oma.dd+xml, */*,text/x-hdml,image/mng,image/x-mng,video/mng,video/x-mng,image/bmp,text/html,text/vnd.wap.wmlscript,text/vnd.wap.wml\n",
      "Accept-Charset: iso-8859-1, utf-8, iso-10646-ucs-2; q=0.6,*\n",
      "Accept-Encoding: deflate, gzip\n",
      "Accept-Language: de;q=1.0, en;q=0.5, fr;q=0.5, tr;q=0.5, it;q=0.5, nl;q=0.5\n",
      "Host: de.wikipedia.org/wiki/UAprof\n",
      "User-Agent: NokiaN73-1/2.0626.0.0.2 S60/3.0 Profile/MIDP-2.0 Configuration/CLDC-1.1 UP.Link/6.3.0.0.0\n",
      "X-Wap-Profile: \"http://nds.nokia.com/uaprof/NN73-1r100.xml\"\n",
      "</nowiki>\n",
      "Na marginesie: niektÃ³rzy mylÄ… UAProf z ciÄ…giem User-Agent (przedostatnia linijka). Ten ostatni zdradza jednak tylko nazwÄ™ telefonu, a nie jego moÅ¼liwoÅ›ci.\n",
      "\n",
      "W ostatniej linijce widoczny jest URL, pod ktÃ³rym znajduje siÄ™ plik definicji UAProf. W aktualnych telefonach plik UAprof wyglÄ…da mniej wiÄ™cej tak, jak poniÅ¼ej:\n",
      "\n",
      "<nowiki>\n",
      "<?xml version=\"1.0\"?>\n",
      "\n",
      "<!--              przykÅ‚ad dla wikipedii                   -->\n",
      "\n",
      "<!--       ten telefon w rzeczywistoÅ›ci nie istnieje       -->\n",
      "\n",
      "<rdf:RDF xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n",
      "xmlns:prf=\"http://www.openmobilealliance.org/tech/profiles/UAPROF/ccppschema-20021212#\"\n",
      "xmlns:mms=\"http://www.openmobilealliance.org/tech/profiles/MMS/ccppschema-20050301-MMS1.2#\">\n",
      "   <rdf:Description rdf:ID=\"Profile\">\n",
      "      <prf:component>\n",
      "         <rdf:Description rdf:ID=\"HardwarePlatform\">\n",
      "            <rdf:type rdf:resource=\"http://www.openmobilealliance.org/tech/profiles/UAPROF/ccppschema-20021212#HardwarePlatform\"/>\n",
      "            <prf:BluetoothProfile>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>Headset Profile</rdf:li>\n",
      "                  <rdf:li>Handsfree Profile</rdf:li>\n",
      "                  <rdf:li>Object Push Profile</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:BluetoothProfile>\n",
      "            <prf:BitsPerPixel>16</prf:BitsPerPixel>\n",
      "            <prf:ColorCapable>Yes</prf:ColorCapable>\n",
      "            <prf:CPU>Wacek</prf:CPU>\n",
      "            <prf:ImageCapable>Yes</prf:ImageCapable>\n",
      "            <prf:InputCharSet>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>ISO-8859-1</rdf:li>\n",
      "                  <rdf:li>UTF-8</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:InputCharSet>\n",
      "            <prf:Keyboard>PhoneKeyPad</prf:Keyboard>\n",
      "            <prf:Model>WikiPhone-1</prf:Model>\n",
      "            <prf:NumberOfSoftKeys>2</prf:NumberOfSoftKeys>\n",
      "            <prf:OutputCharSet>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>ISO-8859-1</rdf:li>\n",
      "                  <rdf:li>UTF-8</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:OutputCharSet>\n",
      "            <prf:PixelAspectRatio>1x1</prf:PixelAspectRatio>\n",
      "            <prf:PointingResolution>Pixel</prf:PointingResolution>\n",
      "            <prf:ScreenSize>640x480</prf:ScreenSize>\n",
      "            <prf:ScreenSizeChar>80x24</prf:ScreenSizeChar>\n",
      "            <prf:StandardFontProportional>Yes</prf:StandardFontProportional>\n",
      "            <prf:SoundOutputCapable>Yes</prf:SoundOutputCapable>\n",
      "            <prf:TextInputCapable>Yes</prf:TextInputCapable>\n",
      "            <prf:Vendor>Fantasy</prf:Vendor>\n",
      "            <prf:VoiceInputCapable>Yes</prf:VoiceInputCapable>\n",
      "         </rdf:Description>\n",
      "      </prf:component>\n",
      "      <prf:component>\n",
      "         <rdf:Description rdf:ID=\"SoftwarePlatform\">\n",
      "            <rdf:type rdf:resource=\"http://www.openmobilealliance.org/tech/profiles/UAPROF/ccppschema-20021212#SoftwarePlatform\"/>\n",
      "            <prf:AcceptDownloadableSoftware>Yes</prf:AcceptDownloadableSoftware>\n",
      "            <prf:CcppAccept>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>application/java</rdf:li>\n",
      "                  <rdf:li>application/vnd.wap.mms-message</rdf:li>\n",
      "                  <rdf:li>application/vnd.wap.wbxml</rdf:li>\n",
      "                  <rdf:li>application/vnd.wap.wmlc</rdf:li>\n",
      "                  <rdf:li>application/vnd.wap.wmlscriptc</rdf:li>\n",
      "                  <rdf:li>application/vnd.wap.xhtml+xml</rdf:li>\n",
      "                  <rdf:li>application/xhtml+xml</rdf:li>\n",
      "                  <rdf:li>audio/midi</rdf:li>\n",
      "                  <rdf:li>image/gif</rdf:li>\n",
      "                  <rdf:li>image/jpeg</rdf:li>\n",
      "                  <rdf:li>image/jpg</rdf:li>\n",
      "                  <rdf:li>image/vnd.wap.wbmp</rdf:li>\n",
      "                  <rdf:li>multipart/mixed</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:CcppAccept>\n",
      "            <prf:CcppAccept-Charset>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>ISO-8859-1</rdf:li>\n",
      "                  <rdf:li>UTF-8</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:CcppAccept-Charset>\n",
      "            <prf:CcppAccept-Encoding>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>base64</rdf:li>\n",
      "                  <rdf:li>quoted-printable</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:CcppAccept-Encoding>\n",
      "         </rdf:Description>\n",
      "      </prf:component>\n",
      "      <prf:component>\n",
      "         <rdf:Description rdf:ID=\"NetworkCharacteristics\">\n",
      "            <rdf:type rdf:resource=\"http://www.openmobilealliance.org/tech/profiles/UAPROF/ccppschema-20021212#NetworkCharacteristics\"/>\n",
      "            <prf:SupportedBluetoothVersion>1.2</prf:SupportedBluetoothVersion>\n",
      "            <prf:CurrentBearerService>TwoWayPacket</prf:CurrentBearerService>\n",
      "            <prf:SecuritySupport>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>SSL</rdf:li>\n",
      "                  <rdf:li>TLS</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:SecuritySupport>\n",
      "            <prf:SupportedBearers>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>GPRS</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:SupportedBearers>\n",
      "         </rdf:Description>\n",
      "      </prf:component>\n",
      "      <prf:component>\n",
      "         <rdf:Description rdf:ID=\"BrowserUA\">\n",
      "            <rdf:type rdf:resource=\"http://www.openmobilealliance.org/tech/profiles/UAPROF/ccppschema-20021212#BrowserUA\"/>\n",
      "            <prf:BrowserName>WikiBrowser</prf:BrowserName>\n",
      "            <prf:BrowserVersion>1.0</prf:BrowserVersion>\n",
      "            <prf:FramesCapable>Yes</prf:FramesCapable>\n",
      "            <prf:HtmlVersion>4.1</prf:HtmlVersion>\n",
      "            <prf:TablesCapable>Yes</prf:TablesCapable>\n",
      "            <prf:XhtmlVersion>2.0</prf:XhtmlVersion>\n",
      "            <prf:XhtmlModules>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>XHTML1-struct</rdf:li>\n",
      "                  <rdf:li>xhtml-basic10</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:XhtmlModules>\n",
      "         </rdf:Description>\n",
      "      </prf:component>\n",
      "      <prf:component>\n",
      "         <rdf:Description rdf:ID=\"WapCharacteristics\">\n",
      "            <rdf:type rdf:resource=\"http://www.openmobilealliance.org/tech/profiles/UAPROF/ccppschema-20021212#WapCharacteristics\"/>\n",
      "            <prf:WapDeviceClass>C</prf:WapDeviceClass>\n",
      "            <prf:WapVersion>2.0</prf:WapVersion>\n",
      "            <prf:WmlDeckSize>65535</prf:WmlDeckSize>\n",
      "            <prf:WmlScriptLibraries>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>Lang</rdf:li>\n",
      "                  <rdf:li>Float</rdf:li>\n",
      "                  <rdf:li>String</rdf:li>\n",
      "                  <rdf:li>URL</rdf:li>\n",
      "                  <rdf:li>WMLBrowser</rdf:li>\n",
      "                  <rdf:li>Dialogs</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:WmlScriptLibraries>\n",
      "            <prf:WmlScriptVersion>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>1.2</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:WmlScriptVersion>\n",
      "            <prf:WmlVersion>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>1.3</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:WmlVersion>\n",
      "            <prf:WtaiLibraries>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>WTA.Public.makeCall</rdf:li>\n",
      "                  <rdf:li>WTA.Public.sendDTMF</rdf:li>\n",
      "                  <rdf:li>WTA.Public.addPBEntry</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:WtaiLibraries>\n",
      "            <prf:WtaVersion>1.1</prf:WtaVersion>\n",
      "            <prf:DrmClass>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>ForwardLock</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:DrmClass>\n",
      "            <prf:OmaDownload>Yes</prf:OmaDownload>\n",
      "         </rdf:Description>\n",
      "      </prf:component>\n",
      "      <prf:component>\n",
      "         <rdf:Description rdf:ID=\"PushCharacteristics\">\n",
      "            <rdf:type rdf:resource=\"http://www.openmobilealliance.org/tech/profiles/UAPROF/ccppschema-20021212#PushCharacteristics\"/>\n",
      "            <prf:Push-Accept>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>application/vnd.wap.wmlc</rdf:li>\n",
      "                  <rdf:li>application/vnd.wap.xhtml+xml</rdf:li>\n",
      "                  <rdf:li>multipart/mixed</rdf:li>\n",
      "                  <rdf:li>text/html</rdf:li>\n",
      "                  <rdf:li>text/plain</rdf:li>\n",
      "                  <rdf:li>text/vnd.wap.si</rdf:li>\n",
      "                  <rdf:li>text/vnd.wap.sl</rdf:li>\n",
      "                  <rdf:li>text/vnd.wap.wml</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:Push-Accept>\n",
      "            <prf:Push-Accept-Charset>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>ISO-8859-1</rdf:li>\n",
      "                  <rdf:li>UTF-8</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:Push-Accept-Charset>\n",
      "            <prf:Push-Accept-Encoding>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>base64</rdf:li>\n",
      "                  <rdf:li>quoted-printable</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:Push-Accept-Encoding>\n",
      "            <prf:Push-Accept-Language>\n",
      "               <rdf:Seq>\n",
      "                  <rdf:li>de-DE</rdf:li>\n",
      "               </rdf:Seq>\n",
      "            </prf:Push-Accept-Language>\n",
      "            <prf:Push-Accept-AppID>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>x-wap-application:wml.ua</rdf:li>\n",
      "                  <rdf:li>*</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </prf:Push-Accept-AppID>\n",
      "            <prf:Push-MsgSize>30000</prf:Push-MsgSize>\n",
      "            <prf:Push-MaxPushReq>5</prf:Push-MaxPushReq>\n",
      "         </rdf:Description>\n",
      "      </prf:component>\n",
      "      <prf:component>\n",
      "         <rdf:Description rdf:ID=\"MmsCharacteristics\">\n",
      "            <rdf:type rdf:resource=\"http://www.openmobilealliance.org/tech/profiles/MMS/ccppschema-20050301-MMS1.2#MmsCharacteristics\"/>\n",
      "            <mms:MmsMaxMessageSize>102400</mms:MmsMaxMessageSize>\n",
      "            <mms:MmsMaxImageResolution>640x480</mms:MmsMaxImageResolution>\n",
      "            <mms:MmsCcppAccept>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>application/java-archive</rdf:li>\n",
      "                  <rdf:li>application/sdp</rdf:li>\n",
      "                  <rdf:li>application/smil</rdf:li>\n",
      "                  <rdf:li>audio/mid</rdf:li>\n",
      "                  <rdf:li>audio/midi</rdf:li>\n",
      "                  <rdf:li>image/gif</rdf:li>\n",
      "                  <rdf:li>image/jpeg</rdf:li>\n",
      "                  <rdf:li>image/jpg</rdf:li>\n",
      "                  <rdf:li>image/vnd.wap.wbmp</rdf:li>\n",
      "                  <rdf:li>text/plain</rdf:li>\n",
      "                  <rdf:li>text/vnd.sun.j2me.app-descriptor</rdf:li>\n",
      "                  <rdf:li>text/vnd.wap.wml</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </mms:MmsCcppAccept>\n",
      "            <mms:MmsCcppAcceptCharSet>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>ISO-8859-1</rdf:li>\n",
      "                  <rdf:li>UTF-8</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </mms:MmsCcppAcceptCharSet>\n",
      "            <mms:MmsCcppAcceptLanguage>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>de-DE</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </mms:MmsCcppAcceptLanguage>\n",
      "            <mms:MmsCcppAcceptEncoding>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>base64</rdf:li>\n",
      "                  <rdf:li>quoted-printable</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </mms:MmsCcppAcceptEncoding>\n",
      "            <mms:MmsVersion>\n",
      "               <rdf:Bag>\n",
      "                  <rdf:li>1.0</rdf:li>\n",
      "               </rdf:Bag>\n",
      "            </mms:MmsVersion>\n",
      "         </rdf:Description>\n",
      "      </prf:component>\n",
      "   </rdf:Description>\n",
      "</rdf:RDF>\n",
      "</nowiki>\n",
      "\n",
      "Sekcja \"HardwarePlatform\" opisuje m.in. rozdzielczoÅ›Ä‡ wyÅ›wietlacza, profile Bluetooth, itd.\n",
      "\n",
      "W zakresie \"SoftwarePlatform\" sÄ… m.in. wymienione sÄ… wszystkie typy MIME-Type akceptowane przez dany telefon.\n",
      "\n",
      "Dla pojedynczych programÃ³w (np. Browser, MMS, Streaming) i serwisÃ³w (np. WAP-Push czy DRM) istniejÄ… specjalne zakresy.\n",
      "\n",
      "Linki zewnÄ™trzne \n",
      " User Agent Profile Specyfikacja WAP Forum\n",
      " UAProf 2.0 Specyfikacja OMA\n",
      "\n",
      "Telefonia komÃ³rkowa<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "def create_word_frequency_df(text_list, n=200):\n",
    "    \"\"\"\n",
    "    Create a DataFrame with counts of the n most common words in a list of texts.\n",
    "    \n",
    "    Args:\n",
    "    - text_list (list): A list of strings to analyze\n",
    "    - n (int): Number of top words to include (default: 200)\n",
    "    \n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame with columns 'word' and 'count', sorted by count in descending order\n",
    "    \"\"\"\n",
    "    # Combine all texts into a single string\n",
    "    all_text = ' '.join(text_list)\n",
    "    \n",
    "    # Convert to lowercase and split into words\n",
    "    words = re.findall(r'\\w+', all_text.lower())\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Get the n most common words\n",
    "    most_common = word_counts.most_common(n)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(most_common, columns=['word', 'count'])\n",
    "    \n",
    "    return df\n",
    "pd.set_option('display.max_rows', 500)\n",
    "# create_word_frequency_df(dataset[\"text\"][:100000])\n",
    "for t in dataset[\"text\"][::-1]:\n",
    "    if len(t)>10_000:\n",
    "        print(t)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuKOAUDpUeDL"
   },
   "source": [
    "If you're looking to make your own chat template, that also is possible! You must use the Jinja templating regime. We provide our own stripped down version of the `Unsloth template` which we find to be more efficient, and leverages ChatML, Zephyr and Alpaca styles.\n",
    "\n",
    "More info on chat templates on [our wiki page!](https://github.com/unslothai/unsloth/wiki#chat-templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPJlNOKJcfmk",
    "outputId": "e2f91cf4-62a0-4d21-bccf-72b6917dda38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masobieszek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "import math\n",
    "import wandb\n",
    "import os\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "wandb.login(key=\"2b23111621454d465a8227978bee4da77bc05133\")\n",
    "os.environ[\"WANDB_API_KEY\"] = \"2b23111621454d465a8227978bee4da77bc05133\"\n",
    "\n",
    "# Define a custom callback for logging\n",
    "class WandbLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        # Log training loss\n",
    "        if state.log_history[-1].get(\"loss\") is not None:\n",
    "            wandb.log({\"train_loss\": state.log_history[-1][\"loss\"], \"step\": state.global_step})\n",
    "\n",
    "        # Log evaluation loss\n",
    "        if state.log_history[-1].get(\"eval_loss\") is not None:\n",
    "            wandb.log({\"eval_loss\": state.log_history[-1][\"eval_loss\"], \"step\": state.global_step})\n",
    "\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciQ8qoLudFiL"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890,
     "referenced_widgets": [
      "68750c4500144a6898c8f600cdfcc686",
      "49d1f028750247bfbb45ee771fa4980d",
      "462b9b8ca27f436aa7e766caa58dab58",
      "9dab5587948d45098c7a1176caf1ee4d",
      "cf05daf114d641d9a6629bd84b7e0e4e",
      "51ba937383a6484ca5901374ea6cdbc1",
      "e168e7dcf8674be1881769f14d4701f7",
      "c0e1e9166a1f41878536aec84f7d8828",
      "51ab7cfff7664bbab3e882b94eb0008b",
      "df4b72362d36428d88d1689f8a13fdf1",
      "9673ecc7c10e40129e8bca47d3205c16",
      "79b9b09e6b614f549824497cec1dc60a",
      "c16f69d33eec4cfe867bf86ff107e0a0",
      "0ba2f90b9ea640e1a86879a4f5f19975",
      "dd44c480e77e4139930eeaf6284c04c0",
      "67505ee3591a459eb70cbf5b2841e257",
      "a035db54dc2f4084822e3e564d747a31",
      "74360785d46c4416bbdc069caa89d597",
      "b380c6c7b0074fe1ae636c7e3308904f",
      "e273799ceedc4e86be89d998b114c92c",
      "d8e081831b9c4c019afd3fa820d40154",
      "14cea07ee2bf4d6b9d0f699920754345",
      "e7c1107db7dd463c920c9454a3a0c1f1",
      "b76fd9c04d4d44a68e14449d66dc17a8",
      "2ee69bde4b294798bb522a735890f1b2",
      "b142bc1aa35642b1aaecdf51a5eaaa08",
      "1d5246f9361b4e469c32ef1e2199e85c",
      "ad17eb7d907f4d0988b163d5b86eeff7",
      "4ec4fbc8194041a6a299980c4cfc5f95",
      "cec84bfc1dbd4e718634387edd6b25f0"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "4ae2894c-c0c6-4b26-a56e-1e6c7d8c6b43"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153c56dba20d40b8b48b22e63c17ca38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 7.5e-5\n",
    "embedding_learning_rate = 1.5e-5\n",
    "batch_size = 32\n",
    "weight_decay = 0.5\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing=True,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 32,\n",
    "        gradient_accumulation_steps = 1,\n",
    "\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 3,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = learning_rate,\n",
    "        embedding_learning_rate = embedding_learning_rate,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = weight_decay,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs_filtered\",\n",
    "        report_to=\"wandb\",  # Enable wandb logging\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 1000,\n",
    "    ),\n",
    "    callbacks=[WandbLoggingCallback()],\n",
    ")\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"wiki-pretrain-llama3.1-8b\",\n",
    "           entity=\"jutro\",\n",
    "\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"embedding_learning_rate\": embedding_learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lora_r\": 128,\n",
    "        \"weight_decay\": weight_decay,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4PSupDtR7NP"
   },
   "outputs": [],
   "source": [
    "# flush cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "trainer.args.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "49ea85e6-03cc-43ee-d5a7-63ab190c8c6a"
   },
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "60086041-53c8-4443-8ed5-9f8e88fac046",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train() # resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upcOlWe7A1vc"
   },
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"7k_wiki\") # Local saving\n",
    "# model.push_to_hub(\"ASobieszek/l3.1-7k\", token = \"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\") # Online saving\n",
    "model.push_to_hub(\"ASobieszek/l3.1-wiki-15k-128-wd0.5\", token = \"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\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFfaXG0WsQuE"
   },
   "outputs": [],
   "source": [
    "\n",
    "from peft import AutoModelForPeftCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "model = AutoModelForPeftCausalLM.from_pretrained(\n",
    "    \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    load_in_4bit = False,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDp0zNpwe6U_"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ba2f90b9ea640e1a86879a4f5f19975": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14cea07ee2bf4d6b9d0f699920754345": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d5246f9361b4e469c32ef1e2199e85c",
      "max": 122,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ad17eb7d907f4d0988b163d5b86eeff7",
      "value": 122
     }
    },
    "1d5246f9361b4e469c32ef1e2199e85c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ee69bde4b294798bb522a735890f1b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "462b9b8ca27f436aa7e766caa58dab58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e168e7dcf8674be1881769f14d4701f7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c0e1e9166a1f41878536aec84f7d8828",
      "value": 1
     }
    },
    "49d1f028750247bfbb45ee771fa4980d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf05daf114d641d9a6629bd84b7e0e4e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_51ba937383a6484ca5901374ea6cdbc1",
      "value": "0.023 MB of 0.023 MB uploaded\r"
     }
    },
    "4ec4fbc8194041a6a299980c4cfc5f95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51ab7cfff7664bbab3e882b94eb0008b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df4b72362d36428d88d1689f8a13fdf1",
       "IPY_MODEL_9673ecc7c10e40129e8bca47d3205c16",
       "IPY_MODEL_79b9b09e6b614f549824497cec1dc60a"
      ],
      "layout": "IPY_MODEL_c16f69d33eec4cfe867bf86ff107e0a0"
     }
    },
    "51ba937383a6484ca5901374ea6cdbc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67505ee3591a459eb70cbf5b2841e257": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68750c4500144a6898c8f600cdfcc686": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49d1f028750247bfbb45ee771fa4980d",
       "IPY_MODEL_462b9b8ca27f436aa7e766caa58dab58"
      ],
      "layout": "IPY_MODEL_9dab5587948d45098c7a1176caf1ee4d"
     }
    },
    "74360785d46c4416bbdc069caa89d597": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79b9b09e6b614f549824497cec1dc60a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74360785d46c4416bbdc069caa89d597",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b380c6c7b0074fe1ae636c7e3308904f",
      "value": "â€‡12018/12018â€‡[00:05&lt;00:00,â€‡3015.18â€‡examples/s]"
     }
    },
    "9673ecc7c10e40129e8bca47d3205c16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67505ee3591a459eb70cbf5b2841e257",
      "max": 12018,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a035db54dc2f4084822e3e564d747a31",
      "value": 12018
     }
    },
    "9dab5587948d45098c7a1176caf1ee4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a035db54dc2f4084822e3e564d747a31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ad17eb7d907f4d0988b163d5b86eeff7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b142bc1aa35642b1aaecdf51a5eaaa08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b380c6c7b0074fe1ae636c7e3308904f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b76fd9c04d4d44a68e14449d66dc17a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0e1e9166a1f41878536aec84f7d8828": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c16f69d33eec4cfe867bf86ff107e0a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cec84bfc1dbd4e718634387edd6b25f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf05daf114d641d9a6629bd84b7e0e4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8e081831b9c4c019afd3fa820d40154": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ee69bde4b294798bb522a735890f1b2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b142bc1aa35642b1aaecdf51a5eaaa08",
      "value": "Mapâ€‡(num_proc=2):â€‡100%"
     }
    },
    "dd44c480e77e4139930eeaf6284c04c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df4b72362d36428d88d1689f8a13fdf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ba2f90b9ea640e1a86879a4f5f19975",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_dd44c480e77e4139930eeaf6284c04c0",
      "value": "Mapâ€‡(num_proc=2):â€‡100%"
     }
    },
    "e168e7dcf8674be1881769f14d4701f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e273799ceedc4e86be89d998b114c92c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8e081831b9c4c019afd3fa820d40154",
       "IPY_MODEL_14cea07ee2bf4d6b9d0f699920754345",
       "IPY_MODEL_e7c1107db7dd463c920c9454a3a0c1f1"
      ],
      "layout": "IPY_MODEL_b76fd9c04d4d44a68e14449d66dc17a8"
     }
    },
    "e7c1107db7dd463c920c9454a3a0c1f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ec4fbc8194041a6a299980c4cfc5f95",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cec84bfc1dbd4e718634387edd6b25f0",
      "value": "â€‡122/122â€‡[00:01&lt;00:00,â€‡152.70â€‡examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
