{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7934b5cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib.pde_ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 246\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, List\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpde_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PDEState\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpde_losses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_losses \n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mWavePDE\u001b[39;00m(nn.Module):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lib.pde_ops'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import grad\n",
    "from torch.func import jvp as jvp_fwd\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Core stacked layers (vectorized over support-set axis K)\n",
    "# ================================================================\n",
    "class StackedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Per-k linear layers evaluated in parallel.\n",
    "    Weight: [K, out, in], Bias: [K, out]\n",
    "    Forward expects x: [B, K, in] -> y: [B, K, out]\n",
    "    \"\"\"\n",
    "    def __init__(self, K: int, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.K = int(K)\n",
    "        self.in_features = int(in_features)\n",
    "        self.out_features = int(out_features)\n",
    "\n",
    "        # [K, out, in]\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty(self.K, self.out_features, self.in_features)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(self.K, self.out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # K independent Kaiming-uniform initializations\n",
    "        for k in range(self.K):\n",
    "            nn.init.kaiming_uniform_(self.weight.data[k], a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in = self.in_features\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            nn.init.uniform_(self.bias.data, -bound, bound)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, K, in]\n",
    "        # y[b,k,o] = sum_i x[b,k,i] * W[k,o,i] + b[k,o]\n",
    "        y = torch.einsum(\"bki,koi->bko\", x, self.weight)\n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias.unsqueeze(0)  # [1,K,out]\n",
    "        return y\n",
    "\n",
    "\n",
    "class OutputBatchNormPerK(nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm applied per (k, channel) using batch statistics over B only.\n",
    "    x: [B, K, C] -> y: [B, K, C]\n",
    "\n",
    "    Keeps running_mean/var per (K,C). Supports affine per (K,C).\n",
    "    This preserves BatchNorm semantics for each potential independently.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        K: int,\n",
    "        C: int,\n",
    "        eps: float = 1e-5,\n",
    "        momentum: float = 0.1,\n",
    "        affine: bool = True,\n",
    "        track_running_stats: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.K = int(K)\n",
    "        self.C = int(C)\n",
    "        self.eps = float(eps)\n",
    "        self.momentum = float(momentum)\n",
    "        self.affine = bool(affine)\n",
    "        self.track_running_stats = bool(track_running_stats)\n",
    "\n",
    "        if self.affine:\n",
    "            self.weight = nn.Parameter(torch.ones(self.K, self.C))  # gamma\n",
    "            self.bias = nn.Parameter(torch.zeros(self.K, self.C))   # beta\n",
    "        else:\n",
    "            self.register_parameter(\"weight\", None)\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            self.register_buffer(\"running_mean\", torch.zeros(self.K, self.C))\n",
    "            self.register_buffer(\"running_var\", torch.ones(self.K, self.C))\n",
    "            self.register_buffer(\"num_batches_tracked\", torch.tensor(0, dtype=torch.long))\n",
    "        else:\n",
    "            self.register_buffer(\"running_mean\", None)\n",
    "            self.register_buffer(\"running_var\", None)\n",
    "            self.register_buffer(\"num_batches_tracked\", None)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert x.dim() == 3 and x.size(1) == self.K and x.size(2) == self.C, (\n",
    "            f\"Expected [B,K,{self.C}] got {tuple(x.shape)}\")\n",
    "        B = x.size(0)\n",
    "\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=0)  # [K,C]\n",
    "            var = x.var(dim=0, unbiased=False)  # [K,C]\n",
    "\n",
    "            if self.track_running_stats:\n",
    "                with torch.no_grad():\n",
    "                    self.num_batches_tracked += 1\n",
    "                    m = self.momentum\n",
    "                    self.running_mean.mul_(1 - m).add_(m * mean)\n",
    "                    self.running_var.mul_(1 - m).add_(m * var)\n",
    "        else:\n",
    "            if self.track_running_stats:\n",
    "                mean = self.running_mean\n",
    "                var = self.running_var\n",
    "            else:\n",
    "                # fall back to batch stats if not tracking\n",
    "                mean = x.mean(dim=0)\n",
    "                var = x.var(dim=0, unbiased=False)\n",
    "\n",
    "        y = (x - mean.unsqueeze(0)) / torch.sqrt(var.unsqueeze(0) + self.eps)\n",
    "        if self.affine:\n",
    "            y = y * self.weight.unsqueeze(0) + self.bias.unsqueeze(0)\n",
    "        return y\n",
    "\n",
    "\n",
    "class StackedSinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal embedding broadcast over K.\n",
    "    Input: t [B, K, 1] -> emb [B, K, E]; E must be even.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim: int):\n",
    "        super().__init__()\n",
    "        assert emb_dim % 2 == 0, \"time embedding dim should be even\"\n",
    "        self.emb_dim = int(emb_dim)\n",
    "        half_dim = emb_dim // 2\n",
    "        emb_scale = math.log(10000.0) / max(half_dim - 1, 1)\n",
    "        freqs = torch.exp(torch.arange(half_dim) * -emb_scale)\n",
    "        self.register_buffer(\"freqs\", freqs, persistent=False)\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        # t: [B,K,1]\n",
    "        angles = t * self.freqs.view(1, 1, -1).to(t.dtype)  # [B,K,half]\n",
    "        return torch.cat([angles.sin(), angles.cos()], dim=-1)  # [B,K,E]\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Stacked potentials: f (SemanticPotential) and ψ (SliceEnergy)\n",
    "#   - Hidden layers have NO BatchNorm per your requirement\n",
    "#   - Only the FINAL output of f (and optionally ψ) is BatchNormed per-k\n",
    "# ================================================================\n",
    "class StackedSemanticPotential(nn.Module):\n",
    "    def __init__(self, K: int, n_in: int, n_out: int = 1, n_hidden: int = 128,  activation: nn.Module = nn.Tanh(), final_activation: nn.Module = nn.Identity()):\n",
    "        super().__init__()\n",
    "        self.K = int(K)\n",
    "        self.n_in = int(n_in)\n",
    "        self.n_out = int(n_out)\n",
    "        self.n_hidden = int(n_hidden)\n",
    "\n",
    "        self.fc1 = StackedLinear(self.K, self.n_in, self.n_in)\n",
    "        self.act1 = activation\n",
    "\n",
    "        self.fc2 = StackedLinear(self.K, self.n_in, self.n_hidden)\n",
    "        self.act2 = activation\n",
    "\n",
    "        self.fc3 = StackedLinear(self.K, self.n_hidden, self.n_hidden)\n",
    "        self.act3 = activation\n",
    "\n",
    "        self.fc4 = StackedLinear(self.K, self.n_hidden, self.n_out)\n",
    "\n",
    "        # Additional linear component from input to output, initialized to random unit directions (per k)\n",
    "        self.dir_linear = StackedLinear(self.K, self.n_in, self.n_out, bias=False)\n",
    "        with torch.no_grad():\n",
    "            W = self.dir_linear.weight.data  # [K, out(=1), in]\n",
    "            # normalize each row vector (per k, per out)\n",
    "            norms = W.norm(dim=-1, keepdim=True).clamp_min_(1e-8)  # [K,1,1]\n",
    "            W.normal_()\n",
    "            W.div_(norms)\n",
    "\n",
    "        # Output-only BatchNorm per k\n",
    "        self.out_bn = OutputBatchNormPerK(self.K, self.n_out)\n",
    "        self.final_activation = final_activation\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B,K,D]\n",
    "        h = self.act1(self.fc1(x))\n",
    "        h = self.act2(self.fc2(h))\n",
    "        h = self.act3(self.fc3(h))\n",
    "        out = self.fc4(h) + self.dir_linear(x)  # [B,K,1]\n",
    "        out = self.out_bn(out)\n",
    "        return self.final_activation(out)\n",
    "\n",
    "\n",
    "    def update_y_distributions(self, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Update the density estimates for each k. This should stay implementation-agnostic with a plug-in estimator class call.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class StackedSliceEnergy(nn.Module):\n",
    "    def __init__(self, K: int, n_in: int, n_out: int = 1, n_hidden: int = 64, final_activation: nn.Module = nn.Identity(),\n",
    "                 apply_output_bn: bool = False):\n",
    "        super().__init__()\n",
    "        self.K = int(K)\n",
    "        self.n_in = int(n_in)\n",
    "        self.n_out = int(n_out)\n",
    "        self.n_hidden = int(n_hidden)\n",
    "\n",
    "\n",
    "        # x pathway\n",
    "        self.layer_x = StackedLinear(self.K, self.n_in, self.n_in)\n",
    "        self.activation1 = nn.Tanh()\n",
    "\n",
    "        # time pathway (uses sinusoidal embeddings of size n_in)\n",
    "        self.layer_pos = StackedSinusoidalPositionEmbeddings(self.n_hidden)\n",
    "        self.layer_time = StackedLinear(self.K, self.n_hidden, self.n_hidden)\n",
    "        self.activation2 = nn.GELU()\n",
    "        self.layer_time2 = StackedLinear(self.K, self.n_hidden, self.n_in)\n",
    "\n",
    "        # fusion + output\n",
    "        self.layer_fusion = StackedLinear(self.K, self.n_in, self.n_in)\n",
    "        self.activation3 = nn.Tanh()\n",
    "        self.layer_out = StackedLinear(self.K, self.n_in, self.n_out)\n",
    "        self.activation4 = nn.Tanh()\n",
    "\n",
    "        self.apply_output_bn = bool(apply_output_bn)\n",
    "        self.out_bn = OutputBatchNormPerK(self.K, self.n_out) if self.apply_output_bn else None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B,K,D], time: [B,K,1]\n",
    "        xh = self.activation1(self.layer_x(x))\n",
    "        t_feat = self.layer_pos(time)\n",
    "        t_feat = self.layer_time(t_feat)\n",
    "        t_feat = self.activation2(t_feat)\n",
    "        t_feat = self.layer_time2(t_feat)\n",
    "\n",
    "        h = self.activation3(self.layer_fusion(xh + t_feat))\n",
    "        out = self.activation4(self.layer_out(h))  # [B,K,1]\n",
    "        if self.apply_output_bn:\n",
    "            out = self.out_bn(out)\n",
    "        return out\n",
    "# wave_pde.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "from lib.pde_ops import PDEState\n",
    "from lib.pde_losses import build_losses \n",
    "\n",
    "class WavePDE(nn.Module):\n",
    "    \"\"\"\n",
    "    K-parallel WavePDE powered by PDEState and modular PDE losses.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_support_sets: int,\n",
    "        num_support_timesteps: int,\n",
    "        support_vectors_dim: int,\n",
    "        lambdas: Dict[str, float] = {},                   # ONLY what you want active (may include 0.0)\n",
    "        n_laplace_probes: int = 1,\n",
    "        apply_bn_on_psi_output: bool = False,\n",
    "        # PDEState config\n",
    "        time_ad: str = \"reverse\",\n",
    "        detach_between_steps: bool = True,\n",
    "        eps_norm2: float = 1e-8,\n",
    "        divergence_probes: int = 1,\n",
    "        rng: str = \"rademacher\",\n",
    "        seed: Optional[int] = None,\n",
    "        # optional: prior score function for DivPrior (defaults to Gaussian score -x)\n",
    "        prior_score: Optional[callable] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_support_sets = int(num_support_sets)\n",
    "        self.num_support_timesteps = int(num_support_timesteps)\n",
    "        self.support_vectors_dim = int(support_vectors_dim)\n",
    "\n",
    "        # Learnable per-k scale (kept for parity; not wired to dt by default)\n",
    "        self.c = nn.Parameter(torch.full((self.num_support_sets, 1), 1.0))\n",
    "\n",
    "        # Stacked potentials\n",
    "        self.PSI = StackedSliceEnergy(\n",
    "            K=self.num_support_sets,\n",
    "            n_in=self.support_vectors_dim,\n",
    "            n_out=1,\n",
    "            final_activation=nn.Identity(),\n",
    "            apply_output_bn=apply_bn_on_psi_output,\n",
    "        )\n",
    "        self.F = StackedSemanticPotential(\n",
    "            K=self.num_support_sets,\n",
    "            n_in=self.support_vectors_dim,\n",
    "            n_out=1,\n",
    "            final_activation=nn.Identity(),\n",
    "        )\n",
    "\n",
    "        # PDEState config for each step\n",
    "        self._pde_cfg = dict(\n",
    "            time_ad=time_ad,\n",
    "            detach_between_steps=detach_between_steps,\n",
    "            eps_norm2=eps_norm2,\n",
    "            laplace_probes=int(n_laplace_probes),\n",
    "            divergence_probes=int(divergence_probes),\n",
    "            rng=rng,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        # Build modular loss list from registry (only specified keys are included).\n",
    "        # Pass modules/params needed by certain losses through ctx.\n",
    "        epsilon = float(lambdas.get(\"epsilon\", 0.0))  # a scalar param (not a loss)\n",
    "        self.losses, self._needs_next = build_losses(\n",
    "            lambdas,\n",
    "            F=self.F,\n",
    "            epsilon=epsilon,\n",
    "            prior_score=prior_score,\n",
    "        )\n",
    "\n",
    "        # for telemetry\n",
    "        self._acc: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "    # ---- one step ----\n",
    "    def _per_step(self, z_bkd: torch.Tensor, direction: int = +1):\n",
    "        st = PDEState(\n",
    "            f=self.F,\n",
    "            psi=self.PSI,\n",
    "            z=z_bkd,\n",
    "            direction=direction,\n",
    "            need_next=self._needs_next,\n",
    "            dt_value=1.0,  # use ±1 step; wire self.c here if desired\n",
    "            **self._pde_cfg,\n",
    "        )\n",
    "\n",
    "        # compute & sum selected losses [B,K,1]\n",
    "        per_bk = [L(st) for L in self.losses]\n",
    "        L_sum = sum(per_bk) if per_bk else st.zeros()  # if no losses, zero tensor\n",
    "\n",
    "        # next latent (semi-implicit Euler @ now)\n",
    "        x_next = st.x_next()\n",
    "\n",
    "        # (optional) same small step noise as before\n",
    "        with torch.no_grad():\n",
    "            step_delta_norms = (x_next - st.x()).norm(dim=-1, keepdim=True)\n",
    "            latent_noise = torch.randn_like(x_next)\n",
    "            latent_noise = latent_noise / latent_noise.norm(dim=-1, keepdim=True).clamp_min_(1e-12)\n",
    "            latent_noise = latent_noise * (step_delta_norms / 5.0)\n",
    "        x_next_noisy = x_next + latent_noise\n",
    "\n",
    "        return st, x_next_noisy, L_sum\n",
    "\n",
    "    # ---- unrolled training ----\n",
    "    def forward(self, z: torch.Tensor, t_index: torch.Tensor, direction: int = +1):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          potential_preds: [B,K,1] (detached)\n",
    "          latent1_bk, latent2_bk: [B,K,D] (pair captured at t_index)\n",
    "          L_total_mean: scalar\n",
    "        \"\"\"\n",
    "        B, D = z.shape\n",
    "        K = self.num_support_sets\n",
    "        T = max(1, int(self.num_support_timesteps))\n",
    "\n",
    "        if t_index.ndim == 1:\n",
    "            t_index = t_index.unsqueeze(-1)\n",
    "        i_target = torch.clamp(t_index, 0, T - 1).long().squeeze(-1)\n",
    "\n",
    "        # expand once to K stacks\n",
    "        z_curr = z.unsqueeze(1).expand(B, K, D).contiguous()\n",
    "\n",
    "        latent1_bk = None\n",
    "        latent2_bk = None\n",
    "        last_st: Optional[PDEState] = None\n",
    "\n",
    "        L_accum = None  # accumulate per-[B,K,1]\n",
    "\n",
    "        step_iter = range(T) if direction == +1 else reversed(range(T))\n",
    "        for i in step_iter:\n",
    "            st, x_next, L_step = self._per_step(z_curr, direction=direction)\n",
    "\n",
    "            # accumulate loss per step\n",
    "            L_accum = (L_step if L_accum is None else L_accum + L_step)\n",
    "\n",
    "            # capture (latent1, latent2) at the requested index\n",
    "            mask_b = (i_target == i).view(B, 1, 1)\n",
    "            if latent1_bk is None:\n",
    "                latent1_bk = torch.where(mask_b, z_curr, torch.zeros_like(z_curr))\n",
    "                latent2_bk = torch.where(mask_b, x_next, torch.zeros_like(x_next))\n",
    "            else:\n",
    "                latent1_bk = torch.where(mask_b, z_curr, latent1_bk)\n",
    "                latent2_bk = torch.where(mask_b, x_next, latent2_bk)\n",
    "\n",
    "            # advance\n",
    "            z_curr = x_next\n",
    "            last_st = st\n",
    "\n",
    "        # average over steps\n",
    "        L_total_per_bk = L_accum / float(T) if L_accum is not None else last_st.zeros()\n",
    "        L_total_mean = L_total_per_bk.mean()\n",
    "\n",
    "        # telemetry\n",
    "        potential_preds = last_st.f().detach() if last_st is not None else torch.zeros(B, K, 1, device=z.device, dtype=z.dtype)\n",
    "        \n",
    "        self._acc = {\n",
    "            \"xf_now\": last_st.Xf() if last_st is not None else torch.zeros(B, K, D, device=z.device, dtype=z.dtype),\n",
    "            \"L_mean\": L_total_mean.detach(),\n",
    "            **last_st.state[\"losses\"],\n",
    "        }\n",
    "        return potential_preds, latent1_bk, latent2_bk, L_total_mean\n",
    "\n",
    "    def get_losses(self) -> Dict[str, torch.Tensor]:\n",
    "        return self._acc\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def inference(self, z: torch.Tensor, direction: int = +1) -> List[torch.Tensor]:\n",
    "        B, D = z.shape\n",
    "        K = self.num_support_sets\n",
    "        T = max(1, int(self.num_support_timesteps))\n",
    "        traj: List[torch.Tensor] = []\n",
    "\n",
    "        z_curr = z.unsqueeze(1).expand(B, K, D).contiguous()\n",
    "        traj.append(z_curr)\n",
    "\n",
    "        for _ in (range(T) if direction == +1 else reversed(range(T))):\n",
    "            st = PDEState(\n",
    "                f=self.F,\n",
    "                psi=self.PSI,\n",
    "                z=z_curr,\n",
    "                direction=direction,\n",
    "                need_next=False,\n",
    "                dt_value=1.0,\n",
    "                **self._pde_cfg,\n",
    "            )\n",
    "            z_next = st.x() + st.dt() * st.v(\"now\")\n",
    "            traj.append(z_next)\n",
    "            z_curr = z_next\n",
    "        return traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title init\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import shutil\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image\n",
    "from PIL import Image, ImageDraw\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interact, Button, FloatSlider, Layout\n",
    "import sys\n",
    "\n",
    "import io\n",
    "import os, time\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import copy\n",
    "import unicodedata\n",
    "import re\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from IPython.display import display\n",
    "from einops import rearrange\n",
    "from time import perf_counter\n",
    "\n",
    "# df = pd.read_hdf('coords_wlosses.h5')\n",
    "#   \n",
    "df = pd.read_csv('/Users/adamsobieszek/PycharmProjects/psychGAN/content/coords_wlosses.csv')\n",
    "sys.path.append('/Users/adamsobieszek/PycharmProjects/_manipy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adamsobieszek/PycharmProjects/_manipy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torchvision.transforms import Compose, Resize\n",
    "import torchvision.transforms.functional as TF\n",
    "from typing import List, Dict, Union, Tuple\n",
    "%cd /Users/adamsobieszek/PycharmProjects/_manipy/\n",
    "# Assuming the existence of these from a library like torchdyn\n",
    "from torchdyn.core import NeuralODE\n",
    "from torchdyn.datasets import *\n",
    "from torchdyn.models import *\n",
    "\n",
    "\n",
    "from manipy.models.flow_models import VectorFieldTransformer, RatingODE\n",
    "from manipy.models.layers import EnsembleRegressor, MeanRegressor\n",
    "from manipy.models.rating_models import AlphaBetaRegressor\n",
    "\n",
    "from manipy.stylegan.utils import sample_w, G_context\n",
    "\n",
    "final_models_path = \"/Users/adamsobieszek/PycharmProjects/psychGAN/content/\"\n",
    "device = torch.device('mps')\n",
    "\n",
    "# del control_models\n",
    "if not \"control_models\" in dir():\n",
    "  # if not os.path.exists(\"final_models.zip\"):\n",
    "  #   !gdown 1pPjOd-mx-d-vOw1QR_lpJoJmLAGdkI3W\n",
    "  #   !unzip final_models.zip\n",
    "  #   !unzip final_models.zip\n",
    "  control_names = ['happy', 'gender', 'age', 'black', 'trustworthy','attractive','smart', 'dominant', 'asian', 'white']\n",
    "  control_models = [EnsembleRegressor([MeanRegressor(512,1) for _ in range(8)], model_kwargs={}).to(device) for label in control_names]\n",
    "  for m,l in zip(control_models,control_names):\n",
    "    m.load_state_dict(torch.load(f\"{final_models_path}/final_models/ensemble_{l}.pt\", map_location=torch.device(\"mps\")))\n",
    "    m.eval()\n",
    "  ALL_MODELS = {**{l:m for l,m in zip(control_names,control_models)}}\n",
    "\n",
    "\n",
    "# G, face_w, device = setup_stylegan()\n",
    "\n",
    "sys.path.append('/Users/adamsobieszek/PycharmProjects/psychGAN/content/psychGAN/stylegan3')\n",
    "with open('/Users/adamsobieszek/PycharmProjects/psychGAN/stylegan2-ffhq-1024x1024.pkl', 'rb') as fp:\n",
    "    G = pickle.load(fp)['G_ema'].to(device)\n",
    "\n",
    "# Compute the average latent vector\n",
    "all_z = torch.randn([1, G.mapping.z_dim], device=device)\n",
    "face_w = G.mapping(all_z, None, truncation_psi=0.5)\n",
    "\n",
    "dim = \"trustworthy\"\n",
    "\n",
    "# models = [AlphaBetaRegressor(dim=512).to(device) for i in range(8)]\n",
    "# for i,m in enumerate([torch.load(f\"/Users/adamsobieszek/PycharmProjects/psychGAN/best_models/{mm}\",map_location=device) for mm in [f\"model_{dim}_v{3+i}.pt\" for i in range(8)]]):\n",
    "#     models[i].load_state_dict(m)\n",
    "#     models[i].eval()\n",
    "# model = EnsembleRegressor(models)\n",
    "\n",
    "# flow = VectorFieldTransformer(\n",
    "#     rating_model=model,\n",
    "#     dim=512,\n",
    "#     depth=8,\n",
    "#     num_heads=8,\n",
    "#     dim_head=48,\n",
    "#     num_registers=32,\n",
    "#     dropout=0.15,\n",
    "#     add_rating_gradient=True,\n",
    "#     use_rotary=True\n",
    "# ).to(device)\n",
    "# flow.eval()\n",
    "# checkpoint_path =  '/Users/adamsobieszek/PycharmProjects/psychGAN/trustworthy_final_final_final.pt'\n",
    "# flow.load_state_dict(torch.load(checkpoint_path,map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e13ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7262809, 0.738845)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deixis.data.load_datasets import prepare_data2, load_psychGAN_data\n",
    "from manipy.stylegan.utils import sample_w, show_faces\n",
    "\n",
    "def cpu_numpy(*args):\n",
    "    return tuple([a.detach().cpu().numpy() for a in args]) if len(args)>1 else args[0].detach().cpu().numpy()\n",
    "def tensor_dd(*args):\n",
    "    return tuple([torch.tensor(a, dtype=torch.float32).to(device) for a in args]) if len(args)>1 else torch.tensor(args[0], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "dim = 'trustworthy'\n",
    "\n",
    "\n",
    "def get_data(dim, study=3):\n",
    "    X, y, weights, imgs = prepare_data2({'data':{'attribute_dim':dim}},return_imgs=True)\n",
    "    if study == 3:\n",
    "        imgs_vector = [i for i in imgs if any([f\"{j}_peterson\" in i for j in range(1,5)]) or \"0_our\" in i]\n",
    "        imgs_flow = [i for i in imgs if any([f\"{j}_our\" in i for j in range(5)])]\n",
    "    else:\n",
    "        imgs_vector = [i for i in imgs if any([f\"_vector_level_{j}\" in i for j in range(1,5)]) or \"_flow_level_0\" in i]\n",
    "        imgs_flow = [i for i in imgs if any([f\"_flow_level_{j}\" in i for j in range(5)])]\n",
    "\n",
    "    X_vector, y_vector, weights_vector, imgs_vector = prepare_data2({'data':{'attribute_dim':dim,\"imgs\":sorted(imgs_vector)}},return_imgs=True)\n",
    "    X_flow, y_flow, weights_flow, imgs_flow = prepare_data2({'data':{'attribute_dim':dim,\"imgs\":sorted(imgs_flow)}},return_imgs=True)\n",
    "    for i in range(5):\n",
    "        imgs_flow = [m.replace(f\"{i}_our\", f\"flow_level_{i}\") for m in imgs_flow]\n",
    "        imgs_vector = [m.replace(f\"{i}_peterson\", f\"vector_level_{i}\") for m in imgs_vector]\n",
    "    imgs_vector = [m.replace(\"0_our\", \"vector_level_0\").replace(\"_flow_level_0\", \"_vector_level_0\") for m in imgs_vector]\n",
    "    y_vector = torch.nanmean(y_vector,dim=1)\n",
    "    y_flow = torch.nanmean(y_flow,dim=1)\n",
    "\n",
    "    preds_vector = ALL_MODELS['trustworthy'](X_vector).squeeze()\n",
    "    preds_flow = ALL_MODELS['trustworthy'](X_flow).squeeze()\n",
    "    preds_np, X_np, y_np, weights_np = cpu_numpy(preds_vector, X_vector, y_vector, weights_vector)\n",
    "    preds_np_flow, X_np_flow, y_np_flow, weights_np_flow = cpu_numpy(preds_flow, X_flow, y_flow, weights_flow)\n",
    "    return preds_np, preds_np_flow, X_np, X_np_flow, y_np, y_np_flow, weights_np, weights_np_flow, imgs_vector, imgs_flow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preds_np, preds_np_flow, X_np, X_np_flow, y_np, y_np_flow, weights_np, weights_np_flow, imgs_vector, imgs_flow = get_data(dim)\n",
    "# calculate weighted spearmanr coef between predicted ratings and true ratings\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "pearsonr(preds_np, y_np)[0], pearsonr(preds_np_flow, y_np_flow)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444101fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ef010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.01, corr: 0.6341\n",
      "alpha: 0.04, corr: 0.6343\n",
      "alpha: 0.16, corr: 0.6347\n",
      "alpha: 0.64, corr: 0.6364\n",
      "alpha: 2.54, corr: 0.6421\n",
      "alpha: 10.14, corr: 0.6559\n",
      "alpha: 40.48, corr: 0.6741\n",
      "alpha: 161.59, corr: 0.6782\n",
      "alpha: 645.09, corr: 0.6460\n",
      "alpha: 2575.26, corr: 0.5846\n",
      "Best L2 penalty (alpha): 103.76050197669117\n",
      "Best test MSE: 0.16102901101112366\n",
      "Its test Pearson correlation: 0.6800405383110046\n",
      "Its train Pearson correlation: 0.877437949180603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(103.76050197669117, 0.16102901101112366, 0.68004054, 0.87743795)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "def load_psychGAN_data():\n",
    "    psychGAN_path = \"/Users/adamsobieszek/PycharmProjects/psychGAN/\"\n",
    "    with open(psychGAN_path+\"photo_to_coords.pkl\", \"rb\") as f:\n",
    "        photo_to_coords = pickle.load(f)\n",
    "    with open(psychGAN_path+\"dim_to_photo_to_ratings.pkl\", \"rb\") as f:\n",
    "        dim_to_photo_to_ratings = pickle.load(f)\n",
    "\n",
    "\n",
    "    return photo_to_coords, dim_to_photo_to_ratings\n",
    "\n",
    "photo_to_coords, dim_to_photo_to_ratings = load_psychGAN_data()\n",
    "\n",
    "def pad_list(l, max_len):\n",
    "    return l[:max_len] + [np.nan] * (max_len - len(l[:max_len]))\n",
    "    \n",
    "def prepare_data(config: dict, verbose=False, return_imgs=False):\n",
    "    global photo_to_coords, dim_to_photo_to_ratings, device, dtype\n",
    "    xs, ys = photo_to_coords, dim_to_photo_to_ratings[config['data']['attribute_dim']]\n",
    "    _imgs = set(ys.keys()).intersection(set(xs.keys()))#-set('638.jpg')\n",
    "    if config['data'].get('imgs', None) is not None:\n",
    "        imgs = _imgs.intersection(set(config['data']['imgs']))\n",
    "        if len(imgs)<len(config['data']['imgs']):\n",
    "            print(f\"Warning: {len(config['data']['imgs'])-len(imgs)} images not found in psychGAN dataset\")\n",
    "    else:\n",
    "        imgs = sorted(_imgs)\n",
    "\n",
    "    X = torch.stack([xs[k] for k in imgs]).to(device, dtype)\n",
    "    max_len = int(max(len(ys[k]) for k in imgs) * config['data'].get('rating_oversampling_factor', 1.0))\n",
    "    y = torch.tensor([pad_list(ys[k], max_len) for k in imgs], device=device, dtype=dtype)\n",
    "    n = y.shape[1] - y.isnan().sum(dim=1)\n",
    "    sample_size_weights = (n.reshape(-1)).to(device, dtype)\n",
    "    sample_size_weights = sample_size_weights / sample_size_weights.mean()\n",
    "    if return_imgs:\n",
    "        return X, y, sample_size_weights, imgs\n",
    "    else:\n",
    "        return X, y, sample_size_weights\n",
    "\n",
    "def cpu_numpy(*args):\n",
    "    if len(args)==1 and isinstance(args[0], (tuple, list,set)):\n",
    "        args = tuple(args[0])\n",
    "    output = tuple([a.detach().cpu().numpy() if isinstance(a, torch.Tensor) else a for a in args])\n",
    "    return output if len(output)>1 else output[0]\n",
    "def tensor_dd(*args):\n",
    "    if len(args)==1 and isinstance(args[0], (tuple, list,set)):\n",
    "        args = tuple(args[0])\n",
    "    output = tuple([a.to(dtype=torch.float32,device=device) if isinstance(a, torch.Tensor) else (torch.tensor(a, dtype=torch.float32).to(device) if isinstance(a, np.ndarray) else a) for a in args])\n",
    "    return output if len(output)>1 else output[0]\n",
    "\n",
    "def get_data(dim, train=True, logit=True):\n",
    "    X, y, weights, imgs = prepare_data({'data':{'attribute_dim':dim}},return_imgs=True)\n",
    "    train_imgs = [f\"{i}.jpg\" for i in range(1,1005)]+[i for i in imgs if \"0_our\" in i or \"_flow_level_0\" in i]\n",
    "    X, y, weights, imgs = prepare_data({'data':{'attribute_dim':dim,\"imgs\":(train_imgs if train else list(set(imgs)-set(train_imgs)))}},return_imgs=True) #list(set(imgs)-set(train_imgs))\n",
    "    \n",
    "    weights[[len(i)>8 for i in imgs]] = weights[[len(i)>8 for i in imgs]]*1.5\n",
    "    y = y.nanmean(dim=1)\n",
    "    \n",
    "    return X, (torch.logit if logit else lambda x: x)(y), weights, imgs\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_coefs(dim: str, alpha: float, *, fit_intercept: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Fit Ridge on the full dataset for `dim` and return the coefficient vector as a torch tensor.\n",
    "    \n",
    "    Trains on -y (as in your search loop), then returns -coef_ so that X @ returned_coefs (+ intercept)\n",
    "    predicts the original y.\n",
    "\n",
    "    Args:\n",
    "        dim: attribute dimension name, e.g. \"attractive\"\n",
    "        alpha: L2 penalty for Ridge\n",
    "        device: torch.device to place the returned tensor on (defaults to CUDA if available, else CPU)\n",
    "        fit_intercept: whether to fit an intercept in Ridge (does not affect returned coef shape)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_features,) with dtype float32 on `device`\n",
    "    \"\"\"\n",
    "    # Full set for this dimension (get_data already returns y.nanmean(dim=1))\n",
    "    X, y, _, _ = get_data(dim, train=True)\n",
    "\n",
    "    # To NumPy (fast path using your helpers)\n",
    "    X_np, y_np = cpu_numpy(X, y)\n",
    "\n",
    "    # Fit Ridge on -y (to match your correlation code’s convention)\n",
    "    model = Ridge(alpha=alpha, fit_intercept=fit_intercept)\n",
    "    model.fit(X_np, y_np)\n",
    "\n",
    "    # Coefficients for predicting original y (negate back), as float32 tensor on desired device\n",
    "    coefs_np = (model.coef_).astype(np.float32, copy=False)\n",
    "    coefs_t = torch.from_numpy(coefs_np).to(device)\n",
    "    # print(f\"Pearson correlation: {pearsonr(y_np, X_np @ coefs_np)[0]}\")\n",
    "    return coefs_t\n",
    "\n",
    "# Example:\n",
    "coefs = ridge_coefs(\"attractive\", alpha=100)\n",
    "\n",
    "\n",
    "dim = \"attractive\"\n",
    "train_X, train_y, train_weights, train_imgs = cpu_numpy(*get_data(dim, train=True))\n",
    "test_X, test_y, test_weights, test_imgs = cpu_numpy(*get_data(dim, train=False))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define a very detailed range of L2 penalties (alphas)\n",
    "l2_penalties = 10**(np.linspace(-2, 4, 500))  # 200 values from 1e-6 to 1e4\n",
    "\n",
    "\n",
    "def find_best_alpha(train_X, train_y, test_X, test_y, test_weights):\n",
    "    best_alpha = None\n",
    "    best_score = float('inf')\n",
    "    best_ridge = None\n",
    "    best_corr = 0\n",
    "    best_train_corr = 0\n",
    "    print_every = len(l2_penalties)//10\n",
    "    for i,alpha in enumerate(l2_penalties):\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(train_X, -train_y)\n",
    "        preds = -ridge.predict(test_X)\n",
    "        score = mean_squared_error(test_y, preds, sample_weight=test_weights)\n",
    "        corr = pearsonr(test_y, preds)[0]\n",
    "        train_corr = pearsonr(train_y, -ridge.predict(train_X))[0]\n",
    "        if i%print_every==0:\n",
    "            print(f\"alpha: {alpha:.2f}, corr: {corr:.4f}\")\n",
    "        if best_corr < corr:\n",
    "            best_score = score\n",
    "            best_alpha = alpha\n",
    "            best_ridge = ridge\n",
    "            best_corr = corr\n",
    "            best_train_corr = train_corr\n",
    "    print(f\"Best L2 penalty (alpha): {best_alpha}\")\n",
    "    print(f\"Best test MSE: {best_score}\")\n",
    "    print(f\"Its test Pearson correlation: {best_corr}\")\n",
    "    print(f\"Its train Pearson correlation: {best_train_corr}\")\n",
    "    return best_alpha, best_score, best_corr, best_train_corr\n",
    "\n",
    "\n",
    "\n",
    "find_best_alpha(train_X, train_y, test_X, test_y, test_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e8812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(coefs.shape, coefs.device, coefs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fb231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bafb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de33175",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'levelset_viz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlevelset_viz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Projection2D, sample_and_fields\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlevelset_viz_plotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plotly_viewer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'levelset_viz'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe300476",
   "metadata": {},
   "source": [
    "# Train prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3323b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n",
    "\n",
    "class CosineScheduleWithWarmup(_LRScheduler):\n",
    "    \"\"\"\n",
    "    A custom learning rate scheduler that implements a linear warmup followed\n",
    "    by a cosine decay. This avoids the need for the `transformers` library.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, num_warmup_steps: int, num_training_steps: int, last_epoch: int = -1):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.num_warmup_steps:\n",
    "            progress = float(self.last_epoch) / float(max(1, self.num_warmup_steps))\n",
    "            return [base_lr * progress for base_lr in self.base_lrs]\n",
    "        \n",
    "        progress = float(self.last_epoch - self.num_warmup_steps) / float(max(1, self.num_training_steps - self.num_warmup_steps))\n",
    "        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "        \n",
    "        return [base_lr * cosine_decay for base_lr in self.base_lrs]\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
    "    return CosineScheduleWithWarmup(optimizer, num_warmup_steps, num_training_steps, last_epoch)\n",
    "    \n",
    "\n",
    "# photo_to_coords, dim_to_photo_to_ratings are already loaded\n",
    "device=torch.device('mps')\n",
    "# Exclude 'Unnamed: 0', 'stimulus', and 'loss' from target variables\n",
    "target_columns = [col for col in df.columns if col not in ['Unnamed: 0', 'stimulus', 'loss', 'dlatents']]\n",
    "sys.path.append('/Users/adamsobieszek/PycharmProjects/psychGAN/content/psychGAN/stylegan3')\n",
    "with open('/Users/adamsobieszek/PycharmProjects/psychGAN/stylegan2-ffhq-1024x1024.pkl', 'rb') as fp:\n",
    "    G = pickle.load(fp)['G_ema'].to(device)\n",
    "\n",
    "# Data Preparation\n",
    "w_avg = G.mapping.w_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c27584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Trainer (name preserved), stripped to essentials ---\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler, criterion, device, config, model_save_path, regularization=None, regularization_weight=None ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.model_save_path = model_save_path\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def validate(self, val_dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss_val = 0.0\n",
    "        all_pred_mu, all_true_mu, all_w = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y, w in val_dataloader:\n",
    "                x, y, w = x.to(self.device), y.to(self.device), w.to(self.device)\n",
    "                y = y.nanmean(dim=1,keepdim=True)\n",
    "    \n",
    "                pred_mu = self.model(x)\n",
    "                val_loss = self.criterion(pred_mu, y)#, sample_weights=w, n_obs=(~torch.isnan(y)).sum(1))\n",
    "                total_loss_val += val_loss.item()\n",
    "\n",
    "                mask = ~torch.isnan(y)\n",
    "                true_mu = y\n",
    "                all_pred_mu.append(pred_mu.cpu())\n",
    "                all_true_mu.append(true_mu.cpu())\n",
    "                all_w.append(w.view(-1, 1).cpu())\n",
    "\n",
    "        pred = torch.cat(all_pred_mu).numpy().flatten()\n",
    "        true = torch.cat(all_true_mu).numpy().flatten()\n",
    "        wts  = torch.cat(all_w).numpy().flatten()\n",
    "        wts  = wts / (wts.mean() + 1e-12)\n",
    "\n",
    "        pred_l = np.log(np.clip(pred, 1e-6, 1-1e-6)) - np.log(np.clip(1-pred, 1e-6, 1-1e-6))\n",
    "        true_l = np.log(np.clip(true, 1e-6, 1-1e-6)) - np.log(np.clip(1-true, 1e-6, 1-1e-6))\n",
    "        mean_corr = np.corrcoef(pred, true)[0,1] if pred.std()>0 and true.std()>0 else 0.0\n",
    "        logit_corr = np.corrcoef(pred_l, true_l)[0,1] if pred_l.std()>0 and true_l.std()>0 else 0.0\n",
    "\n",
    "        logit_mse = np.mean(wts * (pred_l - true_l)**2)\n",
    "        logit_mse_unweighted = np.mean((pred_l - true_l)**2)\n",
    "\n",
    "        return dict(mean_val_loss=total_loss_val/len(val_dataloader),\n",
    "                    mean_corr=mean_corr,\n",
    "                    logit_corr=logit_corr,\n",
    "                    logit_mse=logit_mse,\n",
    "                    logit_mse_unweighted=logit_mse_unweighted)\n",
    "\n",
    "    def train(self, train_dataloader, val_dataloader):\n",
    "        epochs = self.config['training']['epochs']\n",
    "        logging.info(f\"Starting training for {epochs} epochs...\")\n",
    "\n",
    "        for epoch_num in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for x, y, weights in train_dataloader:\n",
    "                x, y, weights = x.to(self.device), y.to(self.device), weights.to(self.device)\n",
    "                y = torch.logit(y.nanmean(dim=1,keepdim=True))\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                outputs = self.model(x)\n",
    "                loss = self.criterion(outputs, y)#, sample_weights=weights, n_obs=(~torch.isnan(y)).sum(1))\n",
    "                if torch.isnan(loss):\n",
    "                    continue\n",
    "                loss.backward()\n",
    "                if self.config['training']['grad_clip_norm'] is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.config['training']['grad_clip_norm'])\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_loss / max(1, len(train_dataloader))\n",
    "\n",
    "            if (epoch_num + 1) % self.config['logging_and_saving']['validation_freq'] == 0:\n",
    "                log = self.validate(val_dataloader)\n",
    "                mean_val_loss = log[\"logit_mse\"]  # keep same early-stopping proxy\n",
    "\n",
    "                if mean_val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = mean_val_loss\n",
    "                    os.makedirs(os.path.dirname(self.model_save_path), exist_ok=True)\n",
    "                    torch.save(self.model.state_dict(), self.model_save_path)\n",
    "                    logging.info(f\"Epoch {epoch_num + 1}: New best (Val Corr: {log['mean_corr']:.7f})\")\n",
    "\n",
    "                if (epoch_num + 1) % self.config['logging_and_saving']['print_freq'] == 0:\n",
    "                    logging.info(\n",
    "                        f'Epoch {epoch_num + 1}/{epochs} | '\n",
    "                        f'Train Loss: {avg_train_loss:.4f} | '\n",
    "                        f'Val Loss: {log[\"mean_val_loss\"]:.4f} | '\n",
    "                        f'mean_corr: {log[\"mean_corr\"]:.4f} | '\n",
    "                        f'logit_corr: {log[\"logit_corr\"]:.4f} | '\n",
    "                        f'logit_mse: {log[\"logit_mse\"]:.4f}'\n",
    "                    )\n",
    "\n",
    "        logging.info(f\"Training finished. Best validation loss: {self.best_val_loss:.4f}\")\n",
    "        try:\n",
    "            self.model.load_state_dict(torch.load(self.model_save_path))\n",
    "            logging.info(f\"Loaded best model from {self.model_save_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not load the best model. Error: {e}\")\n",
    "        return self.model, self.best_val_loss\n",
    "\n",
    "# --- Data Preparation (name preserved; simplified for speed) ---\n",
    "def pad_list(l, max_len):\n",
    "    return l[:max_len] + [np.nan] * (max_len - len(l[:max_len]))\n",
    "\n",
    "def prepare_data2(config: dict, verbose=False):\n",
    "    xs, ys = photo_to_coords, dim_to_photo_to_ratings[config['data']['attribute_dim']]\n",
    "    imgs = set(ys.keys()).intersection(set(xs.keys())) #- set('638.jpg')\n",
    "    imgs = imgs.intersection(set([f\"{i}.jpg\" for i in range(1,1005)]))\n",
    "\n",
    "    X = torch.stack([xs[k] for k in imgs]).to(device, DTYPE)\n",
    "    max_len = int(max(len(ys[k]) for k in imgs) * config['data'].get('rating_oversampling_factor', 1.0))\n",
    "    y = torch.tensor([pad_list(ys[k], max_len) for k in imgs], device=device, dtype=DTYPE)\n",
    "\n",
    "    n = y.shape[1] - y.isnan().sum(dim=1)\n",
    "    reject_by_n = n < config['data'].get('min_ratings', 10)\n",
    "    X = X[~reject_by_n]\n",
    "    y = y[~reject_by_n]\n",
    "    n = n[~reject_by_n]\n",
    "\n",
    "    # Fast weights: proportional to sample size, normalized\n",
    "    sample_size_weights = n.to(torch.float32).clamp(max=30.0)\n",
    "    sample_size_weights = sample_size_weights / (sample_size_weights.mean() + 1e-12)\n",
    "\n",
    "    return X, y, sample_size_weights\n",
    "\n",
    "# -----------------------\n",
    "# Device & Config\n",
    "# -----------------------\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "DTYPE = torch.float32\n",
    "\n",
    "class SmallMeanRegressor(nn.Module):\n",
    "    def __init__(self, latent_dim=512, target_dim=1, encoder=lambda x: x-w_avg):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(512, target_dim),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "        self.encoder = encoder\n",
    "        if isinstance(encoder, nn.Module):\n",
    "            self.encoder = encoder.to(device).eval().requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(self.encoder(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# WavePDE Training (one potential per target)\n",
    "# =======================\n",
    "import os, math, logging, numpy as np, torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import grad\n",
    "from torch.autograd.functional import jvp\n",
    "\n",
    "\n",
    "\n",
    "def _safe_logit_(y: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    y = y.clamp(eps, 1.0 - eps)\n",
    "    return torch.log(y) - torch.log(1.0 - y)\n",
    "\n",
    "\n",
    "def dims_from_ratings():\n",
    "    \"\"\"Stable ordered list of attribute dims present in dim_to_photo_to_ratings.\"\"\"\n",
    "    return sorted(list(dim_to_photo_to_ratings.keys()))\n",
    "\n",
    "def prepare_multitask_means(config: dict, dims: list[str] | None = None, min_proportion: float = 1.0, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Build X [N,512], Y [N,D] with per-photo mean rating for each dim (NaN if missing/too few),\n",
    "    using sparse tensor accumulation (no per-photo inner loop).\n",
    "\n",
    "    Returns:\n",
    "        X:        torch.Tensor [N, 512] on `device`\n",
    "        Y:        torch.Tensor [N, D]   on `device`, NaN where < min_ratings\n",
    "        weights:  torch.Tensor [N, D]   on `device`, per-dim, zeros where Y is NaN\n",
    "        meta:     dict with 'photos' and 'dims'\n",
    "    \"\"\"\n",
    "    xs = photo_to_coords\n",
    "    if dims is None:\n",
    "        dims = sorted(list(dim_to_photo_to_ratings.keys()))\n",
    "    D = len(dims)\n",
    "\n",
    "    # photos_all: photos that appear in >10% of dims\n",
    "    photo_counts = {}\n",
    "    num_dims = len(dims)\n",
    "    for d in dims:\n",
    "        for p in dim_to_photo_to_ratings[d].keys():\n",
    "            photo_counts[p] = photo_counts.get(p, 0) + 1\n",
    "    min_dims = max(1, int(np.ceil(min_proportion * num_dims)))\n",
    "    photos_all = {p for p, c in photo_counts.items() if c >= min_dims}\n",
    "    photos = sorted(list(photos_all.intersection(set(xs.keys()))))\n",
    "    N = len(photos)\n",
    "\n",
    "    X = torch.stack([xs[p] for p in photos]).to(device=device, dtype=DTYPE)\n",
    "\n",
    "    photo_index = {p: i for i, p in enumerate(photos)}\n",
    "    min_ratings = int(config[\"data\"].get(\"min_ratings\", 10))\n",
    "\n",
    "    # ---- Build sparse COO buffers for SUMS and COUNTS over the (N,D) grid ----\n",
    "    row_idx_chunks, col_idx_chunks, sum_chunks, cnt_chunks = [], [], [], []\n",
    "    for j, d in enumerate(dims):\n",
    "        table = dim_to_photo_to_ratings[d]\n",
    "        if not table:\n",
    "            continue\n",
    "        keys = list(table.keys())\n",
    "        rows = np.fromiter((photo_index.get(k, -1) for k in keys), count=len(keys), dtype=np.int64)\n",
    "        values_list = list(table.values())\n",
    "        cnts = np.fromiter((len(v) for v in values_list), count=len(values_list), dtype=np.int64)\n",
    "        sums = np.fromiter((float(np.sum(v)) if len(v) > 0 else 0.0 for v in values_list),\n",
    "                           count=len(values_list), dtype=np.float32)\n",
    "        mask = (rows >= 0) & (cnts > 0)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        rows = rows[mask]; cnts = cnts[mask]; sums = sums[mask]\n",
    "        cols = np.full(rows.shape[0], j, dtype=np.int64)\n",
    "        row_idx_chunks.append(rows); col_idx_chunks.append(cols)\n",
    "        cnt_chunks.append(cnts.astype(np.float32)); sum_chunks.append(sums.astype(np.float32))\n",
    "\n",
    "    if len(row_idx_chunks) == 0:\n",
    "        Y = torch.full((N, D), float('nan'), dtype=DTYPE, device=device)\n",
    "        weights = torch.zeros((N, D), dtype=DTYPE, device=device)\n",
    "        meta = {\"photos\": photos, \"dims\": dims}\n",
    "        return X, Y, weights, meta\n",
    "\n",
    "    rows = np.concatenate(row_idx_chunks)\n",
    "    cols = np.concatenate(col_idx_chunks)\n",
    "    cnts = np.concatenate(cnt_chunks)\n",
    "    sums = np.concatenate(sum_chunks)\n",
    "\n",
    "    indices = torch.tensor(np.vstack([rows, cols]), dtype=torch.long, device=\"cpu\")\n",
    "    val_counts = torch.from_numpy(cnts).to(dtype=torch.float32)\n",
    "    val_sums   = torch.from_numpy(sums).to(dtype=torch.float32)\n",
    "\n",
    "    sparse_counts = torch.sparse_coo_tensor(indices, val_counts, size=(N, D)).coalesce()\n",
    "    sparse_sums   = torch.sparse_coo_tensor(indices, val_sums,   size=(N, D)).coalesce()\n",
    "\n",
    "    counts_dense = sparse_counts.to_dense()  # [N, D] CPU float32\n",
    "    sums_dense   = sparse_sums.to_dense()    # [N, D] CPU float32\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Y_cpu = torch.full((N, D), float('nan'), dtype=torch.float32)\n",
    "        valid_mask = counts_dense >= float(min_ratings)\n",
    "        print(f\"valid_mask: {(~valid_mask).sum()}\")\n",
    "        Y_cpu[valid_mask] = (sums_dense[valid_mask] / counts_dense[valid_mask])\n",
    "\n",
    "    per_sample_valid = torch.isfinite(Y_cpu).sum(dim=1).to(dtype=torch.float32)\n",
    "    keep_idx = torch.nonzero(per_sample_valid > 0, as_tuple=False).squeeze(1)\n",
    "    if keep_idx.numel() == 0:\n",
    "        Y = torch.full((0, D), float('nan'), dtype=DTYPE, device=device)\n",
    "        X = X[:0]\n",
    "        weights = torch.empty((0, D), dtype=DTYPE, device=device)\n",
    "        meta = {\"photos\": [], \"dims\": dims}\n",
    "        if verbose:\n",
    "            print(\"[prepare_multitask_means] No samples meet the min_ratings threshold.\")\n",
    "        return X, Y, weights, meta\n",
    "\n",
    "    # Keep rows with at least one valid dim\n",
    "    X = X.index_select(0, keep_idx.to(device))\n",
    "    Y = Y_cpu.index_select(0, keep_idx).to(device=device, dtype=DTYPE)\n",
    "\n",
    "    # --- Weighting to [N_kept, D] ---\n",
    "    data_cfg = config.get(\"data\", {})\n",
    "    mode = str(data_cfg.get(\"weighting\", \"ikd_per_dim\")).lower()\n",
    "    kde_bandwidth = float(data_cfg.get(\"kde_bandwidth\", 0.15))\n",
    "\n",
    "    Y_kept_cpu = Y_cpu.index_select(0, keep_idx)              # [N_kept, D] CPU float32 with NaNs\n",
    "    counts_kept = counts_dense.index_select(0, keep_idx)      # [N_kept, D] CPU float32\n",
    "    N_kept = Y_kept_cpu.shape[0]\n",
    "    W_cpu = torch.zeros_like(Y_kept_cpu, dtype=torch.float32) # [N_kept, D], zeros for invalids\n",
    "\n",
    "    if mode in (\"counts\", \"sample_size\", \"num_ratings\"):\n",
    "        # Per-dim counts where valid, else 0\n",
    "        valid = torch.isfinite(Y_kept_cpu)\n",
    "        W_cpu = torch.where(valid, counts_kept, torch.zeros_like(counts_kept))\n",
    "        # Normalize per-dim to mean 1 over valid entries\n",
    "        col_sums = W_cpu.sum(dim=0)\n",
    "        col_cnts = (W_cpu > 0).sum(dim=0).clamp(min=1)\n",
    "        col_means = col_sums / col_cnts\n",
    "        W_cpu = W_cpu / (col_means.clamp(min=1e-12))\n",
    "\n",
    "    elif mode in (\"ikd\", \"ikd_per_dim\", \"inverse_kde\", \"inverse_kde_per_dim\"):\n",
    "        try:\n",
    "            from sklearn.neighbors import KernelDensity\n",
    "            for j in range(D):\n",
    "                col = Y_kept_cpu[:, j]\n",
    "                valid = torch.isfinite(col)\n",
    "                n_valid = int(valid.sum().item())\n",
    "                if n_valid >= 2:\n",
    "                    vals = col[valid].cpu().numpy().reshape(-1, 1)\n",
    "                    kde = KernelDensity(kernel='gaussian', bandwidth=kde_bandwidth).fit(vals)\n",
    "                    log_dens = kde.score_samples(vals)\n",
    "                    inv = torch.from_numpy(np.exp(-log_dens)).to(dtype=torch.float32)\n",
    "                    W_cpu[valid, j] = inv\n",
    "                elif n_valid == 1:\n",
    "                    # Single point -> uniform weight 1\n",
    "                    W_cpu[valid, j] = 1.0\n",
    "                # else: remains 0 for this dim (no valid labels)\n",
    "            # Normalize per-dim to mean 1 over valid entries\n",
    "            col_sums = W_cpu.sum(dim=0)\n",
    "            col_cnts = (W_cpu > 0).sum(dim=0).clamp(min=1)\n",
    "            col_means = col_sums / col_cnts\n",
    "            W_cpu = W_cpu / (col_means.clamp(min=1e-12))\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"[prepare_multitask_means] IKD error, falling back to 'counts': {e}\")\n",
    "            valid = torch.isfinite(Y_kept_cpu)\n",
    "            W_cpu = torch.where(valid, counts_kept, torch.zeros_like(counts_kept))\n",
    "            col_sums = W_cpu.sum(dim=0)\n",
    "            col_cnts = (W_cpu > 0).sum(dim=0).clamp(min=1)\n",
    "            col_means = col_sums / col_cnts\n",
    "            W_cpu = W_cpu / (col_means.clamp(min=1e-12))\n",
    "\n",
    "    elif mode in (\"ikd_global\", \"inverse_kde_global\"):\n",
    "        try:\n",
    "            from sklearn.neighbors import KernelDensity\n",
    "            y_mean = torch.nanmean(Y_kept_cpu, dim=1)  # [N_kept]\n",
    "            valid = torch.isfinite(y_mean)\n",
    "            n_valid = int(valid.sum().item())\n",
    "            if n_valid >= 2:\n",
    "                vals = y_mean[valid].cpu().numpy().reshape(-1, 1)\n",
    "                kde = KernelDensity(kernel='gaussian', bandwidth=kde_bandwidth).fit(vals)\n",
    "                log_dens = kde.score_samples(vals)\n",
    "                inv = torch.from_numpy(np.exp(-log_dens)).to(dtype=torch.float32)\n",
    "                w = torch.zeros(N_kept, dtype=torch.float32); w[valid] = inv\n",
    "            elif n_valid == 1:\n",
    "                w = torch.zeros(N_kept, dtype=torch.float32); w[valid] = 1.0\n",
    "            else:\n",
    "                w = torch.zeros(N_kept, dtype=torch.float32)\n",
    "            W_cpu = w.unsqueeze(1).repeat(1, D)\n",
    "            # Normalize per-dim to mean 1 over valid entries\n",
    "            col_sums = W_cpu.sum(dim=0)\n",
    "            col_cnts = (torch.isfinite(Y_kept_cpu)).sum(dim=0).clamp(min=1).to(col_sums.dtype)\n",
    "            col_means = col_sums / col_cnts\n",
    "            W_cpu = W_cpu / (col_means.clamp(min=1e-12))\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"[prepare_multitask_means] IKD global error, falling back to 'counts': {e}\")\n",
    "            valid = torch.isfinite(Y_kept_cpu)\n",
    "            W_cpu = torch.where(valid, counts_kept, torch.zeros_like(counts_kept))\n",
    "            col_sums = W_cpu.sum(dim=0)\n",
    "            col_cnts = (W_cpu > 0).sum(dim=0).clamp(min=1)\n",
    "            col_means = col_sums / col_cnts\n",
    "            W_cpu = W_cpu / (col_means.clamp(min=1e-12))\n",
    "\n",
    "    else:\n",
    "        valid = torch.isfinite(Y_kept_cpu)\n",
    "        W_cpu = torch.where(valid, counts_kept, torch.zeros_like(counts_kept))\n",
    "        col_sums = W_cpu.sum(dim=0)\n",
    "        col_cnts = (W_cpu > 0).sum(dim=0).clamp(min=1)\n",
    "        col_means = col_sums / col_cnts\n",
    "        W_cpu = W_cpu / (col_means.clamp(min=1e-12))\n",
    "\n",
    "    weights = W_cpu.to(device=device, dtype=DTYPE)\n",
    "\n",
    "    kept_photos = [photos[i] for i in keep_idx.tolist()]\n",
    "    meta = {\"photos\": kept_photos, \"dims\": dims}\n",
    "    if verbose:\n",
    "        avg_valid = float(per_sample_valid[keep_idx].mean().item())\n",
    "        print(f\"[prepare_multitask_means] N={X.shape[0]}, D={D}, avg_valid_dims_per_photo={avg_valid:.2f}\")\n",
    "    return X, Y, weights, meta\n",
    "# --- Model: Representation encoder (512->512) + linear probes (512->D) ---\n",
    "\n",
    "class RepresentationEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    f: R^512 -> R^512 representation. Similar size/shape to your SmallMeanRegressor body,\n",
    "    but outputs a vector (no sigmoid), optionally centered by w_avg for StyleGAN W-space.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=512, rep_dim=512, dropout=0.1, center_w_avg: bool = True):\n",
    "        super().__init__()\n",
    "        self.center_w_avg = center_w_avg\n",
    "        if center_w_avg:\n",
    "            # Keep compatibility with your prior designs (subtract StyleGAN w_avg)\n",
    "            self.register_buffer('w_avg', w_avg)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, rep_dim),  # final representation (linear)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.center_w_avg:\n",
    "            x = x - self.w_avg\n",
    "        z = self.net(x)\n",
    "        return z\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ProbeOptimizedRep(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines the representation encoder with a bank of linear probes.\n",
    "    The HEAD is strictly linear, meeting the \"linear probe\" requirement.\n",
    "    Forward returns logits (not squashed) of shape [B, D] and the representation z.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=512, rep_dim=512, num_tasks: int = 1, center_w_avg: bool = True):\n",
    "        super().__init__()\n",
    "        self.encoder = RepresentationEncoder(latent_dim=latent_dim, rep_dim=rep_dim, center_w_avg=center_w_avg)\n",
    "        self.probes  = nn.Linear(rep_dim, num_tasks, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        logits = self.probes(z)     # [B, D] linear predictions in logit space\n",
    "        return logits, z\n",
    "\n",
    "    @staticmethod\n",
    "    def logits_to_probs(logits):\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_probes_with_ols(\n",
    "        self,\n",
    "        dataloader,\n",
    "        device=None,\n",
    "        ridge: float = 1e-4,\n",
    "        use_logit: bool = True,\n",
    "        max_batches: int | None = None,\n",
    "        verbose: bool = True,\n",
    "        noise_scale: float = 0.001  \n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Fit per-dimension OLS (with optional ridge, sample weights, and logit targets)\n",
    "        for the linear probe head using the encoder's current features.\n",
    "\n",
    "        dataloader: yields (x, y, w) with y in [0,1] and NaNs for missing dims, w is [B] weights.\n",
    "        ridge:      L2 regularization strength (applied to weights but NOT to intercept).\n",
    "        use_logit:  regress on logit(y) for a better-behaved squared loss.\n",
    "        max_batches: limit for speed; None uses all batches.\n",
    "\n",
    "        Returns a small summary dict (counts per dim).\n",
    "        \"\"\"\n",
    "        dev = device or next(self.parameters()).device\n",
    "        self.eval()\n",
    "\n",
    "        Z_chunks, Y_chunks, W_chunks = [], [], []\n",
    "        n_seen = 0\n",
    "\n",
    "        for b, (x, y, w) in enumerate(dataloader):\n",
    "            if (max_batches is not None) and (b >= max_batches):\n",
    "                break\n",
    "            x = x.to(dev)\n",
    "            y = y.to(dev)\n",
    "            w = w.to(dev)\n",
    "\n",
    "            # Encode features (representation) — use current encoder\n",
    "            logits_or_rep = self.encoder(x)  # shape [B, rep_dim]\n",
    "            z = logits_or_rep  # representation\n",
    "\n",
    "            Z_chunks.append(z.detach().cpu())\n",
    "            Y_chunks.append(y.detach().cpu())\n",
    "            W_chunks.append(w.detach().cpu())\n",
    "            n_seen += x.shape[0]\n",
    "\n",
    "        if n_seen == 0:\n",
    "            if verbose:\n",
    "                print(\"[init_probes_with_ols] No data seen; probes unchanged.\")\n",
    "            return {\"used_samples\": 0, \"dims_fitted\": 0}\n",
    "\n",
    "        Z = torch.cat(Z_chunks, dim=0).to(dtype=torch.float64)         # [N, d]\n",
    "        Y = torch.cat(Y_chunks, dim=0).to(dtype=torch.float64)         # [N, D]\n",
    "        W = torch.cat(W_chunks, dim=0).to(dtype=torch.float64)         # [N]\n",
    "        N, d = Z.shape\n",
    "        D = Y.shape[1]\n",
    "        # Convert to logit\n",
    "        if use_logit:\n",
    "            Y = _safe_logit_(Y)\n",
    "\n",
    "        # Add intercept by augmenting features with ones column\n",
    "        ones = torch.ones((N, 1), dtype=torch.float64)\n",
    "        Z_aug = torch.cat([Z, ones], dim=1)                             # [N, d+1]\n",
    "        d_aug = d + 1\n",
    "\n",
    "        # We will solve per-dimension weighted ridge regression:\n",
    "        #   argmin_beta || diag(sqrt(w)) (Z_aug beta - y) ||^2 + ridge * ||R beta||^2\n",
    "        # where R = diag([1,...,1,0]) (no penalty on intercept).\n",
    "        # Build the ridge matrix once.\n",
    "        R = torch.eye(d_aug, dtype=torch.float64)\n",
    "        R[-1, -1] = 0.0  # do not regularize intercept\n",
    "\n",
    "        # Precompute nothing global because of per-dim masking;\n",
    "        # we’ll mask rows per dim and solve normal equations stably.\n",
    "        weight_summaries = []\n",
    "        dims_fitted = 0\n",
    "\n",
    "        new_W = torch.zeros((d, D), dtype=torch.float64)  # weights without intercept\n",
    "        new_b = torch.zeros((D,), dtype=torch.float64)    # intercepts\n",
    "\n",
    "        for j in range(D):\n",
    "            yj = Y[:, j]\n",
    "            mask = torch.isfinite(yj)\n",
    "            n_j = int(mask.sum().item())\n",
    "            if n_j < 2:\n",
    "                # Not enough data; leave zeros\n",
    "                weight_summaries.append((j, n_j, False))\n",
    "                continue\n",
    "\n",
    "            Zj = Z_aug[mask]                  # [n_j, d+1]\n",
    "            bj = yj[mask]                     # [n_j]\n",
    "            wj = W[mask, j].clamp_min(1e-12)     # [n_j, d]\n",
    "            sj = torch.sqrt(wj)               # weights via left-multiplication\n",
    "\n",
    "            # Weighted design / targets\n",
    "            A = Zj * sj.unsqueeze(1)          # [n_j, d+1]\n",
    "            b_vec = bj * sj                   # [n_j]\n",
    "\n",
    "            # Normal equations with ridge on weights (not on intercept)\n",
    "            AtA = A.T @ A                     # [d+1, d+1]\n",
    "            Atb = A.T @ b_vec                 # [d+1]\n",
    "\n",
    "            # Stabilize\n",
    "            AtA = AtA + ridge * R\n",
    "\n",
    "            # Solve (SPD) system\n",
    "            try:\n",
    "                beta = torch.linalg.solve(AtA, Atb)      # [d+1]\n",
    "            except RuntimeError:\n",
    "                # Fallback to lstsq if needed\n",
    "                beta = torch.linalg.lstsq(AtA, Atb.unsqueeze(1)).solution.squeeze(1)\n",
    "\n",
    "            new_W[:, j] = beta[:-1]           # weights\n",
    "            new_b[j] = beta[-1]               # intercept\n",
    "            dims_fitted += 1\n",
    "            weight_summaries.append((j, n_j, True))\n",
    "        \n",
    "        # Write back to the probe head (convert dtype/device properly)\n",
    "        new_W = new_W.to(device=self.probes.weight.device, dtype=self.probes.weight.dtype).T\n",
    "        new_W = new_W+torch.randn_like(new_W)*noise_scale\n",
    "        self.probes.weight.copy_(new_W)\n",
    "        self.probes.bias.copy_(new_b.to(device=self.probes.bias.device, dtype=self.probes.bias.dtype))\n",
    "\n",
    "        if verbose:\n",
    "            fitted = sum(int(ok) for _, _, ok in weight_summaries)\n",
    "            skipped = D - fitted\n",
    "            print(f\"[init_probes_with_ols] Fitted {fitted}/{D} dims using {n_seen} samples \"\n",
    "                  f\"(ridge={ridge}, logit={use_logit})\")\n",
    "\n",
    "        return {\n",
    "            \"used_samples\": int(N),\n",
    "            \"dims_fitted\": int(dims_fitted),\n",
    "            \"per_dim_counts\": {int(j): int(n) for j, n, _ in weight_summaries},\n",
    "        }\n",
    "        \n",
    "class HouseholderProduct(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient product of K Householder reflections.\n",
    "    Applies an orthogonal transform O(x) = H_K ... H_1 x, each H_i = I - 2 v v^T with ||v||=1.\n",
    "    Cost: O(K*d) per forward (no large matrix materialization).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, num_reflections: int = 8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_reflections = num_reflections\n",
    "        if num_reflections > 0:\n",
    "            # Unconstrained parameters; normalized per reflection on forward\n",
    "            self.v_raw = nn.Parameter(torch.randn(num_reflections, dim) * 0.01)\n",
    "        else:\n",
    "            self.register_parameter('v_raw', None)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.num_reflections == 0:\n",
    "            return x\n",
    "        # Normalize v’s once per forward\n",
    "        v = F.normalize(self.v_raw, dim=1, eps=1e-8)  # [K, d]\n",
    "        y = x\n",
    "        # Apply reflections sequentially: y <- y - 2 * (y·v) v\n",
    "        # (y·v) -> [B,K] via (y @ v^T)\n",
    "        # Then accumulate per reflection.\n",
    "        # To keep memory small, apply one-by-one (still vectorized over batch).\n",
    "        for k in range(self.num_reflections):\n",
    "            vk = v[k]                        # [d]\n",
    "            proj = (y * vk).sum(dim=1, keepdim=True)  # [B,1]\n",
    "            y = y - 2.0 * proj * vk         # [B,d]\n",
    "        return y\n",
    "    \n",
    "class ZeroInitResidual(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual MLP with zero-initialized final layer and a learnable scalar gate.\n",
    "    Ensures f(0)=0 and starts with exact identity (no residual).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=512, hidden=1024, depth=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = dim\n",
    "        for _ in range(depth):\n",
    "            lin = nn.Linear(in_dim, hidden)\n",
    "            nn.init.kaiming_uniform_(lin.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(lin.bias)\n",
    "            layers += [lin, nn.ReLU()]\n",
    "            if dropout > 0:\n",
    "                layers += [nn.Dropout(dropout)]\n",
    "            in_dim = hidden\n",
    "\n",
    "        self.body = nn.Sequential(*layers) if layers else nn.Identity()\n",
    "        self.out = nn.Linear(in_dim, dim)\n",
    "        # Zero-init last layer = “zero conv”\n",
    "        nn.init.zeros_(self.out.weight)\n",
    "        nn.init.zeros_(self.out.bias)\n",
    "\n",
    "        # Learnable scalar gate α, start at 0 => pure identity\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = self.out(self.body(x))\n",
    "        return self.alpha * r\n",
    "\n",
    "class RepresentationEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    f: R^512 -> R^512 that starts as Identity and stays near-isometric.\n",
    "    z = S( O(x - w_avg) ) + Residual(z)   with:\n",
    "      - O: product of Householder reflections (exactly orthogonal), gated toward identity\n",
    "      - S: learnable diagonal scale (log-scale initialized to 0)\n",
    "      - Residual: zero-init MLP with gated output (zero at init)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 latent_dim=512,\n",
    "                 rep_dim=512,\n",
    "                 dropout=0.1,\n",
    "                 center_w_avg: bool = True,\n",
    "                 num_reflections: int = 8,\n",
    "                 use_scale: bool = True,\n",
    "                 residual_hidden: int = 1024,\n",
    "                 residual_depth: int = 2,\n",
    "                 residual_dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert rep_dim == latent_dim, \"For a geometry-preserving transform, set rep_dim == latent_dim (512).\"\n",
    "        self.center_w_avg = center_w_avg\n",
    "        if center_w_avg:\n",
    "            self.register_buffer('w_avg', w_avg)\n",
    "\n",
    "        self.dim = latent_dim\n",
    "\n",
    "        # Orthogonal module + gate (blend with identity)\n",
    "        self.ortho = HouseholderProduct(self.dim, num_reflections=num_reflections)\n",
    "        self.ortho_gain = nn.Parameter(torch.tensor(0.0))  # 0 => identity, 1 => full orthogonal\n",
    "\n",
    "        # Diagonal scale (log-scale = 0 => identity)\n",
    "        self.use_scale = use_scale\n",
    "        if use_scale:\n",
    "            self.log_scale = nn.Parameter(torch.zeros(self.dim))\n",
    "\n",
    "        # Zero-init residual (zero by default)\n",
    "        self.residual = ZeroInitResidual(dim=self.dim, hidden=residual_hidden,\n",
    "                                         depth=residual_depth, dropout=residual_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1) center\n",
    "        if self.center_w_avg:\n",
    "            x = x - self.w_avg\n",
    "\n",
    "        # 2) orthogonal blend: x + g * (O(x) - x)\n",
    "        if self.ortho.num_reflections > 0:\n",
    "            ox = self.ortho(x)\n",
    "            g = torch.tanh(self.ortho_gain)  # keep gate in (-1,1) for stability\n",
    "            x = x + g * (ox - x)             # identity at init\n",
    "\n",
    "        # 3) scale\n",
    "        if self.use_scale:\n",
    "            x = x * torch.exp(self.log_scale)\n",
    "\n",
    "        # 4) residual (zero at init)\n",
    "        x = x + self.residual(x)\n",
    "\n",
    "        # z is the representation (zero-centered at w_avg)\n",
    "        return x\n",
    "\n",
    "    # ---- Optional: StyleGAN-like path-length penalty helper ----\n",
    "    def path_length_penalty(self, x, pl_target=1.0, pl_batch_shrink=2):\n",
    "        \"\"\"\n",
    "        Directional Jacobian norm penalty: E[(||J·n|| - pl_target)^2], n ~ N(0,I)/sqrt(d).\n",
    "        Use on a (possibly) smaller batch for efficiency.\n",
    "        \"\"\"\n",
    "        if pl_batch_shrink > 1 and x.shape[0] >= pl_batch_shrink:\n",
    "            x = x[::pl_batch_shrink]\n",
    "\n",
    "        x = x.detach().requires_grad_(True)\n",
    "        z = self.forward(x)\n",
    "\n",
    "        # Random unit-direction in rep space (scale by 1/sqrt(d) like StyleGAN)\n",
    "        noise = gram_schmidt(torch.randn_like(z)) \n",
    "        \n",
    "        noise = noise/noise.norm(dim=-1, keepdim=True)#/ math.sqrt(z.shape[1])\n",
    "        out = (z * noise).sum()  # <z, n> so grad wrt x is J^T n\n",
    "\n",
    "        grad = torch.autograd.grad(out, x, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        path_lengths = grad.pow(2).sum(dim=1).sqrt()  # ||J^T n|| = ||J n|| for unit n\n",
    "        penalty = (path_lengths - pl_target).pow(2).mean()\n",
    "        return penalty\n",
    "\n",
    "def gram_schmidt(x):\n",
    "    \"\"\"\n",
    "    Differentiable Gram-Schmidt orthonormalization for a batch of vectors.\n",
    "    x: [N, D] tensor\n",
    "    Returns: [N, D] tensor of orthonormalized vectors\n",
    "    \"\"\"\n",
    "    # x: [N, D]\n",
    "    N, D = x.shape\n",
    "    basis = []\n",
    "    for i in range(N):\n",
    "        v = x[i]\n",
    "        if i > 0:\n",
    "            prev = torch.stack(basis, dim=0)  # [i, D]\n",
    "            # Project v onto the space spanned by previous basis vectors\n",
    "            proj = (v @ prev.t())  # [i]\n",
    "            v = v - (proj.unsqueeze(1) * prev).sum(dim=0)\n",
    "        basis.append(v)\n",
    "    out = torch.stack(basis, dim=0)\n",
    "    return out / (out.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "\n",
    "    \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def probe_second_moment_isotropy_loss(\n",
    "    W: torch.Tensor,\n",
    "    groups: list[list[int]] | None = None,\n",
    "    eps: float = 1e-12,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encourages probe directions to be orthogonal/evenly spread by making the\n",
    "    trace-normalized 2nd-moment within their span close to identity.\n",
    "\n",
    "    W:      [D, d]   (rows are probe weight vectors)\n",
    "    groups: optional list of index lists; if None, use all rows as one group\n",
    "\n",
    "    Returns: scalar loss\n",
    "    \"\"\"\n",
    "    def group_loss(Wg: torch.Tensor) -> torch.Tensor:\n",
    "        # Unit-length directions (scale invariance)\n",
    "        Ug = F.normalize(Wg, p=2, dim=1, eps=eps)        # [k, d]\n",
    "        k, d = Ug.shape\n",
    "\n",
    "        # Second moment: C = E[u u^T] over the k unit rows\n",
    "        C = (Ug.T @ Ug) / float(k)                        # [d, d], SPD, trace = 1\n",
    "\n",
    "        # Restrict to the row span (rank r ≤ k)\n",
    "        # Q: orthonormal basis for span(Ug)\n",
    "        Q = gram_schmidt(Ug).T\n",
    "        # Q, _ = torch.linalg.qr(Ug.T, mode='reduced')      # Q:[d, r] with Q^T Q = I_r\n",
    "        # Moment in the span\n",
    "        Csub = Q.T @ C @ Q                                # [r, r], trace = 1\n",
    "        r = Csub.shape[0]\n",
    "\n",
    "        # Target = isotropic within the span: I_r / r  (trace 1, equal eigenvalues)\n",
    "        return ((Csub - torch.eye(r, device=Wg.device, dtype=Wg.dtype) / r) ** 2).mean()\n",
    "\n",
    "    if groups:\n",
    "        losses = []\n",
    "        for idx in groups:\n",
    "            if len(idx) >= 2:\n",
    "                losses.append(group_loss(W[idx]))\n",
    "        return torch.stack(losses).mean() if losses else W.new_zeros(())\n",
    "    else:\n",
    "        return group_loss(W)\n",
    "\n",
    "# --- Multitask trainer (inherits but overrides train/validate for [B,D] targets) ---\n",
    "class MultiTaskTrainer(Trainer):\n",
    "    def __init__(self, *args, dims: list[str], **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dims = list(dims)  # ensure stable indexing\n",
    "\n",
    "        # ---- NEW: build probe-ortho groups from config (optional) ----\n",
    "        self.probe_ortho_weight = float(self.config[\"training\"].get(\"probe_ortho_weight\", 0.0))\n",
    "        self.probe_ortho_normalize = bool(self.config[\"training\"].get(\"probe_ortho_normalize\", True))\n",
    "        spec = self.config[\"training\"].get(\"probe_ortho_groups\", None)\n",
    "        self.probe_ortho_groups = None\n",
    "        if spec:\n",
    "            if isinstance(spec, dict):\n",
    "                # {\"group_name\": [\"dimA\", \"dimB\", ...], ...}\n",
    "                name_to_idx = {name: i for i, name in enumerate(self.dims)}\n",
    "                groups = []\n",
    "                for _, names in spec.items():\n",
    "                    idxs = [name_to_idx[n] for n in names if n in name_to_idx]\n",
    "                    if len(idxs) > 1:\n",
    "                        groups.append(idxs)\n",
    "                self.probe_ortho_groups = groups if groups else None\n",
    "            elif isinstance(spec, (list, tuple)) and all(isinstance(g, (list, tuple)) for g in spec):\n",
    "                # [[0, 3, 5], [1, 2], ...]\n",
    "                self.probe_ortho_groups = [list(map(int, g)) for g in spec if len(g) > 1]\n",
    "            else:\n",
    "                raise ValueError(\"probe_ortho_groups must be a dict[name -> [dim names]] or a list of index lists\")\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _compute_metrics(self, pred_logits: torch.Tensor, y_prob: torch.Tensor, sample_weights: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute weighted logit MSE and mean correlation across dims on probabilities.\n",
    "        \"\"\"\n",
    "        device = pred_logits.device\n",
    "        mask = torch.isfinite(y_prob)\n",
    "        pred_prob = torch.sigmoid(pred_logits)\n",
    "\n",
    "        # Weighted logit MSE\n",
    "        y_logit = _safe_logit_(y_prob)\n",
    "        resid2 = (pred_logits - y_logit).pow(2)\n",
    "        resid2 = torch.where(mask, resid2, torch.zeros_like(resid2))\n",
    "        denom = mask.to(pred_logits.dtype) * sample_weights\n",
    "        logit_mse = (resid2 * sample_weights).sum() / denom.sum().clamp_min(1.0)\n",
    "\n",
    "        # Unweighted logit MSE\n",
    "        denom_u = mask.to(pred_logits.dtype).sum().clamp_min(1.0)\n",
    "        logit_mse_unweighted = resid2.sum() / denom_u\n",
    "\n",
    "        # Mean correlation across dims (macro)\n",
    "        pred_np = pred_prob.detach().cpu().numpy()\n",
    "        true_np = y_prob.detach().cpu().numpy()\n",
    "        mask_np = mask.detach().cpu().numpy()\n",
    "\n",
    "        dim_corrs = []\n",
    "        for j in range(pred_np.shape[1]):\n",
    "            m = mask_np[:, j]\n",
    "            if m.sum() >= 3:\n",
    "                pj = pred_np[m, j]\n",
    "                tj = true_np[m, j]\n",
    "                if pj.std() > 0 and tj.std() > 0:\n",
    "                    corr = np.corrcoef(pj, tj)[0, 1]\n",
    "                    dim_corrs.append(corr)\n",
    "        mean_dim_corr = float(np.mean(dim_corrs)) if len(dim_corrs) else 0.0\n",
    "\n",
    "        # Micro correlation: flatten all valid pairs\n",
    "        if mask_np.any():\n",
    "            p_all = pred_np[mask_np]\n",
    "            t_all = true_np[mask_np]\n",
    "            micro_corr = float(np.corrcoef(p_all, t_all)[0, 1]) if p_all.std() > 0 and t_all.std() > 0 else 0.0\n",
    "        else:\n",
    "            micro_corr = 0.0\n",
    "\n",
    "        return dict(\n",
    "            mean_corr_macro=mean_dim_corr,\n",
    "            mean_corr_micro=micro_corr,\n",
    "            logit_mse=float(logit_mse.item()),\n",
    "            logit_mse_unweighted=float(logit_mse_unweighted.item()),\n",
    "        )\n",
    "\n",
    "    def validate(self, val_dataloader):\n",
    "        self.model.eval()\n",
    "        all_logits, all_y, all_w = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, w in val_dataloader:\n",
    "                x, y, w = x.to(self.device), y.to(self.device), w.to(self.device)\n",
    "                out = self.model(x)\n",
    "                logits = out[0] if isinstance(out, (tuple, list)) else out  # [B, D]\n",
    "                all_logits.append(logits)\n",
    "                all_y.append(y)\n",
    "                all_w.append(w)\n",
    "\n",
    "        pred_logits = torch.cat(all_logits, dim=0)\n",
    "        y_prob      = torch.cat(all_y, dim=0)\n",
    "        wts         = torch.cat(all_w, dim=0)\n",
    "        metrics = self._compute_metrics(pred_logits, y_prob, wts)\n",
    "        # Keep early-stopping proxy name consistent with your original usage\n",
    "        metrics[\"mean_val_loss\"] = metrics[\"logit_mse\"]\n",
    "        return metrics\n",
    "\n",
    "    def train(self, train_dataloader, val_dataloader):\n",
    "        epochs = self.config['training']['epochs']\n",
    "        criterion = self.criterion\n",
    "        logging.info(f\"[MultiTask] Starting training for {epochs} epochs over {len(self.dims)} dims...\")\n",
    "\n",
    "        for epoch_num in range(epochs):\n",
    "            self.model.train()\n",
    "            mse_loss = 0.0\n",
    "            total_pl_pen = 0.0\n",
    "\n",
    "            for x, y, weights in train_dataloader:\n",
    "                x, y, weights = x.to(self.device), y.to(self.device), weights.to(self.device)\n",
    "\n",
    "                out = self.model(x)\n",
    "                logits = out[0] if isinstance(out, (tuple, list)) else out  # [B, D]\n",
    "                loss = criterion(logits, y, sample_weights=weights)\n",
    "                mse_loss += float(loss.item())\n",
    "\n",
    "\n",
    "                # Inside MultiTaskTrainer.train loop, after computing the main loss:\n",
    "                pl_weight = float(self.config['training'].get('pl_weight', 0.0))\n",
    "                if pl_weight > 0 and hasattr(self.model, \"encoder\") and hasattr(self.model.encoder, \"path_length_penalty\"):\n",
    "                    # Feed the same x used for the step (cheap, shrunk inside)\n",
    "                    pl_pen = self.model.encoder.path_length_penalty(G.mapping(torch.randn(512,512).to(device), None, truncation_psi=1.0)[:,0],\n",
    "                                                                    pl_target=self.config['training'].get('pl_target', 1.0),\n",
    "                                                                    pl_batch_shrink=self.config['training'].get('pl_batch_shrink', 2))\n",
    "                    total_pl_pen += pl_weight*pl_pen.item()\n",
    "                    loss = loss + pl_weight * pl_pen\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    print(f\"lossisnan: {loss.isnan()} yisnan: {y.isnan().sum(dim=1)} logitsisnan: {logits.isnan().sum(dim=1)}\")\n",
    "                    print(f\"loss: {loss} y {y} logits {logits}\")\n",
    "                    raise ValueError(\"NaN loss\")\n",
    "                    continue\n",
    "\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                if self.config['training']['grad_clip_norm'] is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.config['training']['grad_clip_norm'])\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "                \n",
    "            avg_train_loss = mse_loss / max(1, len(train_dataloader))\n",
    "            avg_pl_pen = total_pl_pen / max(1, self.config['training'].get('pl_batch_shrink', 2))\n",
    "\n",
    "            if (epoch_num + 1) % self.config['logging_and_saving']['validation_freq'] == 0:\n",
    "                \n",
    "                log = self.validate(val_dataloader)\n",
    "                mean_val_loss = log[\"logit_mse\"]  # same early-stopping proxy\n",
    "\n",
    "                if mean_val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = mean_val_loss\n",
    "                    os.makedirs(os.path.dirname(self.model_save_path), exist_ok=True)\n",
    "                    torch.save(self.model.state_dict(), self.model_save_path)\n",
    "                    logging.info(\n",
    "                        f\"[MultiTask] Epoch {epoch_num + 1}: New best \"\n",
    "                        f\"(macro corr: {log['mean_corr_macro']:.4f}, micro corr: {log['mean_corr_micro']:.4f})\"\n",
    "                    )\n",
    "\n",
    "                if (epoch_num + 1) % self.config['logging_and_saving']['print_freq'] == 0:\n",
    "                    logging.info(\n",
    "                        f\"[MultiTask] Epoch {epoch_num + 1}/{epochs} | \"\n",
    "                        f\"Train Loss (logit MSE): {avg_train_loss:.4f} | \"\n",
    "                        f\"Train PL Penalty: {avg_pl_pen:.4f} | \"\n",
    "                        # f\"Val Loss (L1): {log['logit_mse_L1']:.4f} | \"\n",
    "                        f\"Val Loss (logit MSE unweighted): {log['logit_mse_unweighted']:.4f} | \"\n",
    "                        f\"Val Loss (logit MSE): {log['logit_mse']:.4f} | \"\n",
    "                        f\"macro_corr: {log['mean_corr_macro']:.4f} | \"\n",
    "                        f\"micro_corr: {log['mean_corr_micro']:.4f}\"\n",
    "                    )\n",
    "                \n",
    "                # with torch.no_grad():\n",
    "                #     # self.model.init_probes_with_ols(train_loader, ridge=1e4, use_logit=True, max_batches=None, verbose=True, noise_scale=0.0)\n",
    "                #     w_prim = self.model.encoder(G.mapping(torch.randn(512*4,512).to(device), None, truncation_psi=1.0)[:,0])\n",
    "                #     w_prim = w_prim.detach().cpu().numpy()\n",
    "                #     cov = np.cov(w_prim, rowvar=False).astype(np.float32)\n",
    "                #     cov= torch.from_numpy(cov).to(device)\n",
    "                #     # eig_vals, eig_vecs = torch.linalg.eigh(cov)\n",
    "                #     # eig_vals = eig_vals.cpu().numpy()\n",
    "                #     # eig_vecs = eig_vecs.cpu().numpy()\n",
    "                #     # plt.plot(sorted(eig_vals))\n",
    "                #     # plt.show()\n",
    "                #     W = self.model.probes.weight.clone().T\n",
    "                #     new_W = (cov)@W\n",
    "                #     principal_values = new_W.norm(dim=-1)/W.norm(dim=-1)\n",
    "                #     new_W = new_W/principal_values.unsqueeze(1)**2\n",
    "                #     self.model.probes.weight.copy_((new_W.T + W.T*10)/3)\n",
    "\n",
    "                #     out = self.model(x)\n",
    "                #     logits = out[0] if isinstance(out, (tuple, list)) else out  # [B, D]\n",
    "                #     loss = criterion(logits, y, sample_weights=weights)\n",
    "                #     print(loss)\n",
    "                #     if loss>0.9:\n",
    "                #         self.model.probes.weight.copy_(W.T)\n",
    "\n",
    "            # with torch.no_grad():\n",
    "            #         W = self.model.probes.weight.clone()\n",
    "            #         W = W+torch.randn_like(W)*.001\n",
    "            #         self.model.probes.weight.copy_(W)\n",
    "\n",
    "\n",
    "               \n",
    "                \n",
    "\n",
    "        logging.info(f\"[MultiTask] Training finished. Best validation loss: {self.best_val_loss:.4f}\")\n",
    "        try:\n",
    "            self.model.load_state_dict(torch.load(self.model_save_path))\n",
    "            logging.info(f\"[MultiTask] Loaded best model from {self.model_save_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[MultiTask] Could not load the best model. Error: {e}\")\n",
    "        return self.model, self.best_val_loss\n",
    "\n",
    "    def train(self, train_dataloader, val_dataloader):\n",
    "        epochs = self.config['training']['epochs']\n",
    "        criterion = self.criterion\n",
    "        logging.info(f\"[MultiTask] Starting training for {epochs} epochs over {len(self.dims)} dims...\")\n",
    "\n",
    "        for epoch_num in range(epochs):\n",
    "            self.model.train()\n",
    "            mse_loss_sum = 0.0\n",
    "            total_pl_pen = 0.0\n",
    "            total_ortho_pen = 0.0\n",
    "\n",
    "            for x, y, weights in train_dataloader:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                weights = weights.to(self.device)\n",
    "\n",
    "                out = self.model(x)\n",
    "                logits = out[0] if isinstance(out, (tuple, list)) else out  # [B, D]\n",
    "\n",
    "                loss = criterion(logits, y, sample_weights=weights)\n",
    "                mse_loss_sum += float(loss.item())\n",
    "\n",
    "                # ---- Group-level probe orthogonality penalty ----\n",
    "                ortho_w = float(self.config['training'].get('probe_ortho_weight', 0.0))\n",
    "                if ortho_w > 0.0:\n",
    "                    W = self.model.probes.weight   # [D, d]\n",
    "                    # Option A: second-moment isotropy (recommended)\n",
    "                    ortho_pen = probe_second_moment_isotropy_loss(\n",
    "                        W, groups=getattr(self, \"probe_ortho_groups\", None)\n",
    "                    )\n",
    "\n",
    "                    loss = loss + ortho_w * ortho_pen\n",
    "                    total_ortho_pen += float(ortho_pen.item())\n",
    "\n",
    "                # ---- Path-length penalty on encoder (optional) ----\n",
    "                pl_weight = float(self.config['training'].get('pl_weight', 0.0))\n",
    "                if pl_weight > 0.0 and hasattr(self.model, \"encoder\") and hasattr(self.model.encoder, \"path_length_penalty\"):\n",
    "                    pl_input = G.mapping(torch.randn(512, 512, device=self.device), None, truncation_psi=1.0)[:, 0]\n",
    "                    pl_pen = self.model.encoder.path_length_penalty(\n",
    "                        pl_input,\n",
    "                        pl_target=self.config['training'].get('pl_target', 1.0),\n",
    "                        pl_batch_shrink=self.config['training'].get('pl_batch_shrink', 2),\n",
    "                    )\n",
    "                    loss = loss + pl_weight * pl_pen\n",
    "                    total_pl_pen += float(pl_pen.item())\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    print(f\"lossisnan: {loss.isnan()}\")\n",
    "                    print(f\"y NaNs per-sample: {y.isnan().sum(dim=1)}\")\n",
    "                    print(f\"logits NaNs per-sample: {logits.isnan().sum(dim=1)}\")\n",
    "                    print(f\"loss: {loss} y {y} logits {logits}\")\n",
    "                    raise ValueError(\"NaN loss\")\n",
    "\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                if self.config['training']['grad_clip_norm'] is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        max_norm=self.config['training']['grad_clip_norm']\n",
    "                    )\n",
    "\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            avg_train_loss = mse_loss_sum / max(1, len(train_dataloader))\n",
    "            avg_pl_pen = total_pl_pen / max(1, len(train_dataloader))\n",
    "            avg_ortho_pen = total_ortho_pen / max(1, len(train_dataloader))\n",
    "\n",
    "            if (epoch_num + 1) % self.config['logging_and_saving']['validation_freq'] == 0:\n",
    "                log = self.validate(val_dataloader)\n",
    "                mean_val_loss = log[\"logit_mse\"]\n",
    "\n",
    "                if mean_val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = mean_val_loss\n",
    "                    os.makedirs(os.path.dirname(self.model_save_path), exist_ok=True)\n",
    "                    torch.save(self.model.state_dict(), self.model_save_path)\n",
    "                    logging.info(\n",
    "                        f\"[MultiTask] Epoch {epoch_num + 1}: New best \"\n",
    "                        f\"(macro corr: {log['mean_corr_macro']:.4f}, micro corr: {log['mean_corr_micro']:.4f})\"\n",
    "                    )\n",
    "\n",
    "                if (epoch_num + 1) % self.config['logging_and_saving']['print_freq'] == 0:\n",
    "                    logging.info(\n",
    "                        f\"[MultiTask] Epoch {epoch_num + 1}/{epochs} | \"\n",
    "                        f\"Train Loss (logit MSE): {avg_train_loss:.4f} | \"\n",
    "                        f\"Train PL Penalty: {avg_pl_pen:.4f} | \"\n",
    "                        f\"Train Ortho Penalty: {avg_ortho_pen:.4f} | \"\n",
    "                        f\"Val Loss (logit MSE unweighted): {log['logit_mse_unweighted']:.4f} | \"\n",
    "                        f\"Val Loss (logit MSE): {log['logit_mse']:.4f} | \"\n",
    "                        f\"macro_corr: {log['mean_corr_macro']:.4f} | \"\n",
    "                        f\"micro_corr: {log['mean_corr_micro']:.4f}\"\n",
    "                    )\n",
    "\n",
    "        logging.info(f\"[MultiTask] Training finished. Best validation loss: {self.best_val_loss:.4f}\")\n",
    "        try:\n",
    "            self.model.load_state_dict(torch.load(self.model_save_path))\n",
    "            logging.info(f\"[MultiTask] Loaded best model from {self.model_save_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[MultiTask] Could not load the best model. Error: {e}\")\n",
    "        return self.model, self.best_val_loss\n",
    "\n",
    "# --- Example: run multitask training in __main__ ---\n",
    "\n",
    "def mixup_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Minimal, fast collate: just pack tensors and pass through.\n",
    "    Maintains the same return signature (x, y, weights).\n",
    "    \"\"\"\n",
    "    x_list, y_list, w_list = zip(*batch)\n",
    "    x = torch.stack(x_list)\n",
    "    y = torch.stack(y_list)\n",
    "    w = torch.stack(w_list)\n",
    "    # Normalize weights to mean 1 (cheap)\n",
    "    w = w / (w.mean() + 1e-12)\n",
    "    return x, y, w\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG & MAIN\n",
    "# ---------------------------\n",
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        \"min_ratings\": 1,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 150,\n",
    "        \"batch_size\": 64,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"weight_decay\": 1e-2,\n",
    "        \"warmup_fraction\": 0.02,\n",
    "        \"grad_clip_norm\": 5.0,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"seed\": 42,\n",
    "    },\n",
    "    \"pde_training\": {\n",
    "        \"num_support_timesteps\": 20,    # requested\n",
    "        \"t_samples_per_batch\": 3,       # # of random t points for PDE residual\n",
    "        \"lap_samples\": 1,               # Hutchinson probe samples\n",
    "        \"lambda_sup\": 1.0,              # supervise u(z,0) ~= logit(y)\n",
    "        \"lambda_f\": 1.0,                # wave residual weight\n",
    "        \"lambda_u0\": 1e-2,              # initial-zero-flow\n",
    "        \"lambda_jvp\": 0.0,              # set >0 to enable; requires valid G\n",
    "        \"use_jvp\": False,\n",
    "    },\n",
    "    \"logging_and_saving\": {\n",
    "        \"save_dir\": \"best_models/wavepde\",\n",
    "        \"validation_freq\": 5,\n",
    "        \"print_freq\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "    # Optimizer / sched\n",
    "    # After training:\n",
    "    # - u_k(z, 0) ≈ logit(mean rating) for attribute k\n",
    "    # - gradients ∇_z u_k(z, t) give flows usable for latent traversals\n",
    "    # Example traversal for attribute k* at time t:\n",
    "    #   u, g = wave.forward_u(k*, z, torch.tensor([...t...])), wave.grad_z_at_t(k*, z, t_scalar=t)\n",
    "    #   z_next = z + g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d9102ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_mask: 0\n",
      "[prepare_multitask_means] N=1004, D=34, avg_valid_dims_per_photo=34.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Build multitask dataset (do not disturb single-task path above)\n",
    "    DIMS = dims_from_ratings()\n",
    "    DIMS = set(DIMS)-set([\"warm\",\"competent\"])\n",
    "    DIMS = list(DIMS)\n",
    "    X, Y, sample_w, meta = prepare_multitask_means(CONFIG, dims=DIMS, verbose=True)\n",
    "\n",
    "    # Split\n",
    "    num_samples = len(X)\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.seed(CONFIG['training']['seed'])\n",
    "    np.random.shuffle(indices)\n",
    "    val_size = int(num_samples * CONFIG['training']['validation_split'])\n",
    "    val_idx = indices[:val_size]\n",
    "    train_idx = indices[val_size:]\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(X[train_idx], Y[train_idx], sample_w[train_idx])\n",
    "    val_dataset   = torch.utils.data.TensorDataset(X[val_idx],   Y[val_idx],   sample_w[val_idx])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=mixup_collate_fn,  # unchanged\n",
    "        num_workers=0,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e35bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nc/zbqbbkkx08z7v1g3mlf8bdn00000gn/T/ipykernel_84781/1003320093.py:4: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "a,b = ((Y[:,:2].T).detach().cpu().numpy())\n",
    "plt.hist(a)\n",
    "plt.hist(b)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Subclass + tooling for initial training & weight export\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def _safe_logit(y: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    y = y.clamp(eps, 1.0 - eps)\n",
    "    return torch.log(y) - torch.log(1.0 - y)\n",
    "\n",
    "class WavePDEWithInit(WavePDE):\n",
    "    \"\"\"\n",
    "    Extends the base WavePDE with:\n",
    "      • supervised calibration at t=0: u_k(z,0) ~= logit(mean_rating_k)\n",
    "      • PDE + initial-condition regularization (uses your base losses)\n",
    "      • optional JVP sharpening (requires a differentiable generator)\n",
    "      • export/import utilities for interop with other codebases\n",
    "    \"\"\"\n",
    "    def __init__(self, num_support_sets, num_support_timesteps, support_vectors_dim):\n",
    "        super(WavePDEWithInit, self).__init__(num_support_sets, num_support_timesteps, support_vectors_dim)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def export_state_dict(self, path: str):\n",
    "        \"\"\"Single blob export (Torch-native).\"\"\"\n",
    "        os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "\n",
    "    # ------------- Initial-training loop -------------\n",
    "    \n",
    "    # --------- add this import for the scheduler ----------\n",
    "\n",
    "    def fit_initial(\n",
    "        self,\n",
    "        train_loader,\n",
    "        val_loader=None,\n",
    "        *,\n",
    "        epochs: int = 100,\n",
    "        lr: float = 2e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "        grad_clip_norm: float | None = 5.0,\n",
    "        lambda_sup: float = 1.0,\n",
    "        lambda_pde: float = 1.0,\n",
    "        lambda_ic: float = 1e-2,\n",
    "        lambda_jvp: float = 0.0,\n",
    "        use_jvp: bool = False,\n",
    "        generator=None,\n",
    "        device: torch.device | None = None,\n",
    "        log_every: int = 10,\n",
    "        save_best_to: str | None = None,\n",
    "        dim_names=None,\n",
    "        # NEW:\n",
    "        viz_heatmap: bool = True,\n",
    "        heatmap_title: str = \"Validation MSE Heatmap (MLP vs Target)\",\n",
    "        # NEW: scheduler params\n",
    "        use_cosine_schedule: bool = True,\n",
    "        warmup_steps: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains all K heads jointly and (optionally) live-updates a K×K MSE heatmap on validation.\n",
    "        Heatmap (rows = MLP_i, cols = target_j) shows weighted MSE of u_i(z,0) vs logit(y_j).\n",
    "        Optionally uses a cosine learning rate schedule with warmup.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        if device is None:\n",
    "            device = next(self.parameters()).device\n",
    "\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        best_val = float(\"inf\")\n",
    "        half_range = max(1, self.num_support_timesteps // 2)\n",
    "        K = self.num_support_sets\n",
    "\n",
    "        # Scheduler setup\n",
    "        if use_cosine_schedule:\n",
    "            # Estimate total steps\n",
    "            if hasattr(train_loader, \"__len__\"):\n",
    "                steps_per_epoch = len(train_loader)\n",
    "            else:\n",
    "                steps_per_epoch = 100  # fallback\n",
    "            total_steps = epochs * steps_per_epoch\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                opt,\n",
    "                num_warmup_steps=warmup_steps,\n",
    "                num_training_steps=total_steps\n",
    "            )\n",
    "        else:\n",
    "            scheduler = None\n",
    "\n",
    "        # Dashboard\n",
    "        dash = HeatmapDashboard(K, dim_names=list(dim_names), title=heatmap_title) if (viz_heatmap and val_loader is not None) else None\n",
    "\n",
    "\n",
    "        def _compute_val_mse_matrix(VX, VY, VW):\n",
    "            \"\"\"\n",
    "            Returns:\n",
    "              M: [K,K] torch tensor with weighted MSEs (rows=MLP_i, cols=target_j)\n",
    "              diag_mean, offdiag_mean, worst_pair (i,j,val) or None\n",
    "            \"\"\"\n",
    "            B = VX.shape[0]\n",
    "            t0 = torch.zeros(B, dtype=VX.dtype, device=VX.device)\n",
    "            # U[:, i] = u_i(VX, 0)\n",
    "            U_cols = []\n",
    "            for i in range(K):\n",
    "                U_cols.append(self.eval_u(i, VX, t0))\n",
    "            U = torch.stack(U_cols, dim=1)  # [B,K]\n",
    "            Ylog = _safe_logit(VY)          # [B,K]\n",
    "            mask = torch.isfinite(Ylog)     # [B,K]\n",
    "\n",
    "            # VW handling: allow None, [B], or [B,K]\n",
    "            if VW is None:\n",
    "                VW_full = torch.ones_like(Ylog)\n",
    "            else:\n",
    "                if VW.ndim == 1:\n",
    "                    VW_full = VW[:, None].expand_as(Ylog)\n",
    "                else:\n",
    "                    VW_full = VW\n",
    "\n",
    "            M = torch.full((K, K), float('nan'), device=VX.device, dtype=VX.dtype)\n",
    "            for j in range(K):\n",
    "                m = mask[:, j]\n",
    "                if not m.any():\n",
    "                    continue\n",
    "                yj = Ylog[m, j]                    # [b_j]\n",
    "                wj = VW_full[m, j].clamp_min(1e-12)# [b_j]\n",
    "                diff = U[m, :] - yj[:, None]       # [b_j, K]\n",
    "                mse_j = ((diff**2) * wj[:, None]).sum(dim=0) / wj.sum()  # [K]\n",
    "                M[:, j] = mse_j\n",
    "\n",
    "            # Stats\n",
    "            diag_vals = torch.diag(M)\n",
    "            diag_mean = torch.nanmean(diag_vals).item() if torch.isfinite(diag_vals).any() else float(\"nan\")\n",
    "            off_mask = ~torch.eye(K, dtype=torch.bool, device=M.device)\n",
    "            off_vals = M[off_mask]\n",
    "            off_mean = torch.nanmean(off_vals).item() if torch.isfinite(off_vals).any() else float(\"nan\")\n",
    "\n",
    "            worst_pair = None\n",
    "            if torch.isfinite(M).any():\n",
    "                # ignore diagonal when picking \"worst confusion\"\n",
    "                M_off = M.clone()\n",
    "                M_off[~off_mask] = float('-inf')\n",
    "                j_idx = torch.argmax(M_off.view(-1)).item()\n",
    "                i_w, j_w = divmod(j_idx, K)\n",
    "                val_w = float(M_off[i_w, j_w].item())\n",
    "                if val_w != float('-inf'):\n",
    "                    worst_pair = (i_w, j_w, val_w)\n",
    "\n",
    "            return M, diag_mean, off_mean, worst_pair\n",
    "\n",
    "        # ---------------- train loop ----------------\n",
    "        for ep in range(1, epochs + 1):\n",
    "            running = dict(sup=0.0, pde=0.0, ic=0.0, jvp=0.0)\n",
    "            for batch in train_loader:\n",
    "                if len(batch) == 3:\n",
    "                    X, Y, W = batch\n",
    "                    if W.ndim == 1:\n",
    "                        W = W[:, None].expand(-1, K)\n",
    "                else:\n",
    "                    X, Y = batch\n",
    "                    W = None\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                X=X+torch.randn_like(X)*0.02\n",
    "                if W is not None:\n",
    "                    W = W.to(device)\n",
    "\n",
    "                loss = X.new_zeros(())\n",
    "                for k in range(K):\n",
    "                    sup = _supervised_loss_one_k(k, X, Y[:, k], None if W is None else W[:, k])\n",
    "                    pde = _pde_loss_one_k(k, X)\n",
    "                    # ic  = _ic_loss_one_k(k, X)\n",
    "                    jvp = _jvp_loss_one_k(k, X)\n",
    "\n",
    "                    loss = loss + lambda_sup * sup + lambda_pde * pde # + lambda_ic * ic - lambda_jvp * jvp\n",
    "                    running['sup'] += float(sup.item())\n",
    "                    running['pde'] += float(pde.item())\n",
    "                    # running['ic']  += float(ic.item())\n",
    "                    running['jvp'] += float(jvp.item())\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                if grad_clip_norm is not None:\n",
    "                    nn.utils.clip_grad_norm_(self.parameters(), grad_clip_norm)\n",
    "                opt.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            # ----- periodic logging + validation / heatmap -----\n",
    "            if (ep % log_every) == 0 and val_loader is not None:\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    try:\n",
    "                        VX, VY, VW = next(iter(val_loader))\n",
    "                    except StopIteration:\n",
    "                        VX, VY, VW = next(iter(val_loader))\n",
    "                VX, VY = VX.to(device), VY.to(device)\n",
    "                VW = VW.to(device) if VW is not None else None\n",
    "\n",
    "                # full MSE matrix and summary stats\n",
    "                M, diag_mean, off_mean, worst_pair = _compute_val_mse_matrix(VX, VY, VW)\n",
    "\n",
    "                # model selection on diagonal mean (matches your primary goal)\n",
    "                if np.isfinite(diag_mean) and (diag_mean < best_val):\n",
    "                    best_val = diag_mean\n",
    "                    if save_best_to is not None:\n",
    "                        self.export_state_dict(save_best_to)\n",
    "\n",
    "                # live heatmap update\n",
    "                if dash is not None:\n",
    "                    dash.update(\n",
    "                        epoch=ep,\n",
    "                        zmat=M.detach().cpu().numpy(),\n",
    "                        diag_mse=diag_mean,\n",
    "                        off_mse=off_mean,\n",
    "                        worst_pair=worst_pair\n",
    "                    )\n",
    "\n",
    "                # console log (optional)\n",
    "                print(f\"[WavePDEWithInit] epoch {ep}/{epochs} | \"\n",
    "                      f\"sup {running['sup']:.3f} | pde {running['pde']:.3f} | ic {running['ic']:.3f} | jvp {running['jvp']:.3f} | \"\n",
    "                      f\"val diag-MSE {diag_mean:.6f} | offdiag-MSE {off_mean:.6f} | lr {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "                self.train()\n",
    "\n",
    "        # final export if requested and never saved as best\n",
    "        if save_best_to is not None and not os.path.exists(save_best_to):\n",
    "            self.export_state_dict(save_best_to)\n",
    "        return best_val\n",
    "    \n",
    "\n",
    "# --------- add these imports (near your subclass) ----------\n",
    "from typing import Optional, List\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display\n",
    "\n",
    "# --------- lightweight dashboard for the K×K MSE heatmap ----------\n",
    "class HeatmapDashboard:\n",
    "    def __init__(self, K: int, dim_names: Optional[List[str]] = None, title: str = \"Validation MSE Heatmap\"):\n",
    "        self.K = K\n",
    "        self.dim_names = dim_names if dim_names is not None else [f\"dim_{i}\" for i in range(K)]\n",
    "        self.step_hist = []\n",
    "        self.diag_hist = []\n",
    "        self.offdiag_hist = []\n",
    "        self.best_diag = float(\"inf\")\n",
    "        self.best_epoch = -1\n",
    "        self._zmax_running = 0.0\n",
    "\n",
    "        self.fig = go.FigureWidget(make_subplots(\n",
    "            rows=3, cols=2,\n",
    "            specs=[\n",
    "                [{\"type\": \"heatmap\", \"rowspan\": 3}, {\"type\": \"xy\"}],\n",
    "                [None, {\"type\": \"xy\"}],\n",
    "                [None, {\"type\": \"domain\"}],\n",
    "            ],\n",
    "            column_widths=[0.66, 0.34],\n",
    "            row_heights=[0.55, 0.30, 0.15],\n",
    "            vertical_spacing=0.06,\n",
    "            subplot_titles=(\"MLP vs Target — MSE @ t=0\", \"Diagnostics\", None),\n",
    "        ))\n",
    "\n",
    "        # Left: heatmap placeholder\n",
    "        self.hm = go.Heatmap(\n",
    "            z=[[None]*K for _ in range(K)],\n",
    "            colorscale=\"Viridis\",\n",
    "            colorbar=dict(title=\"MSE\"),\n",
    "            zmin=0, zmax=1.0,\n",
    "            x=self.dim_names, y=self.dim_names, hoverongaps=False\n",
    "        )\n",
    "        self.fig.add_trace(self.hm, row=1, col=1)\n",
    "        self.fig.update_xaxes(title_text=\"Target (j)\", row=1, col=1)\n",
    "        self.fig.update_yaxes(title_text=\"MLP (i)\", row=1, col=1, autorange=\"reversed\")\n",
    "\n",
    "        # Right: lines for diag/offdiag\n",
    "        self.fig.add_trace(go.Scatter(mode=\"lines+markers\", name=\"diag MSE\", line=dict(width=2)), row=1, col=2)\n",
    "        self.fig.add_trace(go.Scatter(mode=\"lines+markers\", name=\"offdiag MSE\", line=dict(width=2, dash=\"dash\")), row=2, col=2)\n",
    "        self.fig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n",
    "        self.fig.update_yaxes(title_text=\"MSE\", row=1, col=2)\n",
    "        self.fig.update_xaxes(title_text=\"Epoch\", row=2, col=2)\n",
    "        self.fig.update_yaxes(title_text=\"MSE\", row=2, col=2)\n",
    "\n",
    "        # Bottom-right: stats box as a paper-level annotation (works with any subplot type)\n",
    "        self.fig.add_annotation(\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.90, y=0.14,          # tweak to taste (0..1 across the full figure)\n",
    "            xanchor=\"right\", yanchor=\"bottom\",\n",
    "            text=\"\",\n",
    "            showarrow=False, align=\"left\",\n",
    "            bgcolor=\"rgba(255,255,255,0.85)\", bordercolor=\"#444\", borderwidth=1,\n",
    "            font=dict(size=12, family=\"Courier New\"),\n",
    "        )\n",
    "        self._ann_idx = len(self.fig.layout.annotations) - 1\n",
    "\n",
    "        self.fig.update_layout(\n",
    "            width=1400, height=900, title_text=title, title_x=0.5, showlegend=True,\n",
    "            margin=dict(l=60, r=40, t=60, b=50),\n",
    "        )\n",
    "        display(self.fig)\n",
    "\n",
    "    def _fmt_stats(self, epoch: int, zmat, diag_mse: float, off_mse: float, worst_pair: Optional[tuple]):\n",
    "        lines = [\n",
    "            f\"epoch: {epoch}\",\n",
    "            f\"diag MSE (mean): {diag_mse:.6f}\",\n",
    "            f\"offdiag MSE (mean): {off_mse:.6f}\",\n",
    "            f\"best diag so far: {self.best_diag:.6f} @ epoch {self.best_epoch}\",\n",
    "        ]\n",
    "        if worst_pair is not None:\n",
    "            (i, j, val) = worst_pair\n",
    "            lines.append(f\"worst confusion: MLP[{self.dim_names[i]}] vs target[{self.dim_names[j]}] = {val:.6f}\")\n",
    "        return \"<br>\".join(lines)\n",
    "\n",
    "    def update(self, epoch: int, zmat, diag_mse: float, off_mse: float, worst_pair: Optional[tuple] = None):\n",
    "        # histories\n",
    "        self.step_hist.append(epoch)\n",
    "        self.diag_hist.append(diag_mse)\n",
    "        self.offdiag_hist.append(off_mse)\n",
    "        if diag_mse < self.best_diag:\n",
    "            self.best_diag, self.best_epoch = diag_mse, epoch\n",
    "\n",
    "        # dynamic color range (robust to spikes)\n",
    "        finite_vals = np.asarray(zmat, dtype=np.float64)\n",
    "        finite_vals = finite_vals[np.isfinite(finite_vals)]\n",
    "        if finite_vals.size > 0:\n",
    "            self._zmax_running = max(self._zmax_running, float(np.percentile(finite_vals, 98)))\n",
    "        zmax = max(1e-6, self._zmax_running)\n",
    "\n",
    "        with self.fig.batch_update():\n",
    "            # heatmap\n",
    "            self.fig.data[0].z = zmat\n",
    "            self.fig.data[0].zmax = zmax\n",
    "            # lines\n",
    "            self.fig.data[1].x = self.step_hist\n",
    "            self.fig.data[1].y = self.diag_hist\n",
    "            self.fig.data[2].x = self.step_hist\n",
    "            self.fig.data[2].y = self.offdiag_hist\n",
    "            # stats\n",
    "            self.fig.layout.annotations[self._ann_idx].text = self._fmt_stats(epoch, zmat, diag_mse, off_mse, worst_pair)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e62bc52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e491552c0f64081a72d5319a1b4f369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'colorbar': {'title': {'text': 'MSE'}},\n",
       "              'colorscale': [[0.0, '#440154'], [0.1111111111111111, '#482878'],\n",
       "                             [0.2222222222222222, '#3e4989'], [0.3333333333333333,\n",
       "                             '#31688e'], [0.4444444444444444, '#26828e'],\n",
       "                             [0.5555555555555556, '#1f9e89'], [0.6666666666666666,\n",
       "                             '#35b779'], [0.7777777777777778, '#6ece58'],\n",
       "                             [0.8888888888888888, '#b5de2b'], [1.0, '#fde725']],\n",
       "              'hoverongaps': False,\n",
       "              'type': 'heatmap',\n",
       "              'uid': 'a5e3565f-cbc3-4b60-8490-fd43df56c960',\n",
       "              'x': [age, outgoing, hispanic, looks-like-you, typical, godly, skin-\n",
       "                    color, gay, privileged, islander, memorable, dominant, happy,\n",
       "                    well-groomed, trustworthy, native, attractive, hair-color,\n",
       "                    black, liberal, gender, asian, cute, white, alert, middle-\n",
       "                    eastern, weight, smug, electable, familiar, long-haired,\n",
       "                    outdoors, smart, dorky],\n",
       "              'xaxis': 'x',\n",
       "              'y': [age, outgoing, hispanic, looks-like-you, typical, godly, skin-\n",
       "                    color, gay, privileged, islander, memorable, dominant, happy,\n",
       "                    well-groomed, trustworthy, native, attractive, hair-color,\n",
       "                    black, liberal, gender, asian, cute, white, alert, middle-\n",
       "                    eastern, weight, smug, electable, familiar, long-haired,\n",
       "                    outdoors, smart, dorky],\n",
       "              'yaxis': 'y',\n",
       "              'z': [[None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None], [None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None], [None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None], [None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None], [None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None],\n",
       "                    [None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None], [None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None], [None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None], [None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None], [None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None],\n",
       "                    [None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None], [None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None], [None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None], [None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None], [None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None],\n",
       "                    [None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None], [None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None], [None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None], [None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None], [None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None],\n",
       "                    [None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None], [None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None], [None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None], [None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None], [None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None],\n",
       "                    [None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None], [None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None], [None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None], [None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None], [None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None],\n",
       "                    [None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None], [None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None], [None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None], [None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None]],\n",
       "              'zmax': 1.0,\n",
       "              'zmin': 0},\n",
       "             {'line': {'width': 2},\n",
       "              'mode': 'lines+markers',\n",
       "              'name': 'diag MSE',\n",
       "              'type': 'scatter',\n",
       "              'uid': '40e5e17f-c071-481f-95f1-22981b6f7e83',\n",
       "              'xaxis': 'x2',\n",
       "              'yaxis': 'y2'},\n",
       "             {'line': {'dash': 'dash', 'width': 2},\n",
       "              'mode': 'lines+markers',\n",
       "              'name': 'offdiag MSE',\n",
       "              'type': 'scatter',\n",
       "              'uid': '6c7ef4ec-c714-4881-b87e-a75e5ebcf9d8',\n",
       "              'xaxis': 'x3',\n",
       "              'yaxis': 'y3'}],\n",
       "    'layout': {'annotations': [{'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'MLP vs Target — MSE @ t=0',\n",
       "                                'x': 0.29700000000000004,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.0,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'},\n",
       "                               {'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Diagnostics',\n",
       "                                'x': 0.847,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.0,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'},\n",
       "                               {'align': 'left',\n",
       "                                'bgcolor': 'rgba(255,255,255,0.85)',\n",
       "                                'bordercolor': '#444',\n",
       "                                'borderwidth': 1,\n",
       "                                'font': {'family': 'Courier New', 'size': 12},\n",
       "                                'showarrow': False,\n",
       "                                'text': '',\n",
       "                                'x': 0.9,\n",
       "                                'xanchor': 'right',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 0.14,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'}],\n",
       "               'height': 900,\n",
       "               'margin': {'b': 50, 'l': 60, 'r': 40, 't': 60},\n",
       "               'showlegend': True,\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Validation MSE Heatmap (MLP vs Target)', 'x': 0.5},\n",
       "               'width': 1400,\n",
       "               'xaxis': {'anchor': 'y', 'domain': [0.0, 0.5940000000000001], 'title': {'text': 'Target (j)'}},\n",
       "               'xaxis2': {'anchor': 'y2', 'domain': [0.6940000000000001, 1.0], 'title': {'text': 'Epoch'}},\n",
       "               'xaxis3': {'anchor': 'y3', 'domain': [0.6940000000000001, 1.0], 'title': {'text': 'Epoch'}},\n",
       "               'yaxis': {'anchor': 'x', 'autorange': 'reversed', 'domain': [0.0, 1.0], 'title': {'text': 'MLP (i)'}},\n",
       "               'yaxis2': {'anchor': 'x2', 'domain': [0.516, 1.0], 'title': {'text': 'MSE'}},\n",
       "               'yaxis3': {'anchor': 'x3', 'domain': [0.192, 0.456], 'title': {'text': 'MSE'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'WavePDEWithInit' object has no attribute 'eval_u'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# K targets, 10 timesteps, 512-d latents\u001b[39;00m\n\u001b[32m      2\u001b[39m model = WavePDEWithInit(num_support_sets=\u001b[38;5;28mlen\u001b[39m(DIMS), num_support_timesteps=\u001b[32m10\u001b[39m, support_vectors_dim=\u001b[32m512\u001b[39m).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m best_val = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_initial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_sup\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_pde\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_ic\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_jvp\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_jvp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# refresh heatmap every epoch\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_best_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwavepde_init.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdim_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDIMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mviz_heatmap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Export for other code\u001b[39;00m\n\u001b[32m     17\u001b[39m model.export_state_dict(\u001b[33m\"\u001b[39m\u001b[33mwavepde_init.pt\u001b[39m\u001b[33m\"\u001b[39m)                     \u001b[38;5;66;03m# one-file export\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 198\u001b[39m, in \u001b[36mWavePDEWithInit.fit_initial\u001b[39m\u001b[34m(self, train_loader, val_loader, epochs, lr, weight_decay, grad_clip_norm, lambda_sup, lambda_pde, lambda_ic, lambda_jvp, use_jvp, generator, device, log_every, save_best_to, dim_names, viz_heatmap, heatmap_title, use_cosine_schedule, warmup_steps)\u001b[39m\n\u001b[32m    196\u001b[39m loss = X.new_zeros(())\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K):\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sup = \u001b[43m_supervised_loss_one_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     pde = _pde_loss_one_k(k, X)\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# ic  = _ic_loss_one_k(k, X)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mWavePDEWithInit.fit_initial.<locals>._supervised_loss_one_k\u001b[39m\u001b[34m(k, X, Yk, Wk)\u001b[39m\n\u001b[32m     98\u001b[39m ylog = _safe_logit(Yk[m])\n\u001b[32m     99\u001b[39m t0 = torch.zeros(m.sum(), dtype=X.dtype, device=X.device)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m u0 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_u\u001b[49m(k, X[m], t0)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Wk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ((u0 - ylog) ** \u001b[32m2\u001b[39m).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/nn/modules/module.py:1962\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1960\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1961\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1962\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1963\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1964\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'WavePDEWithInit' object has no attribute 'eval_u'"
     ]
    }
   ],
   "source": [
    "# K targets, 10 timesteps, 512-d latents\n",
    "model = WavePDEWithInit(num_support_sets=len(DIMS), num_support_timesteps=10, support_vectors_dim=512).to(device)\n",
    "\n",
    "best_val = model.fit_initial(\n",
    "    train_loader, val_loader,\n",
    "    epochs=50, lr=1e-3, weight_decay=1e-2,\n",
    "    lambda_sup=1.0, lambda_pde=1.0, lambda_ic=1e-4,\n",
    "    lambda_jvp=0.0, use_jvp=False,\n",
    "    device=device,\n",
    "    log_every=2,                     # refresh heatmap every epoch\n",
    "    save_best_to=\"wavepde_init.pt\",\n",
    "    dim_names=DIMS,\n",
    "    viz_heatmap=True,\n",
    "    warmup_steps=100,\n",
    ")\n",
    "# Export for other code\n",
    "model.export_state_dict(\"wavepde_init.pt\")                     # one-file export\n",
    "model.export_per_attribute(\"wavepde_heads/\", dim_names=DIMS)   # per-attribute blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378073fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value = Y[:,-1].cpu().numpy()\n",
    "probability = sample_w[:,-1].cpu().numpy()\n",
    "# calculate the distribution of the values under the probability distribution\n",
    "\n",
    "# Plot the resulting density\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalize the probability weights\n",
    "probability = probability / probability.sum()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(value, bins=50, weights=probability, density=True, alpha=0.7, color='C0', edgecolor='k')\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Weighted Density of Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb8c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f77831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b5b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcc9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de2fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59efe037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b17259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82a972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7438e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manip311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
