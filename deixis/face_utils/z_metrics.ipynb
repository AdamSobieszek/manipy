{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b09109c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]='1'\n",
    "import torch\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from facefusion.types import VisionFrame\n",
    "\n",
    "# ---------------------------\n",
    "# Internal helpers\n",
    "# ---------------------------\n",
    "\n",
    "def _as_numpy(arr: Any) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Accepts a NumPy array or a PyTorch tensor. If it's a tensor, detach+cpu+numpy.\n",
    "    \"\"\"\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        return arr\n",
    "    # PyTorch-like tensor\n",
    "    if hasattr(arr, \"detach\") and hasattr(arr, \"cpu\") and hasattr(arr, \"numpy\"):\n",
    "        return arr.detach().cpu().numpy()\n",
    "    raise TypeError(\"Expected a NumPy array or a PyTorch tensor.\")\n",
    "\n",
    "def _to_uint8_0_255(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize to [0,255] uint8. Handles common cases:\n",
    "    - [-1, 1] float -> scale\n",
    "    - [0, 1] float  -> scale\n",
    "    - otherwise assumes already in [0,255]\n",
    "    \"\"\"\n",
    "    x = x.astype(np.float32, copy=False)\n",
    "    vmin, vmax = float(x.min()), float(x.max())\n",
    "    if vmin >= -1.01 and vmax <= 1.01:\n",
    "        x = x * 127.5 + 127.5\n",
    "    elif vmin >= 0.0 and vmax <= 1.0:\n",
    "        x = x * 255.0\n",
    "    x = np.clip(x, 0.0, 255.0)\n",
    "    return x.astype(np.uint8)\n",
    "\n",
    "def _hwc3_bgr(img: np.ndarray) -> VisionFrame:\n",
    "    \"\"\"\n",
    "    Ensure output is HWC, 3 channels, BGR, uint8, contiguous.\n",
    "    \"\"\"\n",
    "    if img.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D array (HWC or CHW), got shape {img.shape}\")\n",
    "\n",
    "    # If CHW, convert to HWC\n",
    "    if img.shape[0] in (1, 3, 4) and (img.shape[2] not in (1, 3, 4)):\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "    # Drop alpha if present\n",
    "    if img.shape[2] == 4:\n",
    "        img = img[:, :, :3]\n",
    "\n",
    "    # If grayscale, expand to 3 channels\n",
    "    if img.shape[2] == 1:\n",
    "        img = cv2.cvtColor(_to_uint8_0_255(img), cv2.COLOR_GRAY2BGR)\n",
    "        return np.ascontiguousarray(img, dtype=np.uint8)\n",
    "\n",
    "    # Assume RGB â†’ convert to BGR\n",
    "    img = _to_uint8_0_255(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    return np.ascontiguousarray(img, dtype=np.uint8)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) From a StyleGAN generator output tensor\n",
    "# ---------------------------\n",
    "\n",
    "def visionframe_from_stylegan(tensor: Any, select: int = 0) -> VisionFrame:\n",
    "    \"\"\"\n",
    "    Convert a StyleGAN(2) output to VisionFrame.\n",
    "    Accepts:\n",
    "      - torch.Tensor or np.ndarray\n",
    "      - shape (C,H,W) or (N,C,H,W) or (H,W,C) or (N,H,W,C)\n",
    "    Values can be in [-1,1], [0,1], or [0,255].\n",
    "    \"\"\"\n",
    "    arr = _as_numpy(tensor)\n",
    "\n",
    "    # Handle batch dimension\n",
    "    if arr.ndim == 4:\n",
    "        arr = arr[select]  # pick one sample\n",
    "\n",
    "    if arr.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D tensor after batching, got shape {arr.shape}\")\n",
    "\n",
    "    return _hwc3_bgr(arr)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) From a PIL.Image.Image\n",
    "# ---------------------------\n",
    "\n",
    "def visionframe_from_pil(pil_image: \"Image.Image\") -> VisionFrame:\n",
    "    \"\"\"\n",
    "    Convert a PIL image to VisionFrame, respecting EXIF orientation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PIL import ImageOps\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\"Pillow is required for visionframe_from_pil.\") from e\n",
    "\n",
    "    pil_image = ImageOps.exif_transpose(pil_image).convert(\"RGB\")\n",
    "    rgb = np.array(pil_image, dtype=np.uint8)  # HWC, RGB\n",
    "    bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "    return np.ascontiguousarray(bgr, dtype=np.uint8)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) From a path to an image\n",
    "# ---------------------------\n",
    "\n",
    "def visionframe_from_path(path: str) -> VisionFrame:\n",
    "    \"\"\"\n",
    "    Load an image from disk and return a VisionFrame (BGR, uint8).\n",
    "    Uses PIL to handle EXIF orientation correctly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PIL import Image, ImageOps\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\"Pillow is required for visionframe_from_path.\") from e\n",
    "\n",
    "    with Image.open(path) as im:\n",
    "        im = ImageOps.exif_transpose(im).convert(\"RGB\")\n",
    "        rgb = np.array(im, dtype=np.uint8)\n",
    "    bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "    return np.ascontiguousarray(bgr, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4175475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quality_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bac1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28d6ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from facefusion_wrapper import init_facefusion_state, ensure_facefusion_models\n",
    "from quality_metrics import *          # the metric we wrote\n",
    "\n",
    "# 1) Initialize state like the CLI would\n",
    "init_facefusion_state(\n",
    "    detector_model=\"retinaface\",\n",
    "    detector_size=\"640x640\",\n",
    "    detector_score=0.75,\n",
    "    detector_angles=(0,),         # add 90/-90 if you need rotated detection\n",
    "    use_landmarker_68=True,      # set True if you want real 68-pt landmarks\n",
    "    landmarker_score=0.5,\n",
    "    download_scope=\"full\",\n",
    ")\n",
    "\n",
    "# 2) Ensure models are downloaded & inference pools are ready\n",
    "ensure_facefusion_models(use_landmarker_68=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd4280f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'have_68': True, 'len_5': 5, 'lap_var': 824.5879426848888, 'mean': 151.0980521262003, 'std': 73.74237428882469, 'yaw_pitch_roll': (1.453731463530133, 58.24669680574795, -1.3509761095046997)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 222.56603713703257, 'mean': 93.49672202797203, 'std': 47.56995993435352, 'yaw_pitch_roll': (0.10289692354955951, -24.941466668751847, -0.11632030457258224)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 251.33148564304437, 'mean': 116.12865466101695, 'std': 39.971840258564086, 'yaw_pitch_roll': (-2.316541019304447, 37.10909749587302, 0.20465759932994843)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 553.2036838634896, 'mean': 154.05759885416134, 'std': 55.26678914768132, 'yaw_pitch_roll': (-1.0990503523723587, 3.392706027778289, 0.15638388693332672)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1193.1210611453298, 'mean': 124.82758620689656, 'std': 45.66959259316161, 'yaw_pitch_roll': (-4.491356239591607, 21.881027503658252, -0.6075028777122498)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 411.06007002272617, 'mean': 100.13077511083932, 'std': 57.43112242334647, 'yaw_pitch_roll': (2.618420646713194, -21.147822495671562, -0.23024670779705048)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 831.278704066637, 'mean': 119.10613207547169, 'std': 50.646301041976386, 'yaw_pitch_roll': (-5.197272419979161, -8.79641936608592, -0.7329083681106567)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 262.49971155608995, 'mean': 78.29254683140694, 'std': 38.435319554747736, 'yaw_pitch_roll': (1.9311204504805455, 58.58508799758818, 0.44365042448043823)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1154.2616437627412, 'mean': 120.51274911274912, 'std': 45.83221591852507, 'yaw_pitch_roll': (-2.1900889831582235, 4.746328386963824, -0.971699595451355)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1238.3090806783, 'mean': 115.94010278220804, 'std': 67.59097905181672, 'yaw_pitch_roll': (-7.26072716130929, 25.351443056952416, 1.2590521574020386)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 380.530614428923, 'mean': 77.77847840552616, 'std': 32.37384622117794, 'yaw_pitch_roll': (2.312725827888877, 32.07188322293832, 0.16760744154453278)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 323.73953453144554, 'mean': 99.53921226908331, 'std': 56.83253900099789, 'yaw_pitch_roll': (-1.71099357850727, 47.38513842396667, 0.3136235177516937)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 555.3496393025455, 'mean': 114.57291617473436, 'std': 31.91332725600785, 'yaw_pitch_roll': (0.4883281519294265, 34.96818246595344, -0.17406535148620605)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 598.2658907019171, 'mean': 111.0306553316068, 'std': 57.769994327303976, 'yaw_pitch_roll': (4.466975426109182, -8.238694386847918, 1.0323207378387451)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 584.0506546279433, 'mean': 147.74556105180943, 'std': 55.29198685775771, 'yaw_pitch_roll': (4.756383796340572, 14.77068582092189, 0.009726996533572674)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 368.56592618576127, 'mean': 81.80333217833218, 'std': 41.303722618900835, 'yaw_pitch_roll': (0.5278131954311271, -1.0451506126515095, -0.07473821938037872)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 341.9771169257259, 'mean': 137.03427251971772, 'std': 45.3016897222108, 'yaw_pitch_roll': (-0.6864870497356086, -0.5892998428880037, 0.2818930149078369)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 580.2887925502516, 'mean': 148.62571214392804, 'std': 55.405295235970044, 'yaw_pitch_roll': (-4.67862896984172, 37.836570243071144, 0.5516726970672607)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 681.9363223774413, 'mean': 130.17073297073298, 'std': 40.86116775762637, 'yaw_pitch_roll': (2.8886175011889077, 16.241054599440716, -0.08087649196386337)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 330.7538706375051, 'mean': 94.92131979695432, 'std': 28.471359718253364, 'yaw_pitch_roll': (-6.851021232182556, 13.437291263700397, 0.4824869930744171)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 320.24471556839507, 'mean': 100.2618942121296, 'std': 35.88442487582813, 'yaw_pitch_roll': (-3.2453979969164504, 1.6261760632391418, -0.7330000996589661)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 646.0569748895138, 'mean': 82.31452356176602, 'std': 63.519107071343335, 'yaw_pitch_roll': (1.17622196859569, -10.94103421589552, 0.48357823491096497)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 424.3544249470545, 'mean': 98.01914836235055, 'std': 37.81942746327698, 'yaw_pitch_roll': (-1.1373670251833083, 51.513643178045896, 0.5762176513671875)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1007.1533272272576, 'mean': 102.80712608668063, 'std': 61.681902028606785, 'yaw_pitch_roll': (-2.358512133313765, 39.213839304714185, 0.28878793120384216)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 660.0643231496066, 'mean': 103.20603607296577, 'std': 60.14243594502508, 'yaw_pitch_roll': (1.011953471245345, 67.01699786019896, 0.26850593090057373)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 566.3079680991945, 'mean': 102.2297082979326, 'std': 58.47396887863991, 'yaw_pitch_roll': (0.2525692223227626, -20.25396093805294, 0.22550778090953827)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 692.6077106978814, 'mean': 139.17738889588315, 'std': 54.16467151490885, 'yaw_pitch_roll': (-6.418745378859782, -7.313126713151328, 0.2250899374485016)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 349.2478716559498, 'mean': 129.5198632367307, 'std': 39.97472488531665, 'yaw_pitch_roll': (-1.751665363652865, -4.2383797699242525, 0.9110018014907837)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 640.1201345522583, 'mean': 79.76568769057936, 'std': 76.47761291703661, 'yaw_pitch_roll': (1.3625131622045095, 70.3615483989499, -1.7132214307785034)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 336.0800983510832, 'mean': 110.94091415830546, 'std': 29.04038184212919, 'yaw_pitch_roll': (0.08427214402084302, 69.81388699174123, -0.2927221357822418)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 571.3423407618205, 'mean': 95.71461588541666, 'std': 53.32749185868761, 'yaw_pitch_roll': (1.3571707062013731, 33.15795363967952, 0.26754581928253174)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 614.7848504640999, 'mean': 107.93083870967742, 'std': 71.4697896290394, 'yaw_pitch_roll': (2.4226715547735553, -27.769803902953996, -1.0327167510986328)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 300.38581730313473, 'mean': 88.68049844549192, 'std': 56.15048024232259, 'yaw_pitch_roll': (0.7614249414902742, 4.327694204518729, 0.36890318989753723)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 447.5829417768625, 'mean': 143.63420551563172, 'std': 64.28134132668926, 'yaw_pitch_roll': (-1.4862325603638666, 3.412847971963022, -0.016797669231891632)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 797.185441703366, 'mean': 133.4511684303351, 'std': 50.40930941072456, 'yaw_pitch_roll': (0.8279976423487286, 10.582143737754514, 0.4140380024909973)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 473.30178061679095, 'mean': 114.44212796549245, 'std': 50.78483311815767, 'yaw_pitch_roll': (3.3890501664619133, 42.47156097886406, -0.8096292614936829)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 985.8693348212016, 'mean': 118.96125136545697, 'std': 69.44162922091655, 'yaw_pitch_roll': (4.608340418118329, 58.05035042590024, -0.10824241489171982)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1310.888473414601, 'mean': 84.62148184632704, 'std': 85.67875887804124, 'yaw_pitch_roll': (-1.1261949632131902, 13.151464271438975, -3.6539533138275146)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 593.0656503954322, 'mean': 114.23343883007749, 'std': 52.23845411885191, 'yaw_pitch_roll': (1.3862314421997408, 6.939437387028327, -1.398980975151062)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 725.5704536365648, 'mean': 132.57631231864943, 'std': 52.1899096743214, 'yaw_pitch_roll': (-3.148165646308817, 9.406666010472966, 0.5125710368156433)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 650.811642934667, 'mean': 91.0934850772268, 'std': 48.57160087567792, 'yaw_pitch_roll': (2.8133395439579267, 62.581113892746075, -0.3031638264656067)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 300.8306883525559, 'mean': 129.0140689737399, 'std': 51.50888353185569, 'yaw_pitch_roll': (-2.4150733945009746, 45.43844791272266, -0.11382322758436203)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 399.3225562455602, 'mean': 109.97962243368112, 'std': 41.128062187111645, 'yaw_pitch_roll': (-5.913562471649292, -4.200627231981945, -0.9474038481712341)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 495.1494001849795, 'mean': 155.2776951974271, 'std': 47.6911276348783, 'yaw_pitch_roll': (3.0567744984679788, 23.390807131038738, 0.00957449059933424)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 373.55161643323277, 'mean': 114.08818237963534, 'std': 56.77499473443874, 'yaw_pitch_roll': (-5.464592180433286, 5.3024803446139614, 0.01940646767616272)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 604.374755058839, 'mean': 111.1537657479232, 'std': 56.98828079842132, 'yaw_pitch_roll': (2.736744034564654, -1.5940666228504696, 0.633441686630249)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 475.96445783583107, 'mean': 128.2588172675575, 'std': 44.84445288016363, 'yaw_pitch_roll': (5.5216262522776, 2.4540530239577456, 0.7232893705368042)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 501.1723886971909, 'mean': 124.75079801871216, 'std': 68.79182406744731, 'yaw_pitch_roll': (3.861578855778978, -8.77830024545925, 0.19437898695468903)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1297.2766058459567, 'mean': 122.16627477326168, 'std': 58.747314144975554, 'yaw_pitch_roll': (-5.332478636658046, 28.997792975980804, 0.04893779754638672)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 653.1912859351436, 'mean': 113.83542515169147, 'std': 77.0127854279368, 'yaw_pitch_roll': (-4.412744068034425, 22.29722708133632, 0.14511698484420776)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 411.10327133162315, 'mean': 137.7390354609929, 'std': 53.42713003475403, 'yaw_pitch_roll': (-2.9810448826702722, 35.500831360325364, -0.08064481616020203)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 588.4773302950888, 'mean': 83.64810035842294, 'std': 49.01184885242119, 'yaw_pitch_roll': (-0.6105598967008288, 34.60781836051851, -0.44637203216552734)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 645.8392076501382, 'mean': 96.5999066293184, 'std': 44.526251204939626, 'yaw_pitch_roll': (5.658718242033905, 44.00457349601231, -0.9669588804244995)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 689.0339397128761, 'mean': 110.50342003853565, 'std': 40.06504571457093, 'yaw_pitch_roll': (0.991161245213148, 38.10041048893721, -0.19825376570224762)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 520.1166007030284, 'mean': 116.84680037704231, 'std': 66.57982368398149, 'yaw_pitch_roll': (0.7655081888463197, 55.843061646736444, 0.588891327381134)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 403.0918738960611, 'mean': 130.7193113772455, 'std': 48.13603974253028, 'yaw_pitch_roll': (-5.580833168249514, 37.82497811303245, 0.01984924077987671)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 252.5741101496873, 'mean': 71.60110698569856, 'std': 48.07917579747225, 'yaw_pitch_roll': (-0.4163354806866298, 32.85784309863652, -0.10745121538639069)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 680.7265376028795, 'mean': 131.69951499118164, 'std': 52.36970886199712, 'yaw_pitch_roll': (2.241332224016146, 40.86345621420259, 0.2522799074649811)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 288.5578039754848, 'mean': 123.10926824179836, 'std': 54.01934573667148, 'yaw_pitch_roll': (0.5049385794426228, 28.238505605238082, 0.6117807626724243)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 637.3220897737112, 'mean': 116.45934804667681, 'std': 63.752471297439264, 'yaw_pitch_roll': (3.0424097286662066, 19.33346131501817, 1.0967930555343628)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 674.3133773036013, 'mean': 92.59683460828278, 'std': 58.01583617117663, 'yaw_pitch_roll': (-2.699066464694531, 8.066920694198526, -0.6168681383132935)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 310.2019154628971, 'mean': 115.20081525312294, 'std': 57.456733989404434, 'yaw_pitch_roll': (-5.37989709825779, 71.36759750545525, 1.0193517208099365)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 241.75730629344838, 'mean': 109.94077085215297, 'std': 43.5256930204881, 'yaw_pitch_roll': (1.740418786035674, 32.141279025651805, 0.12065504491329193)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 304.9972087024492, 'mean': 92.29305350098619, 'std': 45.27287906100063, 'yaw_pitch_roll': (5.037597751651752, 16.324802313972167, 0.09225759655237198)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 565.765443613894, 'mean': 81.30781824256599, 'std': 48.25753701479963, 'yaw_pitch_roll': (4.499609852578183, -5.606029825071425, -0.2101343274116516)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1124.9964483678384, 'mean': 117.98854864433812, 'std': 79.61808697257598, 'yaw_pitch_roll': (-2.1215269914401795, 62.13437467089807, 0.48668670654296875)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 609.0223820146832, 'mean': 121.48168143969917, 'std': 60.56988757960431, 'yaw_pitch_roll': (-4.278943411891854, 65.03047795377012, 2.245037078857422)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 802.2661057165024, 'mean': 138.17363251155624, 'std': 56.833471005044956, 'yaw_pitch_roll': (-3.9709869160365794, 43.691246226990316, 0.8135790824890137)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 886.592532418542, 'mean': 103.96391725444784, 'std': 54.36415938587781, 'yaw_pitch_roll': (1.615844787549385, 7.819174991598858, 0.4179936945438385)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 783.5312104562505, 'mean': 94.53674568965518, 'std': 65.95209653745152, 'yaw_pitch_roll': (1.8498416385244565, 3.6832621950405864, -1.3289004564285278)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 533.819355461514, 'mean': 140.82822213551827, 'std': 61.33815996667033, 'yaw_pitch_roll': (-5.572144745773557, -9.153971082271763, -0.4522826075553894)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 495.63231924264875, 'mean': 106.64088085827217, 'std': 61.27888380165682, 'yaw_pitch_roll': (6.700779647722218, -1.9282348457153413, 0.43397945165634155)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 605.9352272709046, 'mean': 106.37043245055314, 'std': 57.65226425204377, 'yaw_pitch_roll': (-2.313742591519575, 24.8866305634404, 0.30566179752349854)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 550.9952393077303, 'mean': 155.4340634137951, 'std': 44.74470737376383, 'yaw_pitch_roll': (1.8302116424547443, -4.226634690469662, -0.10613726824522018)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 747.1889003348435, 'mean': 135.60424297924297, 'std': 55.507849055637635, 'yaw_pitch_roll': (3.183932986306216, 25.452077587769335, 0.31777575612068176)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 623.1842047999959, 'mean': 119.5864782634135, 'std': 70.7589096511279, 'yaw_pitch_roll': (3.2499239363210397, 6.998820797847769, 0.21983379125595093)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 510.6667655399836, 'mean': 164.11764049955397, 'std': 36.47154106430821, 'yaw_pitch_roll': (-4.560607174372118, -17.659792579376074, -1.1824320554733276)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 311.2840483809564, 'mean': 95.94111583558819, 'std': 55.21336551245603, 'yaw_pitch_roll': (-3.6291340641379364, 46.290107881639635, 0.9838126301765442)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 896.8023139988226, 'mean': 126.14514705882353, 'std': 40.752353005299824, 'yaw_pitch_roll': (3.122547983877022, 2.6485161306582126, -0.36163821816444397)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 365.836673819336, 'mean': 121.87121875, 'std': 56.38615806773546, 'yaw_pitch_roll': (2.896010885093338, 16.171411975847757, -0.6441255211830139)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 687.737911963, 'mean': 107.11332179930795, 'std': 65.86736479954364, 'yaw_pitch_roll': (-4.602792853456232, 59.81926614723059, 1.1890144348144531)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 653.2350016364885, 'mean': 165.4762568442011, 'std': 65.49469617359469, 'yaw_pitch_roll': (-3.015278400836927, 0.41916854259173086, -0.26358479261398315)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 621.4224724240678, 'mean': 66.17647058823529, 'std': 45.48986350453142, 'yaw_pitch_roll': (-3.7393005687930816, 16.324140637828894, 0.43224117159843445)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 731.5983065833092, 'mean': 97.83098670242485, 'std': 69.72888197507353, 'yaw_pitch_roll': (2.371498671579419, 21.859010670658925, 0.03858499228954315)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 381.6641824508917, 'mean': 106.93736434108527, 'std': 55.52530632662647, 'yaw_pitch_roll': (0.5727494182816938, -9.820475393720395, -0.37011122703552246)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 503.55337770690113, 'mean': 112.64312122289371, 'std': 50.53752246069117, 'yaw_pitch_roll': (2.8821925722571233, 50.790240753627934, -0.15204201638698578)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 640.7155632454668, 'mean': 112.11147684880403, 'std': 49.18752645391594, 'yaw_pitch_roll': (-2.4531967070156195, 44.49638882598239, 0.24593588709831238)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 965.1715969771982, 'mean': 86.49297769740808, 'std': 40.78331993800735, 'yaw_pitch_roll': (2.7491578311005926, 45.12567888729812, 0.17749352753162384)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 597.6480669041161, 'mean': 109.27402640264026, 'std': 63.49988845362695, 'yaw_pitch_roll': (2.936327882423062, -9.536356879655072, 0.5850846171379089)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 685.8017479127519, 'mean': 91.0556984821199, 'std': 40.48299798845078, 'yaw_pitch_roll': (0.009784182476146167, 61.92319962489296, -0.5285111665725708)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1088.22246220886, 'mean': 105.6150772972034, 'std': 61.926030926778004, 'yaw_pitch_roll': (-0.22543208169541062, -32.21207545436144, 0.5475003719329834)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 433.154321895942, 'mean': 135.71575663758327, 'std': 36.36064489529416, 'yaw_pitch_roll': (-3.1519691759932846, -30.753102646358006, -0.7732581496238708)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 277.2368789884893, 'mean': 87.06154947217749, 'std': 39.17768808353026, 'yaw_pitch_roll': (-3.1824542595126415, -2.5113753862102604, 0.684209406375885)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 934.71366676093, 'mean': 119.149666600647, 'std': 53.75813731806185, 'yaw_pitch_roll': (7.498235234495791, -36.40044107529117, 2.8370821475982666)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 839.0051770477696, 'mean': 166.56347748815165, 'std': 50.625509174123906, 'yaw_pitch_roll': (-1.842334381046041, 35.880818245587385, 0.5457385778427124)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 972.1157749497049, 'mean': 101.57250998225378, 'std': 66.28079895217124, 'yaw_pitch_roll': (3.0350224883041284, 25.977219178187124, 0.23406200110912323)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 484.6644701368608, 'mean': 110.77917144063197, 'std': 61.78611779190328, 'yaw_pitch_roll': (-1.76373617909288, 64.61886503224733, 0.9806804060935974)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 767.4645447277801, 'mean': 137.65533529123837, 'std': 47.746587212882545, 'yaw_pitch_roll': (4.524540463906588, 18.724852545655935, -0.22072754800319672)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 314.18511812472354, 'mean': 108.33397742608268, 'std': 55.07216697221531, 'yaw_pitch_roll': (3.7789637039480635, 29.642842138802507, -0.33174964785575867)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 329.9098565489635, 'mean': 80.20986689814815, 'std': 39.83591955930052, 'yaw_pitch_roll': (1.500680806347489, 28.673706339612544, 0.22882209718227386)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 443.9985698815778, 'mean': 112.70516659407666, 'std': 54.31659215812567, 'yaw_pitch_roll': (6.535282408322814, -9.35071443387364, 0.6097075939178467)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 317.97895208830676, 'mean': 133.9475118105672, 'std': 50.88302034451201, 'yaw_pitch_roll': (2.065970448542515, 39.32624679776975, -0.3299639821052551)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 505.68776960981626, 'mean': 111.71829212454213, 'std': 53.973328227038905, 'yaw_pitch_roll': (0.1757434560476827, 20.260404201376936, 0.05858134850859642)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 432.0678060407673, 'mean': 129.76898699294532, 'std': 61.0480194741149, 'yaw_pitch_roll': (5.845058551403913, 33.59707711572167, -0.01939520239830017)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1071.665025577524, 'mean': 84.48452676756268, 'std': 57.682010162459704, 'yaw_pitch_roll': (1.4328204775844562, 2.3767966615168334, -0.15652921795845032)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 777.370357414327, 'mean': 116.69262903314461, 'std': 50.18620260790937, 'yaw_pitch_roll': (4.0700053639289315, 7.52963203296904, -0.327743798494339)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 630.5891423205633, 'mean': 105.34989500888386, 'std': 54.904561263981826, 'yaw_pitch_roll': (-2.8773900572310205, 11.391468490596163, -0.5617098212242126)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 433.90575855442256, 'mean': 105.96166201963874, 'std': 44.09983910012889, 'yaw_pitch_roll': (-4.343030001488087, 44.44435635099172, -0.1526845544576645)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 596.1439272884224, 'mean': 123.44527582159624, 'std': 52.76884382382975, 'yaw_pitch_roll': (6.575689326575297, -2.979149274136786, -0.1732129454612732)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 221.10720579137015, 'mean': 114.78189792663477, 'std': 39.863926402067754, 'yaw_pitch_roll': (2.7689923697461682, 28.539539675862443, -0.17970122396945953)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 550.6387875763158, 'mean': 148.31811256290686, 'std': 62.53701215523722, 'yaw_pitch_roll': (7.747803349381674, 19.27967200420998, -1.0673785209655762)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 755.9077942308435, 'mean': 131.7861007694217, 'std': 50.77465059577593, 'yaw_pitch_roll': (-2.9118426892693092, 27.707757880292203, -0.049571674317121506)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 584.0318038864352, 'mean': 159.5497525817556, 'std': 39.30869583362136, 'yaw_pitch_roll': (-3.086721617992542, 39.75854908435322, 0.2800346314907074)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 281.5113686441975, 'mean': 125.11391018619935, 'std': 50.98325706465545, 'yaw_pitch_roll': (4.836721671825929, 1.961143718902249, 0.05577974393963814)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 301.00597777568487, 'mean': 136.72400060493018, 'std': 55.12562696572958, 'yaw_pitch_roll': (1.979104964281307, 22.138985878924384, 0.04041564092040062)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 361.7482470581976, 'mean': 131.1772933804532, 'std': 39.30192302422861, 'yaw_pitch_roll': (-0.6317367074936467, 15.227016930215717, -0.21850472688674927)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 307.8810652493131, 'mean': 110.35879194630873, 'std': 47.40213082335327, 'yaw_pitch_roll': (-1.476291761408548, -15.625361105718595, -0.8660784959793091)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 944.6485838207332, 'mean': 115.27958728388177, 'std': 71.72689980437144, 'yaw_pitch_roll': (-4.1863651037880345, 65.90024198677901, -2.3264760971069336)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 734.8921070094484, 'mean': 117.48259020914732, 'std': 72.37561577599104, 'yaw_pitch_roll': (-3.030726791571169, 57.0031075662026, 0.9723372459411621)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 420.6852322032301, 'mean': 169.96609111712087, 'std': 54.79203651349874, 'yaw_pitch_roll': (-6.960483555562319, 17.046267761921705, 0.4247192144393921)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 987.6424817400591, 'mean': 116.75262972321796, 'std': 52.414527840392324, 'yaw_pitch_roll': (4.236931297989871, 0.18361789404504242, 0.5330329537391663)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 347.5757934141519, 'mean': 99.20245161290323, 'std': 49.94135767971091, 'yaw_pitch_roll': (5.912437985262433, 21.506451253803384, -0.6470003724098206)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 767.4222871633839, 'mean': 91.33064129217975, 'std': 69.41489018150695, 'yaw_pitch_roll': (-2.694455747362822, 45.870681035645454, 0.32574009895324707)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1335.617454945952, 'mean': 127.54773120425816, 'std': 49.40809196281643, 'yaw_pitch_roll': (4.106513834742132, 6.158841700122927, 0.4039793312549591)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 602.5285035091941, 'mean': 107.68811053124581, 'std': 57.67829287961214, 'yaw_pitch_roll': (0.7679145502501005, 32.152752045710116, 0.35684096813201904)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 679.3826442639191, 'mean': 115.95048259645094, 'std': 54.74431944425867, 'yaw_pitch_roll': (2.0139914676530632, 12.161237481582782, 0.6089392900466919)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 276.44037331914427, 'mean': 94.32121093622706, 'std': 39.906587093037686, 'yaw_pitch_roll': (4.986599520897878, 28.507083016299358, -0.4851192831993103)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 464.9357350285871, 'mean': 128.85155555555556, 'std': 54.60282947310639, 'yaw_pitch_roll': (3.828512599518537, 18.67887450361288, 0.3675276041030884)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 535.057326716798, 'mean': 136.24144647696477, 'std': 55.442799508001016, 'yaw_pitch_roll': (3.6917672722193777, -7.648577228490052, 0.3381579518318176)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 610.2898338179409, 'mean': 142.6724733207784, 'std': 72.25201412028036, 'yaw_pitch_roll': (-2.431992590078358, 5.5860378853643535, -2.0393972396850586)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 231.5551213010204, 'mean': 138.83270408163264, 'std': 44.63041851667601, 'yaw_pitch_roll': (-2.679663602740478, 2.6122595626782927, -0.2351451814174652)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 850.3828788947161, 'mean': 127.47424242424242, 'std': 65.11510119649888, 'yaw_pitch_roll': (-0.3108561205087804, -4.960084176966856, -0.5463742017745972)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 823.9516368124408, 'mean': 109.03009804666718, 'std': 59.33824391305173, 'yaw_pitch_roll': (5.2927653916968165, -3.852057577140611, -1.7189927101135254)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 470.7253397298322, 'mean': 132.62572261355356, 'std': 47.62742308456116, 'yaw_pitch_roll': (-2.8861592020566142, 11.80901775322709, 0.21926282346248627)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 827.8771809628615, 'mean': 107.65990767252966, 'std': 51.52114429656454, 'yaw_pitch_roll': (-1.696269161437353, 8.427946153173528, -2.2852590084075928)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 484.8245427725677, 'mean': 97.26463176463176, 'std': 43.023687297079114, 'yaw_pitch_roll': (1.5758175119886138, 10.819212583287774, -0.5302342772483826)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 334.77289563496385, 'mean': 118.10558671879427, 'std': 40.893356093162105, 'yaw_pitch_roll': (-3.795575624965082, 30.26553099595001, 0.16053886711597443)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1291.0813683439503, 'mean': 119.90801538006507, 'std': 42.01413894483281, 'yaw_pitch_roll': (-0.2082624777395024, -21.089986759470868, -0.24614068865776062)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 324.32086042637206, 'mean': 69.0804500703235, 'std': 44.92195761571503, 'yaw_pitch_roll': (-6.0021604690668955, 48.728503192520975, 1.2428919076919556)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 703.1449477926726, 'mean': 62.10233786669664, 'std': 39.48050073562467, 'yaw_pitch_roll': (3.247982828391409, 40.71897240520328, -0.8549730181694031)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 334.0557719553794, 'mean': 107.24305307096004, 'std': 43.402772662094634, 'yaw_pitch_roll': (5.345060708961245, 61.25053183475984, -0.0639275386929512)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 300.81268885644414, 'mean': 108.87034153430915, 'std': 51.86008230092116, 'yaw_pitch_roll': (-4.25075817461795, 24.968513092617854, -0.48777276277542114)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 584.9439275911059, 'mean': 124.77930597771024, 'std': 56.87978802741054, 'yaw_pitch_roll': (-2.2010226065175145, 28.896838477339013, 0.4850378930568695)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 512.7888899712568, 'mean': 83.59466848940534, 'std': 47.993174130278774, 'yaw_pitch_roll': (-4.203539660046718, 37.01176077292383, -0.25306564569473267)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 442.4158757897727, 'mean': 120.59771141984204, 'std': 60.09335809136985, 'yaw_pitch_roll': (-0.9312495122969511, 36.3571833179644, -0.474300742149353)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 587.4160515436287, 'mean': 168.13616238463456, 'std': 54.061576375916125, 'yaw_pitch_roll': (0.18782644107137483, 5.825443474189401, 0.3309195935726166)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 813.2778269835618, 'mean': 138.2852443308847, 'std': 58.21984499102254, 'yaw_pitch_roll': (6.705483140696646, 30.463067706665694, -1.0337309837341309)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 835.5200246684012, 'mean': 119.88573232323232, 'std': 46.67735914808656, 'yaw_pitch_roll': (-3.7674884038967456, 61.07228047399864, 1.2460092306137085)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 620.7711276025561, 'mean': 148.82356215213358, 'std': 66.45428568209599, 'yaw_pitch_roll': (-1.366653597291706, 10.41646157181401, -1.0775619745254517)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 795.933003838595, 'mean': 115.942780363112, 'std': 43.88153943287635, 'yaw_pitch_roll': (-2.254651530077868, 19.954464519163544, -0.4890991449356079)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 536.0509878703755, 'mean': 94.36940133037695, 'std': 62.872801666285945, 'yaw_pitch_roll': (-1.8237128872932642, -22.653324498847994, -0.1082926094532013)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 540.7792892641983, 'mean': 88.58332643202209, 'std': 65.71970191332788, 'yaw_pitch_roll': (-3.282873444138139, 14.539618092588748, 0.8372758626937866)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 379.6609047375226, 'mean': 112.39681339454496, 'std': 52.28771781288556, 'yaw_pitch_roll': (3.029989234539165, 13.270451203681892, 0.29942208528518677)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1000.725184688483, 'mean': 93.82185868836206, 'std': 71.2863385783474, 'yaw_pitch_roll': (-0.9920087978905903, 51.245542896417334, 0.2753199636936188)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 594.4805661616184, 'mean': 126.73163676736984, 'std': 51.05000627238983, 'yaw_pitch_roll': (3.152585463218979, 23.36944741011121, -0.4056686758995056)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 764.9975087306428, 'mean': 103.66622162883846, 'std': 46.557871741569315, 'yaw_pitch_roll': (-0.9078695179105398, -18.92052472843139, -0.705722451210022)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 584.0411735024734, 'mean': 138.7999033212011, 'std': 63.10008076231641, 'yaw_pitch_roll': (2.3573139758382804, 56.059241472840185, -0.3295523226261139)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1074.344285521919, 'mean': 126.58831453634085, 'std': 53.122874140370534, 'yaw_pitch_roll': (-3.3816304965225146, 16.41564518147928, 0.8207653164863586)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 1579.182177103701, 'mean': 76.15656565656566, 'std': 67.86222802088903, 'yaw_pitch_roll': (4.452418247026977, 10.86450362210306, 0.03796219080686569)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 502.4042575897228, 'mean': 103.61788373568399, 'std': 50.57777665719138, 'yaw_pitch_roll': (4.077573219702899, -3.4786247503614796, 0.6564345359802246)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 575.8624133492768, 'mean': 99.02117665790941, 'std': 49.501532256609146, 'yaw_pitch_roll': (4.966892411906257, 10.690030421704943, 0.5122753381729126)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 319.7959303343428, 'mean': 109.30734217296656, 'std': 51.48673338831303, 'yaw_pitch_roll': (-4.2850008637762524, 18.33121573283433, 0.35576602816581726)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 826.8095864294437, 'mean': 113.08212560386474, 'std': 43.187036044015656, 'yaw_pitch_roll': (2.7453692097549776, 11.101831083582008, -0.35093337297439575)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 666.4809157835075, 'mean': 116.88097491039427, 'std': 62.40158736305895, 'yaw_pitch_roll': (-3.1505425667514304, -7.2585953758865775, -0.7456288933753967)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 483.8149045220701, 'mean': 68.19467121198528, 'std': 43.7933287952923, 'yaw_pitch_roll': (2.9547703139031993, 13.88784881819544, -0.3117768168449402)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 422.168368804091, 'mean': 92.84511440940013, 'std': 45.12268892252716, 'yaw_pitch_roll': (0.7500981976038636, -5.133783469185276, -0.2943602502346039)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 178.48653054045036, 'mean': 62.09594594594594, 'std': 50.88446149860569, 'yaw_pitch_roll': (5.370357205115763, 1.149311261284946, -0.13978631794452667)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 409.2833332435037, 'mean': 107.01651123681394, 'std': 46.80004373591873, 'yaw_pitch_roll': (-3.3228011326811893, 12.230608152085104, -0.7630179524421692)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 573.6505964919245, 'mean': 100.61884526035469, 'std': 49.7921306388905, 'yaw_pitch_roll': (1.9666061770452472, 42.801737661568204, 0.02844739332795143)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 234.51447395460394, 'mean': 144.45583075335398, 'std': 47.31594863331055, 'yaw_pitch_roll': (-4.012593152279055, 32.57406263408497, 0.6849943995475769)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 865.436604036486, 'mean': 126.60949772633319, 'std': 63.41397530569915, 'yaw_pitch_roll': (-0.219143360545649, 45.28699181416421, -0.0996149480342865)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 310.50540900964955, 'mean': 126.81157751491678, 'std': 57.071964043833894, 'yaw_pitch_roll': (3.405754551374435, 80.2315954420846, 0.0)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 779.3985854150934, 'mean': 114.59270386266094, 'std': 55.19735722237374, 'yaw_pitch_roll': (4.037709638888178, 18.00418246455738, -0.31620287895202637)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 319.2925900702751, 'mean': 164.11835585585587, 'std': 39.53374101736796, 'yaw_pitch_roll': (0.9400205298376155, 31.48443697218331, 0.30869293212890625)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 531.5963345423494, 'mean': 96.55323711818235, 'std': 62.181448724277004, 'yaw_pitch_roll': (-2.512120049858327, 32.719142340379044, -0.23134927451610565)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 379.199371851137, 'mean': 126.38582840543624, 'std': 34.33192912463321, 'yaw_pitch_roll': (0.4722461617921849, 20.630411103212047, 0.4722689688205719)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 754.1301446842376, 'mean': 134.4071629971804, 'std': 56.99115138595822, 'yaw_pitch_roll': (-4.683651263262712, 47.57773949109242, 0.5533097982406616)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 456.7930413337884, 'mean': 81.47262807717898, 'std': 39.58260927098328, 'yaw_pitch_roll': (2.656115186666328, -18.68734613157451, 0.2718919515609741)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 749.7520706530335, 'mean': 186.66075661325496, 'std': 59.23071901215589, 'yaw_pitch_roll': (-4.659617441106622, 2.836686105332608, -0.6739410758018494)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 465.4997157024793, 'mean': 97.95983838383839, 'std': 48.27810822563301, 'yaw_pitch_roll': (-7.061880467666242, 4.772388212288193, 0.10644301027059555)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 251.09483682214596, 'mean': 114.32386539055264, 'std': 45.85648320725296, 'yaw_pitch_roll': (-0.19040599137805625, 6.309587259186247, 0.3499194383621216)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 499.56774724076865, 'mean': 125.14096583028622, 'std': 32.993162674727266, 'yaw_pitch_roll': (-3.910262795768458, 21.822786143861094, 0.7912031412124634)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 472.62354582113653, 'mean': 104.70463741051977, 'std': 43.130403605682865, 'yaw_pitch_roll': (0.1521528071743729, 6.658299590432536, -0.212097629904747)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 400.7627562047536, 'mean': 98.0703312664097, 'std': 63.73668645632695, 'yaw_pitch_roll': (-2.462737692482986, 24.126350515466488, 0.2505985498428345)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 497.2735660400727, 'mean': 119.32003007692752, 'std': 57.75443260166215, 'yaw_pitch_roll': (2.8069108222487986, 63.91057934362343, -0.7095935344696045)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 279.00461039501516, 'mean': 116.29632867132867, 'std': 43.585194531234364, 'yaw_pitch_roll': (0.0381379921961537, 21.268259202589086, 0.5084452033042908)}\n",
      "{'have_68': True, 'len_5': 5, 'lap_var': 586.3246594835671, 'mean': 96.34298245614035, 'std': 56.12163193785542, 'yaw_pitch_roll': (6.035765846033716, -10.229766484072245, 1.0248222351074219)}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     32\u001b[39m vf = visionframe_from_pil(Image.open(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/Users/adamsobieszek/PycharmProjects/_manipy/selected_age_additional3/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m).resize((\u001b[32m256\u001b[39m,\u001b[32m256\u001b[39m)))\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m reject, score, subs, reasons = \u001b[43mshould_reject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# metrics = [\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m#     # new edge-partial metrics\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m#     # m_edge_partial,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# values = evaluate_metrics(vf, metrics)\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# reject = (values.get(\"edge_left_fire\", 0.0) >= 0.5) or (values.get(\"edge_right_fire\", 0.0) >= 0.5)\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reject:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/quality_metrics.py:460\u001b[39m, in \u001b[36mshould_reject\u001b[39m\u001b[34m(vision_frame, accept_threshold)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshould_reject\u001b[39m(vision_frame: VisionFrame,\n\u001b[32m    456\u001b[39m                   accept_threshold: \u001b[38;5;28mfloat\u001b[39m = DEFAULT_ACCEPT_THRESHOLD) -> Tuple[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;28mfloat\u001b[39m], List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m    457\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    Returns (reject, score, subscores, reasons)\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     r = \u001b[43mscore_stylegan2_ffhq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r.reject, r.score, r.subscores, r.reasons\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/quality_metrics.py:314\u001b[39m, in \u001b[36mscore_stylegan2_ffhq\u001b[39m\u001b[34m(vision_frame, accept_threshold)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore_stylegan2_ffhq\u001b[39m(vision_frame: VisionFrame,\n\u001b[32m    311\u001b[39m                          accept_threshold: \u001b[38;5;28mfloat\u001b[39m = DEFAULT_ACCEPT_THRESHOLD) -> QualityResult:\n\u001b[32m    313\u001b[39m     h, w = vision_frame.shape[:\u001b[32m2\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     faces = \u001b[43mget_many_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvision_frame\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m     \u001b[38;5;66;03m# --- new: reject if multiple faces detected ---\u001b[39;00m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m faces \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) >= \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/quality_metrics.py:41\u001b[39m, in \u001b[36mget_many_faces\u001b[39m\u001b[34m(vision_frames)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m face_detector_angle \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0\u001b[39m]:\n\u001b[32m     40\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m face_detector_angle == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \t\tbounding_boxes, face_scores, face_landmarks_5 = \u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     43\u001b[39m \t\tbounding_boxes, face_scores, face_landmarks_5 = detect_faces_by_angle(vision_frame, face_detector_angle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/facefusion/face_detector.py:136\u001b[39m, in \u001b[36mdetect_faces\u001b[39m\u001b[34m(vision_frame)\u001b[39m\n\u001b[32m    133\u001b[39m all_face_landmarks_5 : List[FaceLandmark5] = []\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state_manager.get_item(\u001b[33m'\u001b[39m\u001b[33mface_detector_model\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m [ \u001b[33m'\u001b[39m\u001b[33mmany\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mretinaface\u001b[39m\u001b[33m'\u001b[39m ]:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \tbounding_boxes, face_scores, face_landmarks_5 = \u001b[43mdetect_with_retinaface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_item\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mface_detector_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \tall_bounding_boxes.extend(bounding_boxes)\n\u001b[32m    138\u001b[39m \tall_face_scores.extend(face_scores)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/facefusion/face_detector.py:187\u001b[39m, in \u001b[36mdetect_with_retinaface\u001b[39m\u001b[34m(vision_frame, face_detector_size)\u001b[39m\n\u001b[32m    185\u001b[39m detect_vision_frame = prepare_detect_frame(temp_vision_frame, face_detector_size)\n\u001b[32m    186\u001b[39m detect_vision_frame = normalize_detect_frame(detect_vision_frame, [ -\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m ])\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m detection = \u001b[43mforward_with_retinaface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetect_vision_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, feature_stride \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(feature_strides):\n\u001b[32m    190\u001b[39m \tface_scores_raw = detection[index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/facefusion/face_detector.py:366\u001b[39m, in \u001b[36mforward_with_retinaface\u001b[39m\u001b[34m(detect_vision_frame)\u001b[39m\n\u001b[32m    363\u001b[39m face_detector = get_inference_pool().get(\u001b[33m'\u001b[39m\u001b[33mretinaface\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m thread_semaphore():\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m \tdetection = \u001b[43mface_detector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetect_vision_frame\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m detection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:273\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    271\u001b[39m     output_names = [output.name \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._outputs_meta]\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m C.EPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # 3) Run your pipeline\n",
    "# import os\n",
    "# for im in os.listdir(\"/Users/adamsobieszek/PycharmProjects/_manipy/age_oems_img\"):\n",
    "#     if im.endswith(\".pt\") or im.startswith(\".\"):\n",
    "#         continue\n",
    "\n",
    "    \n",
    "#     print(im)\n",
    "#     vf = visionframe_from_pil(Image.open(f\"/Users/adamsobieszek/PycharmProjects/_manipy/age_oems_img/{im}\"))\n",
    "#     reject, score, subs, reasons = should_reject(vf)\n",
    "#     if reject:\n",
    "#         os.remove(f\"/Users/adamsobieszek/PycharmProjects/_manipy/age_oems_img/{im}\")\n",
    "#     print(reject, score, reasons)\n",
    "#     print(subs)\n",
    "# 3) Run your pipeline\n",
    "from quality_metrics import _crop_from_bbox, _scale01, _has_68, _geom_symmetry_score, _estimate_pose_from_68, _clamp01, _laplacian_var, _centering_score, _occlusion_score\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "rej = 0\n",
    "to_remove = []\n",
    "all_reasons = []\n",
    "os.makedirs(\"/Users/adamsobieszek/PycharmProjects/_manipy/selected_age_additional3/rejected_face_metrics\", exist_ok=True)\n",
    "for im in os.listdir(\"/Users/adamsobieszek/PycharmProjects/_manipy/selected_age_additional3\"):\n",
    "    if im.endswith(\".pt\") or im.startswith(\".\") or not 'jpg' in im:\n",
    "        continue\n",
    "\n",
    "\n",
    "    vf = visionframe_from_pil(Image.open(f\"/Users/adamsobieszek/PycharmProjects/_manipy/selected_age_additional3/{im}\").resize((256,256)))\n",
    "    reject, score, subs, reasons = should_reject(vf)\n",
    "\n",
    "    # metrics = [\n",
    "    #     # new edge-partial metrics\n",
    "    #     # m_edge_partial,\n",
    "    #     make_edge_fire(\"left\"), make_edge_fire(\"right\"),\n",
    "    #     # make_edge_fire(\"top\"),  make_edge_fire(\"bottom\"),\n",
    "    #     # m_edge_color_partial, m_edge_color_max_sim \n",
    "    #     # (optional debug)\n",
    "    #     # make_edge_score(\"left\"), make_edge_skin_frac(\"left\"), ...\n",
    "    # ]\n",
    "    # values = evaluate_metrics(vf, metrics)\n",
    "    # reject = (values.get(\"edge_left_fire\", 0.0) >= 0.5) or (values.get(\"edge_right_fire\", 0.0) >= 0.5)\n",
    "    if reject:\n",
    "        if rej%100==0:\n",
    "            print(rej)\n",
    "        rej +=1\n",
    "        to_remove.append(im)\n",
    "        # all_reasons.append(reasons)\n",
    "        shutil.move(f\"/Users/adamsobieszek/PycharmProjects/_manipy/selected_age_additional3/{im}\", f\"/Users/adamsobieszek/PycharmProjects/_manipy/selected_age_additional3/rejected_face_metrics/{im}\")\n",
    "        # if reasons == [\"landmark_geometry_implausible\"]:\n",
    "print(reject, score, reasons)\n",
    "print(subs)\n",
    "print(all_reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592260f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "527e691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adamsobieszek/PycharmProjects/_manipy/deixis\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/adamsobieszek/PycharmProjects/_manipy/deixis\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from metrics.metric_utils import *\n",
    "\n",
    "# ---------- PR helpers (from your snippet) ----------\n",
    "@torch.no_grad()\n",
    "def compute_distances(row_features, col_features, num_gpus, rank, col_batch_size):\n",
    "    assert 0 <= rank < num_gpus\n",
    "    num_cols = col_features.shape[0]\n",
    "    num_batches = ((num_cols - 1) // col_batch_size // num_gpus + 1) * num_gpus\n",
    "    col_batches = torch.nn.functional.pad(col_features, [0, 0, 0, -num_cols % num_batches]).chunk(num_batches)\n",
    "    dist_batches = []\n",
    "    for col_batch in col_batches[rank :: num_gpus]:\n",
    "        dist_batch = torch.cdist(row_features.unsqueeze(0), col_batch.unsqueeze(0))[0]\n",
    "        for src in range(num_gpus):\n",
    "            dist_broadcast = dist_batch.clone()\n",
    "            if num_gpus > 1:\n",
    "                torch.distributed.broadcast(dist_broadcast, src=src)\n",
    "            dist_batches.append(dist_broadcast.cpu() if rank == 0 else None)\n",
    "    return torch.cat(dist_batches, dim=1)[:, :num_cols] if rank == 0 else None\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def pr_per_sample_from_dir_and_tensors(\n",
    "    real_dir: str,\n",
    "    gen_imgs: torch.Tensor,\n",
    "    *,\n",
    "    nhood_size: int = 3,\n",
    "    row_batch_size: int = 512,\n",
    "    col_batch_size: int = 1024,\n",
    "    device: str = \"cuda\",\n",
    "    dataset_xflip: bool = False,\n",
    "    real_features=None\n",
    "):\n",
    "    from torch.nn import functional as F\n",
    "\n",
    "    # --- normalize gen tensors to [-1,1]\n",
    "    assert gen_imgs.ndim == 4 and gen_imgs.shape[1] == 3, \"gen_imgs must be [N,3,H,W]\"\n",
    "    x = gen_imgs.to(torch.float32)\n",
    "    if x.min().item() >= -1.01 and x.max().item() <= 1.01:\n",
    "        pass\n",
    "    elif x.min().item() >= 0.0 and x.max().item() <= 1.0:\n",
    "        x = x * 2 - 1\n",
    "    elif x.min().item() >= 0.0 and x.max().item() <= 255.0:\n",
    "        x = x / 127.5 - 1.0\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected gen tensor range\")\n",
    "    gen_imgs = x.to(device, dtype=torch.float32, non_blocking=True)\n",
    "\n",
    "    \n",
    "    progress = metric_utils.ProgressMonitor(verbose=True)\n",
    "\n",
    "    # opts = metric_utils.MetricOptions()    # Validate arguments.\n",
    "    opts = metric_utils.MetricOptions(\n",
    "        device=torch.device(device),\n",
    "        num_gpus=1,\n",
    "        rank=0,\n",
    "        cache=None,\n",
    "        progress=progress,\n",
    "        dataset_kwargs=dict(\n",
    "            class_name=\"training.dataset.ImageFolderDataset\",  # <-- THIS was missing\n",
    "            path=real_dir,\n",
    "            use_labels=False,\n",
    "            xflip=dataset_xflip,\n",
    "            max_size=None,\n",
    "            # resolution not required; the VGG16 detector resizes internally\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    detector_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/vgg16.pkl\"\n",
    "    detector_kwargs = dict(return_features=True)\n",
    "\n",
    "    # --- REAL features (pass batch_size explicitly)\n",
    "    real_feats = (real_features if real_features is not None else compute_feature_stats_for_dataset(\n",
    "        opts=opts, detector_url=detector_url, detector_kwargs=detector_kwargs,\n",
    "        rel_lo=0, rel_hi=0, capture_all=True, max_items=None, batch_size=row_batch_size,\n",
    "        data_loader_kwargs=dict(num_workers=0),   # macOS/MPS often safer with 0 workers\n",
    "    ).get_all_torch()).to(torch.float32).to(opts.device)\n",
    "\n",
    "    # --- GEN features via the same detector\n",
    "    try:\n",
    "        detector = metric_utils.get_feature_detector(detector_url, device=opts.device)\n",
    "        detector.eval().requires_grad_(False)\n",
    "        def run_det(b): \n",
    "            # print input kwargs\n",
    "            out = detector(b, **detector_kwargs)\n",
    "            return out if isinstance(out, torch.Tensor) else out[0]\n",
    "    except Exception:\n",
    "        def run_det(b):\n",
    "            return metric_utils.compute_feature_detector_output(\n",
    "                b, detector_url, detector_kwargs, device=opts.device\n",
    "            )\n",
    "\n",
    "    gen_feats_list = []\n",
    "    for s in range(0, gen_imgs.shape[0], row_batch_size):\n",
    "        feats = run_det(gen_imgs[s:s+row_batch_size].to(dtype=torch.float32))\n",
    "        gen_feats_list.append(feats.to(torch.float32))\n",
    "    gen_feats = torch.cat(gen_feats_list, dim=0).to(opts.device)\n",
    "\n",
    "    # --- distance helpers (same as before)\n",
    "    @torch.no_grad()\n",
    "    def compute_distances(row_features, col_features, num_gpus, rank, col_batch_size):\n",
    "        assert 0 <= rank < num_gpus\n",
    "        num_cols = col_features.shape[0]\n",
    "        num_batches = ((num_cols - 1) // col_batch_size // num_gpus + 1) * num_gpus\n",
    "        col_batches = torch.nn.functional.pad(col_features, [0, 0, 0, -num_cols % num_batches]).chunk(num_batches)\n",
    "        dist_batches = []\n",
    "        for col_batch in col_batches[rank :: num_gpus]:\n",
    "            dist_batch = torch.cdist(row_features.unsqueeze(0), col_batch.unsqueeze(0))[0]\n",
    "            for src in range(num_gpus):\n",
    "                dist_broadcast = dist_batch.clone()\n",
    "                if num_gpus > 1:\n",
    "                    torch.distributed.broadcast(dist_broadcast, src=src)\n",
    "                dist_batches.append(dist_broadcast.cpu() if rank == 0 else None)\n",
    "        return torch.cat(dist_batches, dim=1)[:, :num_cols] if rank == 0 else None\n",
    "\n",
    "    def kth_radii(manifold):\n",
    "        out = []\n",
    "        for mb in manifold.split(row_batch_size):\n",
    "            dist = compute_distances(mb, manifold, num_gpus=opts.num_gpus, rank=opts.rank, col_batch_size=col_batch_size)\n",
    "            out.append(dist.to(torch.float32).kthvalue(nhood_size + 1).values.to(torch.float32))\n",
    "        return torch.cat(out, dim=0)\n",
    "\n",
    "    kth_real = kth_radii(real_feats)\n",
    "    kth_gen  = kth_radii(gen_feats)\n",
    "\n",
    "    # precision flags for generated samples (probes = gen, manifold = real)\n",
    "    prec_flags = []\n",
    "    for pb in gen_feats.split(row_batch_size):\n",
    "        dist = compute_distances(pb, real_feats, num_gpus=opts.num_gpus, rank=opts.rank, col_batch_size=col_batch_size)\n",
    "        prec_flags.append((dist <= kth_real).any(dim=1))\n",
    "    prec_flags = torch.cat(prec_flags, dim=0)\n",
    "\n",
    "    # recall flags for real samples (probes = real, manifold = gen)\n",
    "    rec_flags = []\n",
    "    for pb in real_feats.split(row_batch_size):\n",
    "        dist = compute_distances(pb, gen_feats, num_gpus=opts.num_gpus, rank=opts.rank, col_batch_size=col_batch_size)\n",
    "        rec_flags.append((dist <= kth_gen).any(dim=1))\n",
    "    rec_flags = torch.cat(rec_flags, dim=0)\n",
    "\n",
    "    P = float(prec_flags.float().mean().item())\n",
    "    R = float(rec_flags.float().mean().item())\n",
    "    return prec_flags, rec_flags, P, R\n",
    "\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "@torch.no_grad()\n",
    "def realness_precision_score_from_dir_and_tensors(\n",
    "    real_dir: str ='',\n",
    "    fake_dir: str ='',                 # [N,3,H,W] in [-1,1] or [0,1] or uint8\n",
    "    *,\n",
    "    gen_imgs: torch.Tensor | None = None,\n",
    "    nhood_size: int = 3,                    # k for real-manifold radii\n",
    "    row_batch_size: int = 128,\n",
    "    col_batch_size: int = 256,\n",
    "    device: str = \"cuda\",\n",
    "    dataset_xflip: bool = False,\n",
    "    real_features: torch.Tensor | None = None,  # optional cache: [N_real, C] fp32\n",
    "    fake_features: torch.Tensor | None = None,  # optional cache: [N_fake, C] fp32\n",
    "    alpha: float = 2.0,                     # softness of the score mapping\n",
    "    return_extra: bool = True,              # include nn idx/dist/radius\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Continuous precision-only 'realness' score for few fake images.\n",
    "    Scores in [0,1], higher is more real-like.\n",
    "\n",
    "    Returns:\n",
    "        scores: FloatTensor [N_gen] in [0,1]\n",
    "        extras (optional): dict with keys\n",
    "            'min_ratio' [N_gen]        (dimensionless)\n",
    "            'nn_index'  [N_gen] (long) (index into real set)\n",
    "            'nn_dist'   [N_gen]        (distance to best real)\n",
    "            'nn_radius' [N_gen]        (k-th radius of that real)\n",
    "            'k'         int\n",
    "    \"\"\"\n",
    "    # --- normalize generated images to [-1,1], fp32 on device\n",
    "    if gen_imgs is not None:\n",
    "        assert gen_imgs.ndim == 4 and gen_imgs.shape[1] == 3, \"gen_imgs must be [N,3,H,W]\"\n",
    "        x = gen_imgs.to(torch.float32)\n",
    "        xmin, xmax = float(x.min()), float(x.max())\n",
    "        if xmin >= -1.01 and xmax <= 1.01:\n",
    "            pass\n",
    "        elif xmin >= 0.0 and xmax <= 1.0:\n",
    "            x = x * 2 - 1\n",
    "        elif xmin >= 0.0 and xmax <= 255.0:\n",
    "            x = x / 127.5 - 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected gen tensor range [{xmin:.3f},{xmax:.3f}]\")\n",
    "        gen_imgs = x.to(device, dtype=torch.float32, non_blocking=True)\n",
    "\n",
    "    # --- metric utils wiring (matches your new calling style)\n",
    "    \n",
    "    progress = metric_utils.ProgressMonitor(verbose=verbose)\n",
    "    opts = metric_utils.MetricOptions(\n",
    "        device=torch.device(device),\n",
    "        num_gpus=1,\n",
    "        rank=0,\n",
    "        cache=None,\n",
    "        progress=progress,\n",
    "        dataset_kwargs=dict(\n",
    "            class_name=\"training.dataset.ImageFolderDataset\",\n",
    "            path=real_dir,\n",
    "            use_labels=False,\n",
    "            xflip=dataset_xflip,\n",
    "            max_size=None,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    detector_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/vgg16.pkl\"\n",
    "    detector_kwargs = dict(return_features=True)\n",
    "\n",
    "    # --- REAL features (compute or reuse)\n",
    "    if real_features is None:\n",
    "        real_feats = metric_utils.compute_feature_stats_for_dataset(\n",
    "            opts=opts, detector_url=detector_url, detector_kwargs=detector_kwargs,\n",
    "            rel_lo=0, rel_hi=0, capture_all=True, max_items=None,\n",
    "            batch_size=row_batch_size, data_loader_kwargs=dict(num_workers=0),\n",
    "        ).get_all_torch().to(torch.float32).to(opts.device)\n",
    "    else:\n",
    "        real_feats = real_features.to(torch.float32).to(opts.device)\n",
    "\n",
    "    if fake_features is None:\n",
    "        # --- GEN features via the same detector (fp32)\n",
    "        if gen_imgs is not None:\n",
    "            try:\n",
    "                detector = metric_utils.get_feature_detector(detector_url, device=opts.device)\n",
    "                detector.eval().requires_grad_(False)\n",
    "                def run_det(b):\n",
    "                    out = detector(b, **detector_kwargs)\n",
    "                    return out if isinstance(out, torch.Tensor) else out[0]\n",
    "            except Exception:\n",
    "                def run_det(b):\n",
    "                    return metric_utils.compute_feature_detector_output(\n",
    "                        b, detector_url, detector_kwargs, device=opts.device\n",
    "                    )\n",
    "            \n",
    "            gen_feats_list = []\n",
    "            for s in range(0, gen_imgs.shape[0], row_batch_size):\n",
    "                feats = run_det(gen_imgs[s:s+row_batch_size])          # detector prefers fp32\n",
    "                gen_feats_list.append(feats.to(torch.float32))\n",
    "            gen_feats = torch.cat(gen_feats_list, dim=0).to(opts.device)  # [N_gen, C]\n",
    "\n",
    "        elif fake_dir is not None:\n",
    "                \n",
    "            opts_fake = metric_utils.MetricOptions(\n",
    "                device=torch.device(\"cpu\"),\n",
    "                num_gpus=1,\n",
    "                rank=0,\n",
    "                cache=None,\n",
    "                progress=progress,\n",
    "                dataset_kwargs=dict(\n",
    "                    class_name=\"training.dataset.ImageFolderDataset\",\n",
    "                    path=fake_dir,\n",
    "                    use_labels=False,\n",
    "                    xflip=dataset_xflip,\n",
    "                    max_size=None,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            gen_feats = metric_utils.compute_feature_stats_for_dataset(\n",
    "                opts=opts_fake, detector_url=detector_url, detector_kwargs=detector_kwargs,\n",
    "                rel_lo=0, rel_hi=0, capture_all=True, max_items=None,\n",
    "                batch_size=row_batch_size, data_loader_kwargs=dict(num_workers=0),\n",
    "            ).get_all_torch().to(torch.float32).to(opts.device)\n",
    "    else:\n",
    "        gen_feats = fake_features.to(torch.float32).to(opts.device)\n",
    "\n",
    "\n",
    "\n",
    "    # --- distance helper (fp32)\n",
    "    def _batched_cdist(rows: torch.Tensor, cols: torch.Tensor) -> torch.Tensor:\n",
    "        num_cols = cols.shape[0]\n",
    "        num_batches = ((num_cols - 1) // col_batch_size // 1 + 1) * 1\n",
    "        col_batches = torch.nn.functional.pad(cols, [0, 0, 0, -num_cols % num_batches]).chunk(num_batches)\n",
    "        parts = []\n",
    "        for cb in col_batches:\n",
    "            parts.append(torch.cdist(rows.unsqueeze(0), cb.unsqueeze(0), p=2.0)[0])\n",
    "        return torch.cat(parts, dim=1)[:, :num_cols]  # [rows, cols]\n",
    "\n",
    "    # --- compute k-th radii for real manifold\n",
    "    N_real = real_feats.shape[0]\n",
    "    if nhood_size >= N_real:\n",
    "        k = max(1, N_real - 1)\n",
    "    else:\n",
    "        k = nhood_size\n",
    "\n",
    "    kth_real = []\n",
    "    for mb in real_feats.split(row_batch_size):\n",
    "        dist = _batched_cdist(mb, real_feats)                         # [B, N_real]\n",
    "        # Use torch.topk to get the (k+1)th smallest value along dim=1 (since topk returns largest, so use negative and reverse)\n",
    "        # +1 skips self 0\n",
    "        kth_vals, _ = torch.topk(dist, k + 1, dim=1, largest=False, sorted=True)\n",
    "        kth_real.append(kth_vals[:, -1])\n",
    "    kth_real = torch.cat(kth_real, dim=0)                              # [N_real], fp32\n",
    "\n",
    "    # --- for each gen probe, compute normalized distance ratios to all real anchors\n",
    "    #     ratio_ij = d(p_i, r_j) / rho_j, then take min over j\n",
    "    scores = []\n",
    "    min_ratios = []\n",
    "    nn_idx_list, nn_dist_list, nn_rad_list = [], [], []\n",
    "\n",
    "    # Pre-broadcast real radii for vectorized compare\n",
    "    rho = kth_real  # [N_real]\n",
    "    eps = 1e-8\n",
    "\n",
    "    for pb in gen_feats.split(row_batch_size):\n",
    "        D = _batched_cdist(pb, real_feats)                 # [B, N_real]\n",
    "        R = rho.unsqueeze(0).expand_as(D) + eps            # [B, N_real]\n",
    "        ratios = D / R                                     # [B, N_real]\n",
    "        min_vals, min_idx = ratios.min(dim=1)              # [B], [B]\n",
    "        # map to (0,1]: score = 1 / (1 + min_ratio^alpha)\n",
    "        s = 1.0 / (1.0 + torch.clamp(min_vals, min=0.0) ** alpha)\n",
    "        scores.append(s)\n",
    "\n",
    "        # extras\n",
    "        min_ratios.append(min_vals)\n",
    "        nn_idx_list.append(min_idx)\n",
    "        # pick corresponding absolute dist & radius\n",
    "        idx2 = min_idx.unsqueeze(1)\n",
    "        nn_dist_list.append(D.gather(1, idx2).squeeze(1))\n",
    "        nn_rad_list.append(R.gather(1, idx2).squeeze(1))\n",
    "\n",
    "    scores     = torch.cat(scores, dim=0)          # [N_gen]\n",
    "    min_ratios = torch.cat(min_ratios, dim=0)      # [N_gen]\n",
    "    nn_index   = torch.cat(nn_idx_list, dim=0).long()\n",
    "    nn_dist    = torch.cat(nn_dist_list, dim=0)\n",
    "    nn_radius  = torch.cat(nn_rad_list, dim=0)\n",
    "\n",
    "    if return_extra:\n",
    "        extras = dict(\n",
    "            min_ratio=min_ratios,\n",
    "            nn_index=nn_index,\n",
    "            nn_dist=nn_dist,\n",
    "            nn_radius=nn_radius,\n",
    "            k=k,\n",
    "        )\n",
    "        return scores, extras\n",
    "    return scores\n",
    "\n",
    "\n",
    "def compute_feature_stats_for_dataset(opts, detector_url, detector_kwargs, rel_lo=0, rel_hi=1, batch_size=16, data_loader_kwargs=None, max_items=None, detector=None, **stats_kwargs):\n",
    "    dataset = dnnlib.util.construct_class_by_name(**opts.dataset_kwargs)\n",
    "    if data_loader_kwargs is None:\n",
    "        data_loader_kwargs = dict(pin_memory=False, num_workers=0, prefetch_factor=None)\n",
    "\n",
    "    # Try to lookup from cache.\n",
    "    cache_file = None\n",
    "    if opts.cache:\n",
    "        # Choose cache file name.\n",
    "        args = dict(dataset_kwargs=opts.dataset_kwargs, detector_url=detector_url, detector_kwargs=detector_kwargs, stats_kwargs=stats_kwargs)\n",
    "        md5 = hashlib.md5(repr(sorted(args.items())).encode('utf-8'))\n",
    "        cache_tag = f'{dataset.name}-{get_feature_detector_name(detector_url)}-{md5.hexdigest()}'\n",
    "        cache_file = dnnlib.make_cache_dir_path('gan-metrics', cache_tag + '.pkl')\n",
    "\n",
    "        # Check if the file exists (all processes must agree).\n",
    "        flag = os.path.isfile(cache_file) if opts.rank == 0 else False\n",
    "        if opts.num_gpus > 1:\n",
    "            flag = torch.as_tensor(flag, dtype=torch.float32, device=opts.device)\n",
    "            torch.distributed.broadcast(tensor=flag, src=0)\n",
    "            flag = (float(flag.cpu()) != 0)\n",
    "\n",
    "        # Load.\n",
    "        if flag:\n",
    "            return FeatureStats.load(cache_file)\n",
    "\n",
    "    # Initialize.\n",
    "    num_items = len(dataset)\n",
    "    if max_items is not None:\n",
    "        num_items = min(num_items, max_items)\n",
    "    stats = FeatureStats(max_items=num_items, **stats_kwargs)\n",
    "    progress = opts.progress.sub(tag='dataset features', num_items=num_items, rel_lo=rel_lo, rel_hi=rel_hi)\n",
    "    \n",
    "    detector = detector if detector is not None else build_feature_extractor(\"vgg16\", device=opts.device)\n",
    "    # detector = #get_feature_detector(url=detector_url, device=opts.device, num_gpus=opts.num_gpus, rank=opts.rank, verbose=progress.verbose)\n",
    "\n",
    "    # Main loop.\n",
    "    item_subset = [(i * opts.num_gpus + opts.rank) % num_items for i in range((num_items - 1) // opts.num_gpus + 1)]\n",
    "    for images, _labels in torch.utils.data.DataLoader(dataset=dataset, sampler=item_subset, batch_size=batch_size, **data_loader_kwargs):\n",
    "        if images.shape[1] == 1:\n",
    "            images = images.repeat([1, 3, 1, 1])\n",
    "        import torchvision.transforms as T\n",
    "        images = images[:,:,200:, 100:images.size(3)-100]\n",
    "        images = T.Resize(224)(images)\n",
    "        if not 'features' in locals(): \n",
    "            plt.imshow(images[0].cpu().numpy().transpose(1,2,0))\n",
    "            plt.show()\n",
    "        features = detector(images.to(opts.device)/255)#, **detector_kwargs)\n",
    "        stats.append_torch(features, num_gpus=opts.num_gpus, rank=opts.rank)\n",
    "        progress.update(stats.num_items)\n",
    "\n",
    "    # Save to cache.\n",
    "    if cache_file is not None and opts.rank == 0:\n",
    "        os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
    "        temp_file = cache_file + '.' + uuid.uuid4().hex\n",
    "        stats.save(temp_file)\n",
    "        os.replace(temp_file, cache_file) # atomic\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6641638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "device = torch.device('mps')\n",
    "\n",
    "sys.path.append('/Users/adamsobieszek/PycharmProjects/psychGAN/content/psychGAN/stylegan3')\n",
    "with open('/Users/adamsobieszek/PycharmProjects/psychGAN/stylegan2-ffhq-1024x1024.pkl', 'rb') as fp:\n",
    "    G = pickle.load(fp)['G_ema'].to(device)\n",
    "with open('/Users/adamsobieszek/PycharmProjects/psychGAN/stylegan2-ffhq-1024x1024.pkl', 'rb') as fp:\n",
    "\n",
    "    D = pickle.load(fp)['D'].to(device)\n",
    "\n",
    "# Compute the average latent vector\n",
    "all_z = torch.randn([1, G.mapping.z_dim], device=device)\n",
    "face_w = G.mapping(all_z, None, truncation_psi=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3fd784d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# example_vgg_mps.py\n",
    "# Import from your file/package (adjust the import to match how you install it)\n",
    "# If it's a local file named sg_output_analysis.py in the same folder:\n",
    "\n",
    "import torch\n",
    "from face_utils.sg_opt.sg_output_analysis import (\n",
    "    build_feature_extractor,\n",
    "    CoordsToFeaturesConfig,\n",
    "    coords_to_features,\n",
    "    feature_sensitivity_at,\n",
    ")\n",
    "\n",
    "# 1) Pick the best available device, preferring Apple MPS\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# # 2) Build the VGG16-based feature extractor on that device\n",
    "# extractor = build_feature_extractor(\"vgg16\", device=device)\n",
    "# print(\"VGG16 feature extractor is ready.\")\n",
    "\n",
    "# # 3) Create a dummy RGB image (1,3,H,W) in [0,1] on the same device\n",
    "# H, W = 480, 640\n",
    "# image = torch.rand(1, 3, H, W, device=device)\n",
    "\n",
    "# # 4) Define a patch centered at the middle of the image and extract features\n",
    "# center_xy = torch.tensor([W / 2.0, H / 2.0], device=device)  # pixels (x, y)\n",
    "# cfg = CoordsToFeaturesConfig(\n",
    "#     extractor_name=\"vgg16\",\n",
    "#     out_res=224,\n",
    "#     patch_px=128,\n",
    "#     coords_mode=\"pixels\",\n",
    "#     clamp=True,\n",
    "# )\n",
    "\n",
    "# feats, patch = coords_to_features(image, center_xy, cfg, extractor)\n",
    "# print(f\"Feature shape: {feats.shape}  |  Patch shape: {patch.shape}\")\n",
    "\n",
    "# # 5) (Optional) Compute gradient/Hessian sensitivity at that coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acaeeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adamsobieszek/PycharmProjects/_manipy/deixis\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_feature_extractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# real_features = compute_feats(opts, None, real_features=None, batch_size=128)\u001b[39;00m\n\u001b[32m     60\u001b[39m opts.dataset_kwargs[\u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m] = fake_dir\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m fake_features = \u001b[43mcompute_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_features\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m scores, extras = realness_precision_score_from_dir_and_tensors(\n\u001b[32m     63\u001b[39m     real_features=real_features,\n\u001b[32m     64\u001b[39m     fake_features=fake_features,\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m     device=\u001b[33m'\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m'\u001b[39m,  \n\u001b[32m     69\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mScores:\u001b[39m\u001b[33m\"\u001b[39m, scores.tolist())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mcompute_feats\u001b[39m\u001b[34m(opts, max_real, batch_size, real_features, extractor)\u001b[39m\n\u001b[32m     28\u001b[39m detector_url = \u001b[33m'\u001b[39m\u001b[33mhttps://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/vgg16.pkl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     29\u001b[39m detector_kwargs = \u001b[38;5;28mdict\u001b[39m(return_features=\u001b[38;5;28;01mTrue\u001b[39;00m,resize_images=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m detector = \u001b[43mbuild_feature_extractor\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mvgg16\u001b[39m\u001b[33m\"\u001b[39m, device=opts.device)\n\u001b[32m     32\u001b[39m real_features = (real_features \u001b[38;5;28;01mif\u001b[39;00m real_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m compute_feature_stats_for_dataset(\n\u001b[32m     33\u001b[39m     opts=opts, detector_url=detector_url, detector_kwargs=detector_kwargs, batch_size=batch_size,\n\u001b[32m     34\u001b[39m     rel_lo=\u001b[32m0\u001b[39m, rel_hi=\u001b[32m0\u001b[39m, capture_all=\u001b[38;5;28;01mTrue\u001b[39;00m, max_items=max_real, detector=detector).get_all_torch()).to(torch.float32).to(opts.device)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m real_features\n",
      "\u001b[31mNameError\u001b[39m: name 'build_feature_extractor' is not defined"
     ]
    }
   ],
   "source": [
    "%cd /Users/adamsobieszek/PycharmProjects/_manipy/deixis\n",
    "import dnnlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from metrics import metric_utils\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_distances(row_features, col_features, num_gpus, rank, col_batch_size):\n",
    "    assert 0 <= rank < num_gpus\n",
    "    num_cols = col_features.shape[0]\n",
    "    num_batches = ((num_cols - 1) // col_batch_size // num_gpus + 1) * num_gpus\n",
    "    col_batches = torch.nn.functional.pad(col_features, [0, 0, 0, -num_cols % num_batches]).chunk(num_batches)\n",
    "    dist_batches = []\n",
    "    for col_batch in col_batches[rank :: num_gpus]:\n",
    "        dist_batch = torch.cdist(row_features.unsqueeze(0), col_batch.unsqueeze(0))[0]\n",
    "        for src in range(num_gpus):\n",
    "            dist_broadcast = dist_batch.clone()\n",
    "            if num_gpus > 1:\n",
    "                torch.distributed.broadcast(dist_broadcast, src=src)\n",
    "            dist_batches.append(dist_broadcast.cpu() if rank == 0 else None)\n",
    "    return torch.cat(dist_batches, dim=1)[:, :num_cols] if rank == 0 else None\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def compute_feats(opts, max_real, batch_size=32, real_features=None, extractor=None):\n",
    "    detector_url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/vgg16.pkl'\n",
    "    detector_kwargs = dict(return_features=True,resize_images=True)\n",
    "\n",
    "    detector = build_feature_extractor(\"vgg16\", device=opts.device)\n",
    "    real_features = (real_features if real_features is not None else compute_feature_stats_for_dataset(\n",
    "        opts=opts, detector_url=detector_url, detector_kwargs=detector_kwargs, batch_size=batch_size,\n",
    "        rel_lo=0, rel_hi=0, capture_all=True, max_items=max_real, detector=detector).get_all_torch()).to(torch.float32).to(opts.device)\n",
    "    return real_features\n",
    "\n",
    "\n",
    "\n",
    "real_dir=\"/Users/adamsobieszek/Downloads/0.75\"\n",
    "fake_dir=\"/Users/adamsobieszek/PycharmProjects/_manipy/selected_age_redo\"\n",
    "progress = metric_utils.ProgressMonitor(verbose=True)\n",
    "\n",
    "# opts = metric_utils.MetricOptions()    # Validate arguments.\n",
    "opts = metric_utils.MetricOptions(\n",
    "    device=torch.device('mps'),\n",
    "    num_gpus=1,\n",
    "    rank=0,\n",
    "    cache=None,\n",
    "    progress=progress,\n",
    "    dataset_kwargs=dict(\n",
    "        class_name=\"training.dataset.ImageFolderDataset\",  \n",
    "        path=real_dir,\n",
    "        use_labels=False,\n",
    "        xflip=False,\n",
    "        max_size=None,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# real_features = compute_feats(opts, None, real_features=None, batch_size=128)\n",
    "opts.dataset_kwargs['path'] = fake_dir\n",
    "fake_features = compute_feats(opts, None, real_features=None, batch_size=128)\n",
    "scores, extras = realness_precision_score_from_dir_and_tensors(\n",
    "    real_features=real_features,\n",
    "    fake_features=fake_features,\n",
    "    nhood_size=3,\n",
    "    row_batch_size=4,\n",
    "    col_batch_size=250,\n",
    "    device='mps',  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Scores:\", scores.tolist())\n",
    "print(\"Nearest real idx:\", extras[\"nn_index\"].tolist())\n",
    "print(\"Min ratios:\", extras[\"min_ratio\"].tolist())      # <1 means inside some radius\n",
    "print(\"Nearest dist:\", extras[\"nn_dist\"].tolist())\n",
    "print(\"Nearest radius:\", extras[\"nn_radius\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd178ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3bc516c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_f_10.jpg',\n",
       " '0_f_12.jpg',\n",
       " '0_f_14.jpg',\n",
       " '0_f_16.jpg',\n",
       " '0_f_18.jpg',\n",
       " '0_f_20.jpg',\n",
       " '0_f_22.jpg',\n",
       " '0_f_24.jpg',\n",
       " '0_f_26.jpg',\n",
       " '0_f_28.jpg',\n",
       " '0_f_30.jpg',\n",
       " '0_f_32.jpg',\n",
       " '0_f_34.jpg',\n",
       " '0_f_40.jpg',\n",
       " '0_f_42.jpg',\n",
       " '0_f_44.jpg',\n",
       " '0_f_46.jpg',\n",
       " '0_f_48.jpg',\n",
       " '0_f_50.jpg',\n",
       " '0_f_52.jpg',\n",
       " '0_f_54.jpg',\n",
       " '0_f_56.jpg',\n",
       " '0_f_8.jpg',\n",
       " '0_m_10.jpg',\n",
       " '0_m_12.jpg',\n",
       " '0_m_14.jpg',\n",
       " '0_m_16.jpg',\n",
       " '0_m_18.jpg',\n",
       " '0_m_20.jpg',\n",
       " '0_m_22.jpg',\n",
       " '0_m_24.jpg',\n",
       " '0_m_26.jpg',\n",
       " '0_m_28.jpg',\n",
       " '0_m_30.jpg',\n",
       " '0_m_32.jpg',\n",
       " '0_m_34.jpg',\n",
       " '0_m_36.jpg',\n",
       " '0_m_38.jpg',\n",
       " '0_m_40.jpg',\n",
       " '0_m_42.jpg',\n",
       " '0_m_44.jpg',\n",
       " '0_m_46.jpg',\n",
       " '0_m_48.jpg',\n",
       " '0_m_50.jpg',\n",
       " '0_m_52.jpg',\n",
       " '0_m_54.jpg',\n",
       " '0_m_56.jpg',\n",
       " '0_m_8.jpg',\n",
       " '10_f_10.jpg',\n",
       " '10_f_12.jpg',\n",
       " '10_f_14.jpg',\n",
       " '10_f_16.jpg',\n",
       " '10_f_18.jpg',\n",
       " '10_f_20.jpg',\n",
       " '10_f_22.jpg',\n",
       " '10_f_24.jpg',\n",
       " '10_f_30.jpg',\n",
       " '10_f_32.jpg',\n",
       " '10_f_34.jpg',\n",
       " '10_f_36.jpg',\n",
       " '10_f_40.jpg',\n",
       " '10_f_42.jpg',\n",
       " '10_f_44.jpg',\n",
       " '10_f_46.jpg',\n",
       " '10_f_48.jpg',\n",
       " '10_f_50.jpg',\n",
       " '10_f_52.jpg',\n",
       " '10_f_54.jpg',\n",
       " '10_f_56.jpg',\n",
       " '10_f_8.jpg',\n",
       " '10_m_52.jpg',\n",
       " '10_m_56.jpg',\n",
       " '11_f_10.jpg',\n",
       " '11_f_12.jpg',\n",
       " '11_f_14.jpg',\n",
       " '11_f_16.jpg',\n",
       " '11_f_18.jpg',\n",
       " '11_f_20.jpg',\n",
       " '11_f_22.jpg',\n",
       " '11_f_24.jpg',\n",
       " '11_f_30.jpg',\n",
       " '11_f_32.jpg',\n",
       " '11_f_34.jpg',\n",
       " '11_f_36.jpg',\n",
       " '11_f_40.jpg',\n",
       " '11_f_42.jpg',\n",
       " '11_f_44.jpg',\n",
       " '11_f_46.jpg',\n",
       " '11_f_48.jpg',\n",
       " '11_f_50.jpg',\n",
       " '11_f_52.jpg',\n",
       " '11_f_54.jpg',\n",
       " '11_f_56.jpg',\n",
       " '11_f_8.jpg',\n",
       " '11_m_52.jpg',\n",
       " '12_f_10.jpg',\n",
       " '12_f_12.jpg',\n",
       " '12_f_14.jpg',\n",
       " '12_f_16.jpg',\n",
       " '12_f_18.jpg',\n",
       " '12_f_20.jpg',\n",
       " '12_f_22.jpg',\n",
       " '12_f_24.jpg',\n",
       " '12_f_30.jpg',\n",
       " '12_f_32.jpg',\n",
       " '12_f_34.jpg',\n",
       " '12_f_36.jpg',\n",
       " '12_f_40.jpg',\n",
       " '12_f_42.jpg',\n",
       " '12_f_44.jpg',\n",
       " '12_f_46.jpg',\n",
       " '12_f_48.jpg',\n",
       " '12_f_50.jpg',\n",
       " '12_f_52.jpg',\n",
       " '12_f_54.jpg',\n",
       " '12_f_56.jpg',\n",
       " '12_m_52.jpg',\n",
       " '13_f_10.jpg',\n",
       " '13_f_12.jpg',\n",
       " '13_f_14.jpg',\n",
       " '13_f_16.jpg',\n",
       " '13_f_18.jpg',\n",
       " '13_f_20.jpg',\n",
       " '13_f_22.jpg',\n",
       " '13_f_24.jpg',\n",
       " '13_f_30.jpg',\n",
       " '13_f_32.jpg',\n",
       " '13_f_34.jpg',\n",
       " '13_f_42.jpg',\n",
       " '13_f_46.jpg',\n",
       " '13_f_48.jpg',\n",
       " '13_f_50.jpg',\n",
       " '13_f_52.jpg',\n",
       " '13_f_54.jpg',\n",
       " '13_f_56.jpg',\n",
       " '13_m_52.jpg',\n",
       " '14_f_10.jpg',\n",
       " '14_f_12.jpg',\n",
       " '14_f_14.jpg',\n",
       " '14_f_16.jpg',\n",
       " '14_f_18.jpg',\n",
       " '14_f_20.jpg',\n",
       " '14_f_22.jpg',\n",
       " '14_f_32.jpg',\n",
       " '14_f_36.jpg',\n",
       " '14_f_40.jpg',\n",
       " '14_f_42.jpg',\n",
       " '14_f_46.jpg',\n",
       " '14_f_48.jpg',\n",
       " '14_f_50.jpg',\n",
       " '14_f_52.jpg',\n",
       " '14_f_54.jpg',\n",
       " '14_f_56.jpg',\n",
       " '14_m_52.jpg',\n",
       " '15_f_12.jpg',\n",
       " '15_f_14.jpg',\n",
       " '15_f_18.jpg',\n",
       " '15_f_20.jpg',\n",
       " '15_f_22.jpg',\n",
       " '15_f_32.jpg',\n",
       " '15_f_36.jpg',\n",
       " '15_f_42.jpg',\n",
       " '15_f_48.jpg',\n",
       " '15_f_50.jpg',\n",
       " '15_f_52.jpg',\n",
       " '15_f_56.jpg',\n",
       " '15_m_52.jpg',\n",
       " '16_f_12.jpg',\n",
       " '16_f_14.jpg',\n",
       " '16_f_18.jpg',\n",
       " '16_f_20.jpg',\n",
       " '16_f_22.jpg',\n",
       " '16_f_32.jpg',\n",
       " '16_f_36.jpg',\n",
       " '16_f_48.jpg',\n",
       " '16_f_50.jpg',\n",
       " '16_f_52.jpg',\n",
       " '16_f_56.jpg',\n",
       " '17_f_12.jpg',\n",
       " '17_f_18.jpg',\n",
       " '17_f_20.jpg',\n",
       " '17_f_32.jpg',\n",
       " '17_f_36.jpg',\n",
       " '17_f_48.jpg',\n",
       " '17_f_50.jpg',\n",
       " '17_f_52.jpg',\n",
       " '17_f_56.jpg',\n",
       " '18_f_12.jpg',\n",
       " '18_f_14.jpg',\n",
       " '18_f_18.jpg',\n",
       " '18_f_20.jpg',\n",
       " '18_f_32.jpg',\n",
       " '18_f_36.jpg',\n",
       " '18_f_48.jpg',\n",
       " '18_f_50.jpg',\n",
       " '18_f_56.jpg',\n",
       " '19_f_12.jpg',\n",
       " '19_f_14.jpg',\n",
       " '19_f_18.jpg',\n",
       " '19_f_20.jpg',\n",
       " '19_f_36.jpg',\n",
       " '19_f_48.jpg',\n",
       " '19_f_50.jpg',\n",
       " '19_f_56.jpg',\n",
       " '1_f_10.jpg',\n",
       " '1_f_12.jpg',\n",
       " '1_f_14.jpg',\n",
       " '1_f_16.jpg',\n",
       " '1_f_18.jpg',\n",
       " '1_f_20.jpg',\n",
       " '1_f_22.jpg',\n",
       " '1_f_24.jpg',\n",
       " '1_f_26.jpg',\n",
       " '1_f_28.jpg',\n",
       " '1_f_30.jpg',\n",
       " '1_f_32.jpg',\n",
       " '1_f_34.jpg',\n",
       " '1_f_36.jpg',\n",
       " '1_f_38.jpg',\n",
       " '1_f_40.jpg',\n",
       " '1_f_42.jpg',\n",
       " '1_f_44.jpg',\n",
       " '1_f_46.jpg',\n",
       " '1_f_48.jpg',\n",
       " '1_f_50.jpg',\n",
       " '1_f_52.jpg',\n",
       " '1_f_54.jpg',\n",
       " '1_f_56.jpg',\n",
       " '1_f_8.jpg',\n",
       " '1_m_10.jpg',\n",
       " '1_m_12.jpg',\n",
       " '1_m_14.jpg',\n",
       " '1_m_16.jpg',\n",
       " '1_m_18.jpg',\n",
       " '1_m_20.jpg',\n",
       " '1_m_22.jpg',\n",
       " '1_m_24.jpg',\n",
       " '1_m_26.jpg',\n",
       " '1_m_28.jpg',\n",
       " '1_m_30.jpg',\n",
       " '1_m_32.jpg',\n",
       " '1_m_34.jpg',\n",
       " '1_m_36.jpg',\n",
       " '1_m_38.jpg',\n",
       " '1_m_40.jpg',\n",
       " '1_m_42.jpg',\n",
       " '1_m_44.jpg',\n",
       " '1_m_46.jpg',\n",
       " '1_m_48.jpg',\n",
       " '1_m_50.jpg',\n",
       " '1_m_52.jpg',\n",
       " '1_m_54.jpg',\n",
       " '1_m_56.jpg',\n",
       " '1_m_8.jpg',\n",
       " '20_f_12.jpg',\n",
       " '20_f_14.jpg',\n",
       " '20_f_18.jpg',\n",
       " '20_f_56.jpg',\n",
       " '21_f_12.jpg',\n",
       " '21_f_14.jpg',\n",
       " '21_f_18.jpg',\n",
       " '21_f_36.jpg',\n",
       " '21_f_56.jpg',\n",
       " '22_f_12.jpg',\n",
       " '22_f_18.jpg',\n",
       " '22_f_36.jpg',\n",
       " '23_f_12.jpg',\n",
       " '23_f_14.jpg',\n",
       " '24_f_12.jpg',\n",
       " '2_f_10.jpg',\n",
       " '2_f_12.jpg',\n",
       " '2_f_14.jpg',\n",
       " '2_f_16.jpg',\n",
       " '2_f_18.jpg',\n",
       " '2_f_20.jpg',\n",
       " '2_f_22.jpg',\n",
       " '2_f_24.jpg',\n",
       " '2_f_26.jpg',\n",
       " '2_f_28.jpg',\n",
       " '2_f_30.jpg',\n",
       " '2_f_32.jpg',\n",
       " '2_f_34.jpg',\n",
       " '2_f_36.jpg',\n",
       " '2_f_38.jpg',\n",
       " '2_f_40.jpg',\n",
       " '2_f_42.jpg',\n",
       " '2_f_44.jpg',\n",
       " '2_f_46.jpg',\n",
       " '2_f_48.jpg',\n",
       " '2_f_50.jpg',\n",
       " '2_f_52.jpg',\n",
       " '2_f_54.jpg',\n",
       " '2_f_56.jpg',\n",
       " '2_f_8.jpg',\n",
       " '2_m_10.jpg',\n",
       " '2_m_12.jpg',\n",
       " '2_m_14.jpg',\n",
       " '2_m_16.jpg',\n",
       " '2_m_20.jpg',\n",
       " '2_m_22.jpg',\n",
       " '2_m_24.jpg',\n",
       " '2_m_26.jpg',\n",
       " '2_m_28.jpg',\n",
       " '2_m_30.jpg',\n",
       " '2_m_32.jpg',\n",
       " '2_m_36.jpg',\n",
       " '2_m_40.jpg',\n",
       " '2_m_42.jpg',\n",
       " '2_m_44.jpg',\n",
       " '2_m_46.jpg',\n",
       " '2_m_48.jpg',\n",
       " '2_m_50.jpg',\n",
       " '2_m_52.jpg',\n",
       " '2_m_54.jpg',\n",
       " '2_m_56.jpg',\n",
       " '2_m_8.jpg',\n",
       " '3_f_10.jpg',\n",
       " '3_f_12.jpg',\n",
       " '3_f_14.jpg',\n",
       " '3_f_16.jpg',\n",
       " '3_f_18.jpg',\n",
       " '3_f_20.jpg',\n",
       " '3_f_22.jpg',\n",
       " '3_f_24.jpg',\n",
       " '3_f_26.jpg',\n",
       " '3_f_28.jpg',\n",
       " '3_f_30.jpg',\n",
       " '3_f_32.jpg',\n",
       " '3_f_34.jpg',\n",
       " '3_f_36.jpg',\n",
       " '3_f_38.jpg',\n",
       " '3_f_40.jpg',\n",
       " '3_f_42.jpg',\n",
       " '3_f_44.jpg',\n",
       " '3_f_46.jpg',\n",
       " '3_f_48.jpg',\n",
       " '3_f_50.jpg',\n",
       " '3_f_52.jpg',\n",
       " '3_f_54.jpg',\n",
       " '3_f_56.jpg',\n",
       " '3_f_8.jpg',\n",
       " '3_m_10.jpg',\n",
       " '3_m_12.jpg',\n",
       " '3_m_14.jpg',\n",
       " '3_m_22.jpg',\n",
       " '3_m_26.jpg',\n",
       " '3_m_28.jpg',\n",
       " '3_m_30.jpg',\n",
       " '3_m_32.jpg',\n",
       " '3_m_36.jpg',\n",
       " '3_m_40.jpg',\n",
       " '3_m_42.jpg',\n",
       " '3_m_44.jpg',\n",
       " '3_m_46.jpg',\n",
       " '3_m_48.jpg',\n",
       " '3_m_50.jpg',\n",
       " '3_m_52.jpg',\n",
       " '3_m_54.jpg',\n",
       " '3_m_56.jpg',\n",
       " '3_m_8.jpg',\n",
       " '4_f_10.jpg',\n",
       " '4_f_12.jpg',\n",
       " '4_f_14.jpg',\n",
       " '4_f_16.jpg',\n",
       " '4_f_18.jpg',\n",
       " '4_f_20.jpg',\n",
       " '4_f_22.jpg',\n",
       " '4_f_24.jpg',\n",
       " '4_f_26.jpg',\n",
       " '4_f_28.jpg',\n",
       " '4_f_30.jpg',\n",
       " '4_f_32.jpg',\n",
       " '4_f_34.jpg',\n",
       " '4_f_36.jpg',\n",
       " '4_f_38.jpg',\n",
       " '4_f_40.jpg',\n",
       " '4_f_42.jpg',\n",
       " '4_f_44.jpg',\n",
       " '4_f_46.jpg',\n",
       " '4_f_48.jpg',\n",
       " '4_f_50.jpg',\n",
       " '4_f_52.jpg',\n",
       " '4_f_54.jpg',\n",
       " '4_f_56.jpg',\n",
       " '4_f_8.jpg',\n",
       " '4_m_10.jpg',\n",
       " '4_m_12.jpg',\n",
       " '4_m_14.jpg',\n",
       " '4_m_22.jpg',\n",
       " '4_m_26.jpg',\n",
       " '4_m_28.jpg',\n",
       " '4_m_30.jpg',\n",
       " '4_m_36.jpg',\n",
       " '4_m_40.jpg',\n",
       " '4_m_42.jpg',\n",
       " '4_m_44.jpg',\n",
       " '4_m_46.jpg',\n",
       " '4_m_50.jpg',\n",
       " '4_m_54.jpg',\n",
       " '4_m_56.jpg',\n",
       " '4_m_8.jpg',\n",
       " '5_f_12.jpg',\n",
       " '5_f_14.jpg',\n",
       " '5_f_16.jpg',\n",
       " '5_f_18.jpg',\n",
       " '5_f_22.jpg',\n",
       " '5_f_28.jpg',\n",
       " '5_f_30.jpg',\n",
       " '5_f_32.jpg',\n",
       " '5_f_34.jpg',\n",
       " '5_f_36.jpg',\n",
       " '5_f_38.jpg',\n",
       " '5_f_40.jpg',\n",
       " '5_f_42.jpg',\n",
       " '5_f_44.jpg',\n",
       " '5_f_46.jpg',\n",
       " '5_f_48.jpg',\n",
       " '5_f_50.jpg',\n",
       " '5_f_52.jpg',\n",
       " '5_f_54.jpg',\n",
       " '5_f_56.jpg',\n",
       " '5_f_8.jpg',\n",
       " '5_m_12.jpg',\n",
       " '5_m_14.jpg',\n",
       " '5_m_22.jpg',\n",
       " '5_m_28.jpg',\n",
       " '5_m_30.jpg',\n",
       " '5_m_36.jpg',\n",
       " '5_m_42.jpg',\n",
       " '5_m_44.jpg',\n",
       " '5_m_46.jpg',\n",
       " '5_m_50.jpg',\n",
       " '5_m_52.jpg',\n",
       " '5_m_54.jpg',\n",
       " '5_m_56.jpg',\n",
       " '5_m_8.jpg',\n",
       " '6_f_12.jpg',\n",
       " '6_f_14.jpg',\n",
       " '6_f_16.jpg',\n",
       " '6_f_18.jpg',\n",
       " '6_f_20.jpg',\n",
       " '6_f_22.jpg',\n",
       " '6_f_24.jpg',\n",
       " '6_f_26.jpg',\n",
       " '6_f_28.jpg',\n",
       " '6_f_30.jpg',\n",
       " '6_f_32.jpg',\n",
       " '6_f_34.jpg',\n",
       " '6_f_36.jpg',\n",
       " '6_f_38.jpg',\n",
       " '6_f_40.jpg',\n",
       " '6_f_42.jpg',\n",
       " '6_f_44.jpg',\n",
       " '6_f_46.jpg',\n",
       " '6_f_48.jpg',\n",
       " '6_f_50.jpg',\n",
       " '6_f_52.jpg',\n",
       " '6_f_54.jpg',\n",
       " '6_f_56.jpg',\n",
       " '6_f_8.jpg',\n",
       " '6_m_12.jpg',\n",
       " '6_m_22.jpg',\n",
       " '6_m_30.jpg',\n",
       " '6_m_44.jpg',\n",
       " '6_m_46.jpg',\n",
       " '6_m_50.jpg',\n",
       " '6_m_52.jpg',\n",
       " '6_m_54.jpg',\n",
       " '6_m_56.jpg',\n",
       " '6_m_8.jpg',\n",
       " '7_f_10.jpg',\n",
       " '7_f_12.jpg',\n",
       " '7_f_14.jpg',\n",
       " '7_f_16.jpg',\n",
       " '7_f_18.jpg',\n",
       " '7_f_20.jpg',\n",
       " '7_f_22.jpg',\n",
       " '7_f_24.jpg',\n",
       " '7_f_26.jpg',\n",
       " '7_f_28.jpg',\n",
       " '7_f_30.jpg',\n",
       " '7_f_32.jpg',\n",
       " '7_f_34.jpg',\n",
       " '7_f_36.jpg',\n",
       " '7_f_38.jpg',\n",
       " '7_f_40.jpg',\n",
       " '7_f_42.jpg',\n",
       " '7_f_44.jpg',\n",
       " '7_f_46.jpg',\n",
       " '7_f_48.jpg',\n",
       " '7_f_50.jpg',\n",
       " '7_f_52.jpg',\n",
       " '7_f_54.jpg',\n",
       " '7_f_56.jpg',\n",
       " '7_f_8.jpg',\n",
       " '7_m_14.jpg',\n",
       " '7_m_30.jpg',\n",
       " '7_m_44.jpg',\n",
       " '7_m_46.jpg',\n",
       " '7_m_50.jpg',\n",
       " '7_m_52.jpg',\n",
       " '7_m_54.jpg',\n",
       " '7_m_56.jpg',\n",
       " '7_m_8.jpg',\n",
       " '8_f_10.jpg',\n",
       " '8_f_12.jpg',\n",
       " '8_f_14.jpg',\n",
       " '8_f_16.jpg',\n",
       " '8_f_18.jpg',\n",
       " '8_f_20.jpg',\n",
       " '8_f_22.jpg',\n",
       " '8_f_24.jpg',\n",
       " '8_f_26.jpg',\n",
       " '8_f_28.jpg',\n",
       " '8_f_30.jpg',\n",
       " '8_f_32.jpg',\n",
       " '8_f_34.jpg',\n",
       " '8_f_36.jpg',\n",
       " '8_f_38.jpg',\n",
       " '8_f_40.jpg',\n",
       " '8_f_42.jpg',\n",
       " '8_f_44.jpg',\n",
       " '8_f_46.jpg',\n",
       " '8_f_48.jpg',\n",
       " '8_f_50.jpg',\n",
       " '8_f_52.jpg',\n",
       " '8_f_54.jpg',\n",
       " '8_f_56.jpg',\n",
       " '8_f_8.jpg',\n",
       " '8_m_30.jpg',\n",
       " '8_m_46.jpg',\n",
       " '8_m_52.jpg',\n",
       " '8_m_54.jpg',\n",
       " '8_m_56.jpg',\n",
       " '9_f_10.jpg',\n",
       " '9_f_12.jpg',\n",
       " '9_f_14.jpg',\n",
       " '9_f_16.jpg',\n",
       " '9_f_18.jpg',\n",
       " '9_f_20.jpg',\n",
       " '9_f_22.jpg',\n",
       " '9_f_24.jpg',\n",
       " '9_f_30.jpg',\n",
       " '9_f_32.jpg',\n",
       " '9_f_34.jpg',\n",
       " '9_f_36.jpg',\n",
       " '9_f_38.jpg',\n",
       " '9_f_40.jpg',\n",
       " '9_f_42.jpg',\n",
       " '9_f_44.jpg',\n",
       " '9_f_46.jpg',\n",
       " '9_f_48.jpg',\n",
       " '9_f_50.jpg',\n",
       " '9_f_52.jpg',\n",
       " '9_f_54.jpg',\n",
       " '9_f_56.jpg',\n",
       " '9_f_8.jpg',\n",
       " '9_m_46.jpg',\n",
       " '9_m_52.jpg',\n",
       " '9_m_54.jpg',\n",
       " '9_m_56.jpg']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dnnlib.util.construct_class_by_name(**dict(\n",
    "        class_name=\"training.dataset.ImageFolderDataset\",  \n",
    "        path=fake_dir,\n",
    "        use_labels=False,\n",
    "        xflip=False,\n",
    "        max_size=None,\n",
    "    ))\n",
    "dataset._image_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1b68ac10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([561])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a363fea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"fname\":dataset._image_fnames, \"scores\":scores.cpu().numpy(), \"scores2\":scores.cpu().numpy()})\n",
    "df.plot.scatter(x=\"scores\", y=\"scores2\", alpha=0.1)\n",
    "ind= len(df)//2+100\n",
    "im_path = df.sort_values(by=\"scores\", ascending=False)[\"fname\"].values[ind]\n",
    "s = df.sort_values(by=\"scores\", ascending=False)[\"scores\"].values[ind]\n",
    "# load and show\n",
    "im = Image.open(fake_dir+'/'+im_path)\n",
    "# add text\n",
    "from PIL import ImageDraw\n",
    "draw = ImageDraw.Draw(im)\n",
    "draw.text((10, 10), f\"Score: {s:.2f}\", fill=\"red\", size=4000)\n",
    "im.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22439950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "base = fake_dir\n",
    "# Sort by \"scores\" descending and get top 50%\n",
    "df_sorted_scores = df.sort_values(by=\"scores\", ascending=False)\n",
    "top_50pct_scores = df_sorted_scores.head(len(df_sorted_scores) // 2)\n",
    "\n",
    "out_dir_scores = \"top50_by_scores\"\n",
    "\n",
    "os.makedirs(base+out_dir_scores, exist_ok=True)\n",
    "\n",
    "# Copy files for top 50% by \"scores\"\n",
    "for fname in top_50pct_scores[\"fname\"]:\n",
    "    src = base\n",
    "    # If the file path is not absolute, you may need to prepend the dataset path\n",
    "    if not os.path.isabs(src):\n",
    "        src = os.path.join(dataset.path, src)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src+f\"/{fname}\", os.path.join(base+out_dir_scores, os.path.basename(fname)))\n",
    "\n",
    "# # Sort by \"scores2\" descending and get top 50%\n",
    "# df_sorted_scores2 = df.sort_values(by=\"scores2\", ascending=False)\n",
    "# top_50pct_scores2 = df_sorted_scores2.head(len(df_sorted_scores2) // 5)\n",
    "\n",
    "# # Sort by \"scores2\" descending and get top 50%\n",
    "# df['combined'] = df['scores'] + df['scores2']\n",
    "# df_sorted_combined = df.sort_values(by=\"combined\", ascending=False)\n",
    "# top_50pct_combined = df_sorted_combined.head(len(df_sorted_combined) // 5)\n",
    "\n",
    "# # Output directories\n",
    "# out_dir_scores2 = \"top50_by_scores2\"\n",
    "# out_dir_combined = \"top50_by_combined\"\n",
    "# os.makedirs(out_dir_scores2, exist_ok=True)\n",
    "# os.makedirs(out_dir_combined, exist_ok=True)\n",
    "\n",
    "# combined\n",
    "# for fname in top_50pct_combined[\"fname\"]:\n",
    "#     src = base+fname\n",
    "#     if not os.path.isabs(src):\n",
    "#         src = os.path.join(dataset.path, src)\n",
    "#     if os.path.exists(src):\n",
    "#         shutil.copy(src, os.path.join(out_dir_combined, os.path.basename(fname)))\n",
    "\n",
    "\n",
    "# # Copy files for top 50% by \"scores2\"\n",
    "# for fname in top_50pct_scores2[\"fname\"]:\n",
    "#     src = base+fname\n",
    "#     if not os.path.isabs(src):\n",
    "#         src = os.path.join(dataset.path, src)\n",
    "#     if os.path.exists(src):\n",
    "#         shutil.copy(src, os.path.join(out_dir_scores2, os.path.basename(fname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e335e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22428"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "with open('/Users/adamsobieszek/PycharmProjects/_manipy/deixis/face_utils/scores.pkl', 'rb') as fp:\n",
    "    scores2 = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "len(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0472a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '/Users/adamsobieszek/PycharmProjects/_manipy/additional_age_faces_filtered_via_feature_distance/'\n",
    "for f in os.listdir(c):\n",
    "    os.rename(c+f\"{f}\", c+f\"{f.replace('.pt.jpg', '.jpg')}\")\n",
    "    # shutil.copy('/Users/adamsobieszek/PycharmProjects/_manipy/deixis/top50_by_combined/'+f\"{f}\", base+out_dir_scores+f\"/{f}\")\n",
    "    # shutil.copy('/Users/adamsobieszek/Downloads/content 2/drive/Shareddrives/[PsychGAN]/age_test_redo_additional/coords/'+f\"{f.replace('.jpg', '')}\", base+out_dir_scores+f\"/{f.replace('.jpg', '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fda290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Edge partial-face detection ----------\n",
    "\n",
    "def _skin_mask_bgr(img_bgr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Race-agnostic skin mask using YCrCb + HSV union, then cleaned.\"\"\"\n",
    "    ycrcb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "    Y, Cr, Cb = cv2.split(ycrcb)\n",
    "    # classic YCrCb box (wide)\n",
    "    m1 = (Cr >= 133) & (Cr <= 180) & (Cb >= 77) & (Cb <= 135) & (Y >= 40)\n",
    "\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "    H, S, V = cv2.split(hsv)\n",
    "    # wide HSV skin zone\n",
    "    m2 = ((H <= 25) | (H >= 160)) & (S >= 30) & (V >= 40)\n",
    "\n",
    "    m = (m1 | m2).astype(np.uint8) * 255\n",
    "    # clean\n",
    "    m = cv2.medianBlur(m, 5)\n",
    "    m = cv2.morphologyEx(m, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
    "    m = cv2.morphologyEx(m, cv2.MORPH_OPEN,  np.ones((3,3), np.uint8))\n",
    "    return m\n",
    "# ---------- replace in your code ----------\n",
    "\n",
    "def _strip_boxes(h: int, w: int, frac: float = 0.18):\n",
    "    t = max(1, int(round(frac * min(h, w))))  # ensure at least 1 px\n",
    "    return {\n",
    "        \"left\":   (slice(0, h), slice(0, t)),\n",
    "        \"right\":  (slice(0, h), slice(w - t, w)),\n",
    "        \"top\":    (slice(0, t), slice(0, w)),\n",
    "        \"bottom\": (slice(h - t, h), slice(0, w)),\n",
    "    }\n",
    "\n",
    "def _eye_cue(gray: np.ndarray, skin_mask: np.ndarray) -> float:\n",
    "    \"\"\"Return [0,1] eye-likeliness in a region (dark circle in skin).\"\"\"\n",
    "    # Skip tiny/empty ROIs\n",
    "    if gray.size == 0 or gray.shape[0] < 8 or gray.shape[1] < 8:\n",
    "        return 0.0\n",
    "\n",
    "    # emphasize dark blobs on skin\n",
    "    g = cv2.GaussianBlur(gray, (0,0), 1.2)\n",
    "    dog = cv2.Laplacian(g, cv2.CV_32F) * -1.0  # dark-on-light positive\n",
    "    if skin_mask is not None and skin_mask.size == gray.size:\n",
    "        dog = np.where(skin_mask.astype(bool), dog, 0.0)\n",
    "\n",
    "    # HoughCircles needs CV_8UC1\n",
    "    dog8 = cv2.normalize(dog, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    dog8 = cv2.equalizeHist(dog8)\n",
    "\n",
    "    h, w = gray.shape\n",
    "    rmin = max(2, int(0.01 * min(h, w)))\n",
    "    rmax = max(rmin + 1, int(0.04 * min(h, w)))\n",
    "\n",
    "    circles = cv2.HoughCircles(\n",
    "        dog8, cv2.HOUGH_GRADIENT, dp=1.2,\n",
    "        minDist=max(4, int(0.08 * min(h, w))),\n",
    "        param1=80, param2=8,\n",
    "        minRadius=rmin, maxRadius=rmax\n",
    "    )\n",
    "    if circles is None:\n",
    "        return 0.0\n",
    "    c = min(3, circles.shape[1])\n",
    "    return float(min(1.0, c / 2.0))\n",
    "\n",
    "def _mouth_cue(img_bgr: np.ndarray, skin_mask: np.ndarray) -> float:\n",
    "    \"\"\"Return [0,1] mouth-likeliness (reddish elongated blob in skin).\"\"\"\n",
    "    b,g,r = cv2.split(img_bgr.astype(np.int16))\n",
    "    lipness = np.clip((r - (g+ b)//2), 0, 255).astype(np.uint8)\n",
    "    lipness[skin_mask == 0] = 0\n",
    "    lipness = cv2.GaussianBlur(lipness, (0,0), 1.0)\n",
    "    _, th = cv2.threshold(lipness, 0, 255, cv2.THRESH_OTSU)\n",
    "    th = cv2.morphologyEx(th, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
    "    cnts,_ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    best = 0.0\n",
    "    for c in cnts:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area < 25: \n",
    "            continue\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        ar = w/(h+1e-6)\n",
    "        if ar >= 1.4:  # elongated horizontally\n",
    "            best = max(best, min(1.0, (area / (img_bgr.shape[0]*img_bgr.shape[1]))*80.0))\n",
    "    return best\n",
    "\n",
    "def _cheek_arc_cue(skin_mask: np.ndarray, border_side: str) -> float:\n",
    "    \"\"\"\n",
    "    Look for a large skin component touching the border that fits an inward-facing ellipse arc.\n",
    "    Returns [0,1].\n",
    "    \"\"\"\n",
    "    h,w = skin_mask.shape\n",
    "    # components\n",
    "    cnts,_ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    best = 0.0\n",
    "    for c in cnts:\n",
    "        if len(c) < 10: \n",
    "            continue\n",
    "        area = cv2.contourArea(c)\n",
    "        if area < 0.003 * h * w:\n",
    "            continue\n",
    "        x,y,bw,bh = cv2.boundingRect(c)\n",
    "        touches = {\n",
    "            \"left\":   x <= 1,\n",
    "            \"right\":  x + bw >= w-2,\n",
    "            \"top\":    y <= 1,\n",
    "            \"bottom\": y + bh >= h-2\n",
    "        }[border_side]\n",
    "        if not touches:\n",
    "            continue\n",
    "        # fit ellipse for curvature check\n",
    "        if len(c) >= 5:\n",
    "            (cx, cy), (ma, mi), angle = cv2.fitEllipse(c)\n",
    "            major = max(ma, mi)\n",
    "            # center must be inside image and offset inward from that border\n",
    "            inward = {\n",
    "                \"left\":   cx > 0.25*major,\n",
    "                \"right\":  (w - cx) > 0.25*major,\n",
    "                \"top\":    cy > 0.25*major,\n",
    "                \"bottom\": (h - cy) > 0.25*major\n",
    "            }[border_side]\n",
    "            if inward:\n",
    "                # strength grows with area and curvature smoothness\n",
    "                perim = cv2.arcLength(c, True) + 1e-6\n",
    "                smooth = min(1.0, (area/perim) / 4.0)  # crude convex smoothness\n",
    "                score = min(1.0, (area/(h*w))*150.0) * 0.6 + 0.4*smooth\n",
    "                best = max(best, score)\n",
    "    return best\n",
    "\n",
    "\n",
    "def detect_partial_face_edges(img_bgr: np.ndarray, main_bbox: Optional[np.ndarray]=None,\n",
    "                              debug: bool=False) -> Tuple[bool, dict]:\n",
    "    \"\"\"\n",
    "    Return (has_partial_face, details_dict).\n",
    "    Heavily biased toward catching *any* face fragment touching the image border,\n",
    "    while trying to ignore neck/shoulder skin.\n",
    "    \"\"\"\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    # Optionally blank out the central main face bbox so it's not counted as 'edge'\n",
    "    mask_exclude = np.zeros((h,w), np.uint8)\n",
    "    if main_bbox is not None:\n",
    "        x1,y1,x2,y2 = np.array(main_bbox, int)\n",
    "        cv2.rectangle(mask_exclude, (x1,y1), (x2,y2), 255, -1)\n",
    "\n",
    "    strips = _strip_boxes(h, w, frac=0.18)\n",
    "    skin = _skin_mask_bgr(img_bgr)\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    details = {}\n",
    "    fired_any = False\n",
    "    for side, (ys, xs) in strips.items():\n",
    "        roi_skin = skin[ys, xs].copy()\n",
    "        roi_skin[mask_exclude[ys, xs] > 0] = 0  # exclude main face region if provided\n",
    "        if roi_skin.sum() < 100:  # almost no skin on this edge\n",
    "            details[side] = {\"skin_frac\": 0.0, \"eye\": 0.0, \"mouth\": 0.0, \"cheek\": 0.0, \"score\": 0.0}\n",
    "            continue\n",
    "\n",
    "        roi_bgr = img_bgr[ys, xs]\n",
    "        roi_gray = gray[ys, xs]\n",
    "\n",
    "        skin_frac = float(roi_skin.mean()/255.0)\n",
    "\n",
    "        eye_s   = _eye_cue(roi_gray, roi_skin)\n",
    "        mouth_s = _mouth_cue(roi_bgr, roi_skin)\n",
    "        cheek_s = _cheek_arc_cue(roi_skin, side)\n",
    "\n",
    "        # Combine: require skin + (eye OR mouth OR cheek_arc)\n",
    "        score = (0.4*eye_s + 0.3*mouth_s + 0.3*cheek_s) * (0.5 + 0.5*skin_frac)\n",
    "\n",
    "        # Heavy-handed thresholds:\n",
    "        # - fire if strong cue OR moderate cue with lots of skin on the edge\n",
    "        fire = (score >= 0.35) or (skin_frac >= 0.25 and (eye_s >= 0.25 or mouth_s >= 0.25 or cheek_s >= 0.25))\n",
    "\n",
    "        details[side] = {\n",
    "            \"skin_frac\": skin_frac,\n",
    "            \"eye\": eye_s,\n",
    "            \"mouth\": mouth_s,\n",
    "            \"cheek\": cheek_s,\n",
    "            \"score\": score,\n",
    "            \"fire\": bool(fire)\n",
    "        }\n",
    "        fired_any |= fire\n",
    "\n",
    "    if debug:\n",
    "        print(details)\n",
    "    return fired_any, details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Callable, Dict, List, Mapping, Tuple, Optional\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# type for metric functions: receive a read-only context, return either a float\n",
    "# or a (name, value) pair. If only a float is returned, the metric's __name__ is used.\n",
    "MetricFn = Callable[[Mapping[str, Any]], float | Tuple[str, float]]\n",
    "# ---- helpers ----\n",
    "def _coalesce_ndarray(*vals):\n",
    "    \"\"\"\n",
    "    Return the first value that is not None and, if ndarray-like, has size > 0.\n",
    "    \"\"\"\n",
    "    for v in vals:\n",
    "        if v is None:\n",
    "            continue\n",
    "        if isinstance(v, np.ndarray):\n",
    "            if v.size == 0:\n",
    "                continue\n",
    "        return v\n",
    "    return None\n",
    "\n",
    "# ---- replace your _build_metric_context with this version ----\n",
    "def _build_metric_context(vision_frame: VisionFrame) -> Dict[str, Any]:\n",
    "    ctx: Dict[str, Any] = {}\n",
    "    h, w = vision_frame.shape[:2]\n",
    "    ctx[\"vision_frame\"] = vision_frame\n",
    "    ctx[\"H\"], ctx[\"W\"] = h, w\n",
    "\n",
    "    faces = get_many_faces([vision_frame])\n",
    "    ctx[\"faces\"] = faces\n",
    "    ctx[\"num_faces\"] = len(faces)\n",
    "\n",
    "    if not faces:\n",
    "        ctx.update({\n",
    "            \"face\": None, \"bbox\": None, \"bbox_w\": None,\n",
    "            \"lmk68\": None, \"lmk5\": None,\n",
    "            \"crop\": None, \"gray\": None,\n",
    "            \"full_gray\": cv2.cvtColor(vision_frame, cv2.COLOR_BGR2GRAY),\n",
    "            \"det_raw\": None, \"emb_norm\": None\n",
    "        })\n",
    "        return ctx\n",
    "\n",
    "    face: Face = max(faces, key=lambda f: f.score_set.get('detector', 0.0))\n",
    "    x1, y1, x2, y2 = face.bounding_box\n",
    "    bbox = np.array([x1, y1, x2, y2], dtype=float)\n",
    "    bbox_w = max(1.0, x2 - x1)\n",
    "\n",
    "    lmk68 = face.landmark_set.get('68')\n",
    "    # SAFE coalesce instead of \"or\"\n",
    "    lmk5  = _coalesce_ndarray(face.landmark_set.get('5/68'),\n",
    "                              face.landmark_set.get('5'))\n",
    "\n",
    "    crop = _crop_from_bbox(vision_frame, bbox)\n",
    "    if crop.size == 0:\n",
    "        crop = vision_frame\n",
    "    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    full_gray = cv2.cvtColor(vision_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    ctx.update({\n",
    "        \"face\": face,\n",
    "        \"bbox\": bbox,\n",
    "        \"bbox_w\": bbox_w,\n",
    "        \"lmk68\": lmk68,\n",
    "        \"lmk5\":  lmk5,\n",
    "        \"crop\": crop,\n",
    "        \"gray\": gray,\n",
    "        \"full_gray\": full_gray,\n",
    "        \"det_raw\": float(face.score_set.get('detector', 0.0)),\n",
    "        \"emb_norm\": getattr(face, \"embedding_norm\", None),\n",
    "    })\n",
    "    return ctx\n",
    "\n",
    "def evaluate_metrics(\n",
    "    vision_frame: VisionFrame,\n",
    "    metric_fns: List[MetricFn],\n",
    "    *,\n",
    "    include_meta: bool = True\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Run a list of metric functions on a single image.\n",
    "    Each metric_fn(ctx) may return either a float or ('name', float).\n",
    "    Returns {name: value}. Adds a few useful meta entries when include_meta=True.\n",
    "    \"\"\"\n",
    "    ctx = _build_metric_context(vision_frame)\n",
    "    out: Dict[str, float] = {}\n",
    "\n",
    "    # Optional meta so you can filter in tests\n",
    "    if include_meta:\n",
    "        out[\"meta_num_faces\"] = float(ctx[\"num_faces\"])\n",
    "        out[\"meta_has_faces\"] = 1.0 if ctx[\"num_faces\"] > 0 else 0.0\n",
    "\n",
    "    for fn in metric_fns:\n",
    "        name = getattr(fn, \"__name__\", \"metric\")\n",
    "        val = fn(ctx)\n",
    "        if isinstance(val, tuple) and len(val) == 2:\n",
    "            name, score = val  # ('custom_name', value)\n",
    "        else:\n",
    "            score = float(val)  # use fn.__name__ as key\n",
    "        out[name] = float(score)\n",
    "        # keep testing even if a metric fails\n",
    "    return out\n",
    "\n",
    "def m_det(ctx):  # detector confidence â†’ [0,1]\n",
    "    det_raw = ctx[\"det_raw\"]\n",
    "    return (\"det\", _scale01(det_raw, DET_SCORE_MIN, DET_SCORE_MAX)) if det_raw is not None else (\"det\", 0.0)\n",
    "\n",
    "def m_geom(ctx):\n",
    "    l68, bw = ctx[\"lmk68\"], ctx[\"bbox_w\"]\n",
    "    return (\"geom\", _geom_symmetry_score(l68, bw) if _has_68(l68) else 0.7)\n",
    "\n",
    "def m_pose(ctx):\n",
    "    l68, l5 = ctx[\"lmk68\"], ctx[\"lmk5\"]\n",
    "    if _has_68(l68):\n",
    "        yaw, pitch, roll = _estimate_pose_from_68(l68)\n",
    "    elif l5 is not None:\n",
    "        yaw, pitch, roll = _pose_from_5(l5)\n",
    "    else:\n",
    "        return (\"pose\", 0.5)\n",
    "    yaw_p = _clamp01(1.0 - abs(yaw)/POSE_MAX_YAW)\n",
    "    pit_p = _clamp01(1.0 - abs(pitch)/POSE_MAX_PITCH)\n",
    "    rol_p = _clamp01(1.0 - abs(roll)/POSE_MAX_ROLL)\n",
    "    return (\"pose\", 0.4*yaw_p + 0.3*pit_p + 0.3*rol_p)\n",
    "\n",
    "def m_sharp(ctx):\n",
    "    if ctx[\"gray\"] is None: return (\"sharp\", 0.0)\n",
    "    lapv = _laplacian_var(ctx[\"gray\"])\n",
    "    return (\"sharp\", _clamp01((lapv - LAPLACIAN_BAD) / (LAPLACIAN_GOOD - LAPLACIAN_BAD)))\n",
    "\n",
    "def m_exposure(ctx):\n",
    "    if ctx[\"gray\"] is None: return (\"exposure\", 0.0)\n",
    "    g = ctx[\"gray\"]; mean, std = float(g.mean()), float(g.std())\n",
    "    if mean < EXPO_LOW: band = _scale01(mean, 0.0, EXPO_LOW)\n",
    "    elif mean > EXPO_HIGH: band = _scale01(255.0 - mean, 0.0, 255.0-EXPO_HIGH)\n",
    "    else: band = 1.0\n",
    "    spread = _scale01(std, EXPO_STD_MIN, 90.0)\n",
    "    return (\"exposure\", 0.7*band + 0.3*spread)\n",
    "\n",
    "def m_center(ctx):\n",
    "    if ctx[\"bbox\"] is None: return (\"center\", 0.0)\n",
    "    return (\"center\", _centering_score(ctx[\"bbox\"], ctx[\"W\"], ctx[\"H\"]))\n",
    "\n",
    "def m_occl(ctx):\n",
    "    if not _has_68(ctx[\"lmk68\"]): return (\"occl\", 0.5)\n",
    "    return (\"occl\", _occlusion_score(ctx[\"full_gray\"], ctx[\"lmk68\"], ctx[\"bbox\"]))\n",
    "# === Edge-partial metrics (uses detect_partial_face_edges) ===\n",
    "\n",
    "def _edge_info(ctx):\n",
    "    \"\"\"\n",
    "    Compute (or reuse cached) edge-partial result for this image.\n",
    "    Returns (has_partial: bool, details: dict).\n",
    "    \"\"\"\n",
    "    key = \"_edge_info_cached\"\n",
    "    if key in ctx:\n",
    "        return ctx[key]\n",
    "    bbox = ctx.get(\"bbox\")\n",
    "    main_bbox = None if bbox is None else np.array(bbox, dtype=int)\n",
    "    has_partial, details = detect_partial_face_edges(\n",
    "        ctx[\"vision_frame\"], main_bbox=main_bbox, debug=False\n",
    "    )\n",
    "    ctx[key] = (has_partial, details)  # cache for sibling metrics\n",
    "    return ctx[key]\n",
    "\n",
    "def m_edge_partial(ctx):\n",
    "    has, _ = _edge_info(ctx)\n",
    "    return (\"edge_partial\", 1.0 if has else 0.0)\n",
    "\n",
    "def make_edge_fire(side: str):\n",
    "    def _m(ctx):\n",
    "        _, det = _edge_info(ctx)\n",
    "        d = det.get(side, {})\n",
    "        return (f\"edge_{side}_fire\", 1.0 if d.get(\"fire\", False) else 0.0)\n",
    "    _m.__name__ = f\"m_edge_{side}_fire\"\n",
    "    return _m\n",
    "\n",
    "def make_edge_score(side: str):\n",
    "    def _m(ctx):\n",
    "        _, det = _edge_info(ctx)\n",
    "        d = det.get(side, {})\n",
    "        return (f\"edge_{side}_score\", float(d.get(\"score\", 0.0)))\n",
    "    _m.__name__ = f\"m_edge_{side}_score\"\n",
    "    return _m\n",
    "\n",
    "def make_edge_skin_frac(side: str):\n",
    "    def _m(ctx):\n",
    "        _, det = _edge_info(ctx)\n",
    "        d = det.get(side, {})\n",
    "        return (f\"edge_{side}_skin_frac\", float(d.get(\"skin_frac\", 0.0)))\n",
    "    _m.__name__ = f\"m_edge_{side}_skin\"\n",
    "    return _m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80522c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# if you already defined these earlier, keep your versions:\n",
    "def _strip_boxes(h: int, w: int, frac: float = 0.18):\n",
    "    t = max(1, int(round(frac * min(h, w))))\n",
    "    # widen at very low resolutions\n",
    "    if max(h, w) <= 256:\n",
    "        t = max(t, int(0.25 * min(h, w)))\n",
    "    return {\n",
    "        \"left\":   (slice(0, h), slice(0, t)),\n",
    "        \"right\":  (slice(0, h), slice(w - t, w)),\n",
    "        \"top\":    (slice(0, t), slice(0, w)),\n",
    "        \"bottom\": (slice(h - t, h), slice(0, w)),\n",
    "    }\n",
    "\n",
    "def _skin_mask_bgr(img_bgr: np.ndarray) -> np.ndarray:\n",
    "    ycrcb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "    Y, Cr, Cb = cv2.split(ycrcb)\n",
    "    m1 = (Cr >= 133) & (Cr <= 180) & (Cb >= 77) & (Cb <= 135) & (Y >= 40)\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "    H, S, V = cv2.split(hsv)\n",
    "    m2 = ((H <= 25) | (H >= 160)) & (S >= 30) & (V >= 40)\n",
    "    m = (m1 | m2).astype(np.uint8) * 255\n",
    "    m = cv2.medianBlur(m, 5)\n",
    "    m = cv2.morphologyEx(m, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
    "    m = cv2.morphologyEx(m, cv2.MORPH_OPEN,  np.ones((3,3), np.uint8))\n",
    "    return m\n",
    "\n",
    "def _bbox_iou(a, b) -> float:\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    ix1, iy1 = max(ax1, bx1), max(ay1, by1)\n",
    "    ix2, iy2 = min(ax2, bx2), min(ay2, by2)\n",
    "    iw, ih = max(0, ix2 - ix1), max(0, iy2 - iy1)\n",
    "    inter = iw * ih\n",
    "    area_a = max(0, ax2-ax1) * max(0, ay2-ay1)\n",
    "    area_b = max(0, bx2-bx1) * max(0, by2-by1)\n",
    "    union = area_a + area_b - inter + 1e-6\n",
    "    return inter / union\n",
    "\n",
    "def detect_similar_skin_blob_outside_bbox(\n",
    "    img_bgr: np.ndarray,\n",
    "    main_bbox: np.ndarray,\n",
    "    *,\n",
    "    border_frac: float = 0.18,\n",
    "    chi2_thresh: float = 9.0,           # ~99% for 2D Gaussian (Cr,Cb)\n",
    "    min_area_frac: float = 0.0015,      # of full image; auto clamps below\n",
    "    exclude_neck: bool = True\n",
    ") -> tuple[bool, dict]:\n",
    "    \"\"\"\n",
    "    Detects border-touching blobs outside main_bbox whose chroma (Cr,Cb) matches\n",
    "    the main face's skin statistics. Returns (has_blob, details).\n",
    "    \"\"\"\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    x1, y1, x2, y2 = np.array(main_bbox, int)\n",
    "    x1 = np.clip(x1, 0, w-1); x2 = np.clip(x2, 0, w-1)\n",
    "    y1 = np.clip(y1, 0, h-1); y2 = np.clip(y2, 0, h-1)\n",
    "\n",
    "    ycrcb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "    Y, Cr, Cb = cv2.split(ycrcb)\n",
    "\n",
    "    # Skin mask inside main face (fallback to whole box if empty)\n",
    "    main_skin = _skin_mask_bgr(img_bgr)[y1:y2, x1:x2]\n",
    "    sel = main_skin > 0\n",
    "    Cr_box = Cr[y1:y2, x1:x2][sel]\n",
    "    Cb_box = Cb[y1:y2, x1:x2][sel]\n",
    "    if Cr_box.size < 50:\n",
    "        Cr_box = Cr[y1:y2, x1:x2].ravel()\n",
    "        Cb_box = Cb[y1:y2, x1:x2].ravel()\n",
    "\n",
    "    # robust chroma center & scale (median + MAD)\n",
    "    mu = np.array([np.median(Cr_box), np.median(Cb_box)], dtype=np.float32)\n",
    "    mad = 1.4826 * np.array([\n",
    "        np.median(np.abs(Cr_box - mu[0])) + 1e-6,\n",
    "        np.median(np.abs(Cb_box - mu[1])) + 1e-6\n",
    "    ], dtype=np.float32)\n",
    "    inv_var = 1.0 / (mad ** 2)\n",
    "\n",
    "    # Mahalanobis distance (diag) for all pixels\n",
    "    dcr = (Cr.astype(np.float32) - mu[0]) ** 2 * inv_var[0]\n",
    "    dcb = (Cb.astype(np.float32) - mu[1]) ** 2 * inv_var[1]\n",
    "    d2  = dcr + dcb\n",
    "\n",
    "    # Candidate mask: similar chroma & skin-like & outside main bbox\n",
    "    sim = (d2 < chi2_thresh).astype(np.uint8) * 255\n",
    "    skin = _skin_mask_bgr(img_bgr)\n",
    "    cand = cv2.bitwise_and(sim, skin)\n",
    "\n",
    "    # Blank a slightly shrunken version of main bbox so neighbors remain\n",
    "    shrink_x = int(0.08 * (x2 - x1))\n",
    "    shrink_y = int(0.08 * (y2 - y1))\n",
    "    ex1 = max(0, x1 + shrink_x); ex2 = min(w-1, x2 - shrink_x)\n",
    "    ey1 = max(0, y1 + shrink_y); ey2 = min(h-1, y2 - shrink_y)\n",
    "    cand[ey1:ey2, ex1:ex2] = 0\n",
    "\n",
    "    # Keep only border strips\n",
    "    strips = _strip_boxes(h, w, border_frac)\n",
    "    mask_border = np.zeros((h, w), np.uint8)\n",
    "    for s in strips.values():\n",
    "        mask_border[s] = 255\n",
    "    cand = cv2.bitwise_and(cand, mask_border)\n",
    "\n",
    "    # Clean and find components\n",
    "    cand = cv2.morphologyEx(cand, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
    "    cand = cv2.morphologyEx(cand, cv2.MORPH_OPEN,  np.ones((3,3), np.uint8))\n",
    "\n",
    "    min_area = max(40, int(min_area_frac * h * w))\n",
    "    cnts, _ = cv2.findContours(cand, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    details = {\"blobs\": []}\n",
    "    fired = False\n",
    "    for c in cnts:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area < min_area:\n",
    "            continue\n",
    "        x, y, bw, bh = cv2.boundingRect(c)\n",
    "\n",
    "        # which side?\n",
    "        touches_left   = x <= 1\n",
    "        touches_right  = x + bw >= w - 2\n",
    "        touches_top    = y <= 1\n",
    "        touches_bottom = y + bh >= h - 2\n",
    "        side = None\n",
    "        if touches_left:   side = \"left\"\n",
    "        if touches_right:  side = \"right\" if side is None else side + \"+right\"\n",
    "        if touches_top:    side = \"top\" if side is None else side + \"+top\"\n",
    "        if touches_bottom: side = \"bottom\" if side is None else side + \"+bottom\"\n",
    "\n",
    "        # optional: ignore neck (tall, below face, overlaps horizontally with face)\n",
    "        if exclude_neck:\n",
    "            overlap_x = not (x2 < x or x + bw < x1)  # horizontal overlap with face\n",
    "            is_below  = y > y2 - int(0.05 * (y2 - y1))\n",
    "            tall      = bh / (bw + 1e-6) > 1.5\n",
    "            if overlap_x and is_below and tall:\n",
    "                continue\n",
    "\n",
    "        # average chroma distance inside blob (stronger means more face-like)\n",
    "        mask = np.zeros((h, w), np.uint8)\n",
    "        cv2.drawContours(mask, [c], -1, 255, -1)\n",
    "        mean_d2 = float(np.mean(d2[mask > 0])) \n",
    "        sim_score = float(np.clip(1.0 - (mean_d2 / (chi2_thresh + 1e-6)), 0.0, 1.0))\n",
    "\n",
    "        details[\"blobs\"].append({\n",
    "            \"bbox\": [int(x), int(y), int(x + bw), int(y + bh)],\n",
    "            \"area\": float(area),\n",
    "            \"side\": side or \"unknown\",\n",
    "            \"mean_chi2\": mean_d2,\n",
    "            \"similarity\": sim_score\n",
    "        })\n",
    "\n",
    "        # Fire if it's a confident, sizable, border-touching blob\n",
    "        if sim_score >= 0.35:\n",
    "            fired = True\n",
    "\n",
    "    details[\"fired\"] = fired\n",
    "    return fired, details\n",
    "def _edge_color_info(ctx):\n",
    "    key = \"_edge_color_cached\"\n",
    "    if key in ctx:\n",
    "        return ctx[key]\n",
    "    bbox = ctx.get(\"bbox\")\n",
    "    if bbox is None:\n",
    "        ctx[key] = (False, {\"blobs\": [], \"fired\": False})\n",
    "    else:\n",
    "        has, info = detect_similar_skin_blob_outside_bbox(ctx[\"vision_frame\"], np.array(bbox, int))\n",
    "        ctx[key] = (has, info)\n",
    "    return ctx[key]\n",
    "\n",
    "def m_edge_color_partial(ctx):\n",
    "    has, _ = _edge_color_info(ctx)\n",
    "    return (\"edge_color_partial\", 1.0 if has else 0.0)\n",
    "\n",
    "def m_edge_color_max_sim(ctx):\n",
    "    _, info = _edge_color_info(ctx)\n",
    "    mx = max((b.get(\"similarity\", 0.0) for b in info.get(\"blobs\", [])), default=0.0)\n",
    "    return (\"edge_color_max_sim\", float(mx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adamsobieszek/PycharmProjects/_manipy\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'control_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 254\u001b[39m\n\u001b[32m    251\u001b[39m c = []\n\u001b[32m    252\u001b[39m device = \u001b[33m'\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mcontrol_models\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcontrol_models\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m():\n\u001b[32m    256\u001b[39m   \u001b[38;5;66;03m# if not os.path.exists(\"final_models.zip\"):\u001b[39;00m\n\u001b[32m    257\u001b[39m   \u001b[38;5;66;03m#   !gdown 1pPjOd-mx-d-vOw1QR_lpJoJmLAGdkI3W\u001b[39;00m\n\u001b[32m    258\u001b[39m   \u001b[38;5;66;03m#   !unzip final_models.zip\u001b[39;00m\n\u001b[32m    259\u001b[39m   \u001b[38;5;66;03m#   !unzip final_models.zip\u001b[39;00m\n\u001b[32m    260\u001b[39m   control_names = [\u001b[33m'\u001b[39m\u001b[33mtrustworthy\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mdominant\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgender\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwhite\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhappy\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'control_models' is not defined"
     ]
    }
   ],
   "source": [
    "# Minimal interactive/notebook-friendly POC for age-preserving traversal toward w_avg in W-space.\n",
    "# Robust to environments without ipywidgets: if ipywidgets is unavailable,\n",
    "# the code still defines all functions and provides a callable `run_static_demo(...)`.\n",
    "#\n",
    "# Assumptions in *your* notebook runtime (not this sandbox):\n",
    "# - Variables `age_model`, `w_avg`, `G`, and `ws` are defined.\n",
    "# - `G.synthesis(ws, noise_mode=\"const\")` returns images in [-1, 1].\n",
    "# - `ws[:4]` gives four starting latent codes. They may be shaped [4, D] or [4, num_ws, D].\n",
    "\n",
    "\n",
    "final_models_path = \"/Users/adamsobieszek/PycharmProjects/psychGAN/content/\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers\n",
    "# ------------------------------\n",
    "\n",
    "def _infer_device():\n",
    "    \"\"\"Prefer the device of w_avg; else fall back to G's device; else cpu.\"\"\"\n",
    "    dev = None\n",
    "    if isinstance(globals().get(\"w_avg\", None), torch.Tensor):\n",
    "        dev = w_avg.device\n",
    "    if dev is None and \"G\" in globals():\n",
    "        try:\n",
    "            dev = next(G.parameters()).device\n",
    "        except Exception:\n",
    "            dev = torch.device(\"cpu\")\n",
    "    if dev is None:\n",
    "        dev = torch.device(\"cpu\")\n",
    "    return dev\n",
    "\n",
    "def _normalize_ws_shape(ws_in: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Normalize various W shapes to:\n",
    "      - w_single: (B, D) tensor to optimize\n",
    "      - expand_for_gen: callable to expand (B,D) -> (B, num_ws, D) for synthesis\n",
    "      - num_ws inferred from G or default 18.\n",
    "    \"\"\"\n",
    "    assert isinstance(ws_in, torch.Tensor), \"Expected `ws` to be a torch.Tensor\"\n",
    "    # If ws_in is (B, D): treat as single W to be broadcast across layers.\n",
    "    # If ws_in is (B, num_ws, D): take the first as the single W for optimization (common trick).\n",
    "    if ws_in.ndim == 2:\n",
    "        w_single = ws_in\n",
    "    elif ws_in.ndim == 3 and ws_in.shape[1] >= 1:\n",
    "        w_single = ws_in[:, 0, :]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported ws shape. Expected (B,D) or (B,num_ws,D).\")\n",
    "    # Infer num_ws from G if possible\n",
    "    num_ws = int(getattr(getattr(globals().get(\"G\", object), \"synthesis\", object), \"num_ws\", 18))\n",
    "    def expand_for_gen(w_bxd: torch.Tensor):\n",
    "        return w_bxd.unsqueeze(1).repeat(1, num_ws, 1)\n",
    "    return w_single, expand_for_gen, num_ws\n",
    "\n",
    "@torch.no_grad()\n",
    "def _synthesize_images(G, w_bxd: torch.Tensor, expand_for_gen, to_uint8=True):\n",
    "    \"\"\"\n",
    "    Synthesizes a batch of images from a batch of (B, D) W vectors by repeating across layers.\n",
    "    Returns float [0,1] if to_uint8=False, else uint8 (B, H, W, C).\n",
    "    \"\"\"\n",
    "    ws_bxLxd = expand_for_gen(w_bxd)\n",
    "    try:\n",
    "        imgs = G.synthesis(ws_bxLxd, noise_mode=\"const\")\n",
    "    except TypeError:\n",
    "        imgs = G.synthesis(ws_bxLxd)\n",
    "    imgs = torch.clamp((imgs + 1.0) * 0.5, 0.0, 1.0)  # [-1,1] -> [0,1]\n",
    "    if to_uint8:\n",
    "        imgs8 = (imgs.detach().cpu() * 255.0).round().to(torch.uint8)  # (B,C,H,W)\n",
    "        imgs8 = imgs8.permute(0, 2, 3, 1).contiguous()                # (B,H,W,C)\n",
    "        return imgs8\n",
    "    return imgs  # (B,3,H,W) float in [0,1]\n",
    "\n",
    "def _ensure_vector_wavg(w_avg_tensor: torch.Tensor, D: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Ensure w_avg is a (D,) vector. If it's (num_ws, D) or (1, D), reduce to (D,).\n",
    "    \"\"\"\n",
    "    if w_avg_tensor.ndim == 1:\n",
    "        return w_avg_tensor\n",
    "    if w_avg_tensor.ndim == 2:\n",
    "        return w_avg_tensor.mean(dim=0)\n",
    "    raise ValueError(\"Unsupported w_avg shape; expected (D,) or (num_ws,D).\")\n",
    "\n",
    "# ------------------------------\n",
    "# Core optimization (POC)\n",
    "# ------------------------------\n",
    "\n",
    "def optimize_towards_wavg_with_age_penalty(\n",
    "    ws_init: torch.Tensor,\n",
    "    w_avg_vec: torch.Tensor,\n",
    "    age_model,  # generic callable nn.Module\n",
    "    steps: int = 100,\n",
    "    step_size: float = 0.05,\n",
    "    age_weight: float = 10.0,\n",
    "    noise_scale: float = 0.0,\n",
    "    stride: int = 5,\n",
    "    rng_seed: int = 0,\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a simple optimization in W-space:\n",
    "      minimize  L = ||w - w_avg||^2 + age_weight * (age(w) - age_0)^2\n",
    "    Using plain gradient descent on w (parameters of age_model are frozen).\n",
    "\n",
    "    Returns:\n",
    "      snapshots: list of 5 tensors, each (B,D), representing progress along the path.\n",
    "      history: dict with scalars for debugging (loss, age drift, dist to w_avg).\n",
    "    \"\"\"\n",
    "    torch.manual_seed(int(rng_seed))\n",
    "    device = device if device is not None else _infer_device()\n",
    "    device = torch.device(device)\n",
    "\n",
    "    ws = ws_init.to(device).detach().clone()\n",
    "    B, D = ws.shape\n",
    "    w_avg_vec = _ensure_vector_wavg(w_avg_vec.to(device), D)\n",
    "\n",
    "    # Freeze age_model params (if any) while keeping autograd for inputs\n",
    "    try:\n",
    "        if isinstance(age_model, nn.Module):\n",
    "            age_model.eval()\n",
    "            for p in age_model.parameters():\n",
    "                p.requires_grad_(False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    with torch.no_grad():\n",
    "        age0 = age_model(ws).view(B)\n",
    "\n",
    "    snapshots = [ws.detach().clone()]\n",
    "    loss_hist, age_drift_hist, dist_hist = [], [], []\n",
    "\n",
    "    # Ensure enough steps for 5 checkpoints\n",
    "    min_steps = max(1, 4 * stride)\n",
    "    steps = max(int(steps), min_steps)\n",
    "\n",
    "    for t in range(1, steps + 1):\n",
    "        ws = ws.detach().requires_grad_(True)\n",
    "        pred_age = age_model(ws).view(B)\n",
    "\n",
    "        dist_term = ((ws - w_avg_vec.unsqueeze(0)) ** 2).mean()\n",
    "        age_pen   = ((pred_age - age0) ** 2).mean()\n",
    "        loss = dist_term + age_weight * age_pen\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            grad = ws.grad\n",
    "            step = -step_size * grad\n",
    "            if noise_scale > 0.0:\n",
    "                step = step + noise_scale * torch.randn_like(ws)\n",
    "            ws = (ws + step).detach()\n",
    "\n",
    "        # Logs\n",
    "        loss_hist.append(float(loss.detach().cpu()))\n",
    "        with torch.no_grad():\n",
    "            age_drift = float(((pred_age - age0) ** 2).mean().sqrt().detach().cpu())\n",
    "            wdist = float(((ws - w_avg_vec.unsqueeze(0)) ** 2).mean().sqrt().detach().cpu())\n",
    "            age_drift_hist.append(age_drift)\n",
    "            dist_hist.append(wdist)\n",
    "\n",
    "        # Save up to 5 snapshots at multiples of `stride`\n",
    "        if (t % stride == 0) and (len(snapshots) < 5):\n",
    "            snapshots.append(ws.detach().clone())\n",
    "        if len(snapshots) >= 5:\n",
    "            break\n",
    "\n",
    "    history = {\n",
    "        \"loss\": np.array(loss_hist, dtype=np.float32),\n",
    "        \"age_drift_rmse\": np.array(age_drift_hist, dtype=np.float32),\n",
    "        \"w_dist_rmse\": np.array(dist_hist, dtype=np.float32),\n",
    "    }\n",
    "    return snapshots, history\n",
    "\n",
    "# ------------------------------\n",
    "# Visualization\n",
    "# ------------------------------\n",
    "\n",
    "def _render_grid(images_by_col, title_by_col=None):\n",
    "    \"\"\"\n",
    "    images_by_col: list of length 5; each element is a (B,H,W,C) uint8 batch to show in a column.\n",
    "    \"\"\"\n",
    "    cols = len(images_by_col)\n",
    "    assert cols == 5, \"Expecting exactly 5 progress columns.\"\n",
    "    B = images_by_col[0].shape[0]\n",
    "    fig, axes = plt.subplots(B, cols, figsize=(2.8*cols, 2.8*B))\n",
    "    if B == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "    if cols == 1:\n",
    "        axes = np.expand_dims(axes, -1)\n",
    "\n",
    "    for c in range(cols):\n",
    "        imgs = images_by_col[c]\n",
    "        for r in range(B):\n",
    "            ax = axes[r, c]\n",
    "            ax.imshow(imgs[r].numpy())\n",
    "            ax.axis(\"off\")\n",
    "            if title_by_col is not None and r == 0:\n",
    "                ax.set_title(title_by_col[c], fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def _run_demo(stride=5, steps=80, step_size=0.05, noise_scale=0.01, age_weight=10.0, rng_seed=0):\n",
    "    global G, ws, age_model, w_avg, names, codes\n",
    "    device = _infer_device()\n",
    "    assert \"ws\" in globals(), \"Expected a global variable `ws` with latent codes.\"\n",
    "    assert \"G\" in globals(),  \"Expected a global variable `G` (generator).\"\n",
    "    assert \"age_model\" in globals(), \"Expected a global variable `age_model`.\"\n",
    "    assert \"w_avg\" in globals(), \"Expected a global variable `w_avg`.\"\n",
    "\n",
    "    ws4 = ws[:4]\n",
    "    w_single, expand_for_gen, num_ws = _normalize_ws_shape(ws4)\n",
    "\n",
    "    # Prepare w_avg vector\n",
    "    D = w_single.shape[1]\n",
    "    wav = _ensure_vector_wavg(w_avg, D)\n",
    "\n",
    "    # Optimize with snapshots\n",
    "    snaps, hist = optimize_towards_wavg_with_age_penalty(\n",
    "        w_single, wav, age_model,\n",
    "        steps=int(steps),\n",
    "        step_size=float(step_size),\n",
    "        age_weight=float(age_weight),\n",
    "        noise_scale=float(noise_scale),\n",
    "        stride=int(stride),\n",
    "        rng_seed=int(rng_seed),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Synthesize 5 columns (each column is a batch of 4 images)\n",
    "    cols = []\n",
    "    for i, wcol in enumerate(snaps):\n",
    "        imgs = _synthesize_images(G, wcol.to(device), expand_for_gen, to_uint8=True)  # (B,H,W,C) uint8\n",
    "        cols.append(imgs)\n",
    "\n",
    "    titles = [f\"step {i*stride}\" for i in range(5)]\n",
    "    _render_grid(cols, titles)\n",
    "\n",
    "    print(f\"Final loss: {hist['loss'][-1]:.4f} | Age drift RMSE: {hist['age_drift_rmse'][-1]:.4f} | W-dist RMSE: {hist['w_dist_rmse'][-1]:.4f}\")\n",
    "\n",
    "%cd /Users/adamsobieszek/PycharmProjects/_manipy/\n",
    "\n",
    "\n",
    "from manipy.models.flow_models import VectorFieldTransformer, RatingODE\n",
    "from manipy.models.layers import EnsembleRegressor, MeanRegressor\n",
    "from manipy.models.rating_models import AlphaBetaRegressor\n",
    "\n",
    "coord_path = '/Users/adamsobieszek/Downloads/content 2/drive/Shareddrives/[PsychGAN]/age_test_redo_additional/coords/'\n",
    "cpath = '/Users/adamsobieszek/Downloads/content 2/drive/Shareddrives/[PsychGAN]/age_test_redo_additional/coords'\n",
    "c = []\n",
    "device = 'mps'\n",
    "\n",
    "del control_models\n",
    "if not \"control_models\" in dir():\n",
    "  # if not os.path.exists(\"final_models.zip\"):\n",
    "  #   !gdown 1pPjOd-mx-d-vOw1QR_lpJoJmLAGdkI3W\n",
    "  #   !unzip final_models.zip\n",
    "  #   !unzip final_models.zip\n",
    "  control_names = ['trustworthy','dominant', 'gender', 'age', 'white', 'happy']\n",
    "  control_models = [EnsembleRegressor([MeanRegressor(512,1) for _ in range(8)], model_kwargs={}).to(device) for label in control_names]\n",
    "  for m,l in zip(control_models,control_names):\n",
    "    m.load_state_dict(torch.load(f\"{final_models_path}/final_models/ensemble_{l}.pt\", map_location=torch.device(\"mps\")))\n",
    "    m.eval()\n",
    "  ALL_MODELS = {**{l:(m) for l,m in zip(control_names,control_models)}}\n",
    "  \n",
    "\n",
    "age_model = ALL_MODELS['age']\n",
    "names = []\n",
    "for f in os.listdir(cpath):\n",
    "    try:\n",
    "        c.append(torch.load(cpath+\"/\"+f, map_location='cpu')['w'])\n",
    "        names.append(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {f}: {e}\")\n",
    "        print(e)\n",
    "ws = torch.stack(c).to(device)\n",
    "codes = ws\n",
    "w_avg = G.mapping.w_avg.to(device)\n",
    "_widgets_available = False\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    _widgets_available = True\n",
    "except ModuleNotFoundError:\n",
    "    _widgets_available = False\n",
    "\n",
    "if _widgets_available:\n",
    "    stride_slider     = widgets.IntSlider(description=\"Stride\", min=1, max=50, step=1, value=5, continuous_update=False)\n",
    "    steps_slider      = widgets.IntSlider(description=\"Steps\", min=20, max=400, step=5, value=80, continuous_update=False)\n",
    "    step_size_slider  = widgets.FloatSlider(description=\"Step size\", min=0.001, max=0.2, step=0.001, value=0.05, readout_format=\".3f\", continuous_update=False)\n",
    "    noise_slider      = widgets.FloatSlider(description=\"Noise scale\", min=0.0, max=0.1, step=0.001, value=0.01, readout_format=\".3f\", continuous_update=False)\n",
    "    age_w_slider      = widgets.FloatSlider(description=\"Age weight\", min=0.0, max=100.0, step=0.5, value=10.0, readout_format=\".1f\", continuous_update=False)\n",
    "    seed_box          = widgets.IntText(description=\"Seed\", value=0)\n",
    "\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Age-preserving W traversal toward w_avg</h3>\"),\n",
    "        widgets.HBox([stride_slider, steps_slider, step_size_slider]),\n",
    "        widgets.HBox([noise_slider, age_w_slider, seed_box])\n",
    "    ])\n",
    "\n",
    "    out = widgets.interactive_output(\n",
    "        _run_demo,\n",
    "        {\n",
    "            \"stride\": stride_slider,\n",
    "            \"steps\": steps_slider,\n",
    "            \"step_size\": step_size_slider,\n",
    "            \"noise_scale\": noise_slider,\n",
    "            \"age_weight\": age_w_slider,\n",
    "            \"rng_seed\": seed_box\n",
    "        }\n",
    "    )\n",
    "\n",
    "    display(controls, out)\n",
    "else:\n",
    "    print(\n",
    "        \"ipywidgets is not installed in this environment.\\n\"\n",
    "        \"All functions have been defined. In your own notebook with `G`, `ws`, `age_model`, and `w_avg` available,\\n\"\n",
    "        \"either install ipywidgets for the interactive UI, or call this directly:\\n\\n\"\n",
    "        \"  _run_demo(stride=5, steps=80, step_size=0.05, noise_scale=0.01, age_weight=10.0, rng_seed=0)\\n\\n\"\n",
    "        \"Nothing was executed because required globals are not present in this sandbox.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8fc7b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ALL_MODELS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 129\u001b[39m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mupdated\u001b[39m\u001b[33m\"\u001b[39m: updated, \u001b[33m\"\u001b[39m\u001b[33mfailed\u001b[39m\u001b[33m\"\u001b[39m: failed}\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Example usage (with your variables already defined):\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m age_model = \u001b[43mALL_MODELS\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    130\u001b[39m device = _infer_device()\n\u001b[32m    131\u001b[39m summary = optimize_latents_in_directory(\n\u001b[32m    132\u001b[39m     cpath,\n\u001b[32m    133\u001b[39m     age_model=age_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m     overwrite=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    142\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'ALL_MODELS' is not defined"
     ]
    }
   ],
   "source": [
    "import os, shutil, tempfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def _restore_original_shape(w_single_1xd: torch.Tensor, original_w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Take an optimized single-W tensor of shape (1, D) and broadcast it back to the\n",
    "    original shape of `original_w` (either (D,) or (num_ws, D)).\n",
    "    \"\"\"\n",
    "    D = w_single_1xd.shape[-1]\n",
    "    if original_w.ndim == 1:\n",
    "        # (D,)\n",
    "        return w_single_1xd.squeeze(0)\n",
    "    elif original_w.ndim == 2:\n",
    "        # (num_ws, D)\n",
    "        num_ws = original_w.shape[0]\n",
    "        return w_single_1xd.repeat(num_ws, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported original 'w' shape {tuple(original_w.shape)}; expected (D,) or (num_ws, D).\")\n",
    "\n",
    "def optimize_latents_in_directory(\n",
    "    cpath: str,\n",
    "    *,\n",
    "    age_model,\n",
    "    G,  \n",
    "    w_avg: torch.Tensor,\n",
    "    steps: int = 120,\n",
    "    step_size: float = 0.03,\n",
    "    age_weight: float = 8.0,\n",
    "    noise_scale: float = 0.0,\n",
    "    stride: int = 10,\n",
    "    rng_seed: int = 0,\n",
    "    key: str = \"w\",\n",
    "    overwrite: bool = True,\n",
    "    make_backup: bool = True,\n",
    "    backup_ext: str = \".bak\",\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each .pt/.pth in `cpath`, load dict, read `dict[key]` (a W vector of shape (D,) or (num_ws, D)),\n",
    "    run age-preserving optimization toward w_avg, and write the updated dict back.\n",
    "\n",
    "    Returns:\n",
    "        summary dict with 'updated' and 'failed' lists.\n",
    "    \"\"\"\n",
    "    assert os.path.isdir(cpath), f\"Not a directory: {cpath}\"\n",
    "    device = device if device is not None else _infer_device()\n",
    "\n",
    "    # If age_model is a torch Module, move it to the right device once.\n",
    "    if isinstance(age_model, nn.Module):\n",
    "        age_model = age_model.to(device)\n",
    "\n",
    "    files = sorted(\n",
    "        f for f in os.listdir(cpath)\n",
    "        if f.endswith(\".pt\") or f.endswith(\".pth\")\n",
    "    )\n",
    "\n",
    "    updated, failed = [], []\n",
    "    for idx, fname in enumerate(files):\n",
    "        fpath = os.path.join(cpath, fname)\n",
    "        try:\n",
    "            obj = torch.load(fpath, map_location=\"cpu\")\n",
    "            if not isinstance(obj, dict) or key not in obj:\n",
    "                raise KeyError(f\"File does not contain dict['{key}'].\")\n",
    "\n",
    "            original_w = torch.as_tensor(obj[key])  # keep original dtype & shape\n",
    "            D = int(original_w.shape[-1])\n",
    "\n",
    "            # Pick a single W to optimize (as in your POC: first layer if (num_ws, D))\n",
    "            if original_w.ndim == 1:\n",
    "                w0_1xd = original_w.unsqueeze(0)\n",
    "            elif original_w.ndim == 2:\n",
    "                w0_1xd = original_w[:1, :]           # take the first W\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported 'w' shape {tuple(original_w.shape)}; expected (D,) or (num_ws, D).\")\n",
    "\n",
    "            # Prepare vectors on device/dtype\n",
    "            w0_1xd = w0_1xd.to(device=device, dtype=w_avg.dtype)\n",
    "            w_avg_vec = _ensure_vector_wavg(w_avg.to(device), D)\n",
    "\n",
    "            # Run the optimization\n",
    "            snapshots, history = optimize_towards_wavg_with_age_penalty(\n",
    "                ws_init=w0_1xd,\n",
    "                w_avg_vec=w_avg_vec,\n",
    "                age_model=age_model,\n",
    "                steps=steps,\n",
    "                step_size=step_size,\n",
    "                age_weight=age_weight,\n",
    "                noise_scale=noise_scale,\n",
    "                stride=stride,\n",
    "                rng_seed=rng_seed + idx,\n",
    "                device=device,\n",
    "            )\n",
    "            w_opt_1xd = snapshots[-1]              # (1, D) on device\n",
    "            w_opt_1xd = w_opt_1xd.to(dtype=original_w.dtype)\n",
    "\n",
    "            # Restore original shape (D,) or (num_ws, D) and move to CPU for saving\n",
    "            w_opt_shaped = _restore_original_shape(w_opt_1xd, original_w).cpu()\n",
    "            obj[key] = w_opt_shaped\n",
    "\n",
    "            # Optional backup\n",
    "            if make_backup:\n",
    "                shutil.copy2(fpath, fpath + backup_ext)\n",
    "\n",
    "            # Atomic-ish write: save to temp then replace\n",
    "            if overwrite:\n",
    "                with tempfile.NamedTemporaryFile(dir=cpath, delete=False) as tmp:\n",
    "                    tmp_path = tmp.name\n",
    "                try:\n",
    "                    torch.save(obj, tmp_path)\n",
    "                    os.replace(tmp_path, fpath)\n",
    "                finally:\n",
    "                    if os.path.exists(tmp_path):\n",
    "                        os.remove(tmp_path)\n",
    "            else:\n",
    "                out_path = fpath.replace(\".pt\", \"_opt.pt\").replace(\".pth\", \"_opt.pth\")\n",
    "                torch.save(obj, out_path)\n",
    "\n",
    "            updated.append(fname)\n",
    "\n",
    "        except Exception as e:\n",
    "            failed.append((fname, str(e)))\n",
    "\n",
    "    return {\"updated\": updated, \"failed\": failed}\n",
    "\n",
    "# ------------------------------\n",
    "# Example usage (with your variables already defined):\n",
    "# ------------------------------\n",
    "age_model = ALL_MODELS['age']\n",
    "device = _infer_device()\n",
    "summary = optimize_latents_in_directory(\n",
    "    cpath,\n",
    "    age_model=age_model,\n",
    "    G=G,\n",
    "    w_avg=G.mapping.w_avg,\n",
    "    steps=120,\n",
    "    step_size=0.03,\n",
    "    age_weight=1.0,\n",
    "    noise_scale=0.0,\n",
    "    stride=10,\n",
    "    overwrite=True\n",
    ")\n",
    "print(\"Updated:\", summary[\"updated\"])\n",
    "print(\"Failed:\", summary[\"failed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1fc0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f23e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.87662267731248}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8290139891615791}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7313054701218749}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8325940953930552}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8160821933589297}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6682847709474169}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6623810242567226}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6386003101089309}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8751576350431133}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8071472859086701}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.9001246669267948}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8894735498847307}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8145254924015592}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6536221094347152}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7731260710710479}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8644574633838495}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8111913149071011}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8679108501546131}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 1.0, 'edge_left_fire': 1.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7588410380547311}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8960748165420377}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6527160659528942}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7512627719052514}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8199975823027521}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.775594179927673}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 0.0, 'edge_color_max_sim': 0.0}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6720227764880969}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6190206633128896}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8032461226051536}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7617910967699834}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8309981613383052}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8665549903368668}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7894676892767089}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8894777487003517}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7557468155880003}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7702285704011712}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8680854917444003}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 1.0, 'edge_left_fire': 0.0, 'edge_right_fire': 1.0, 'edge_top_fire': 1.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7858156998516206}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7606611782672611}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 1.0, 'edge_left_fire': 1.0, 'edge_right_fire': 0.0, 'edge_top_fire': 1.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.9082371164661363}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8882744701335387}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8371642876461474}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7901896206540138}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8571901480088746}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6988405721228439}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6914605112634344}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8566887670382096}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7373841365573344}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.854360331716989}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8407479834013878}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8854937150815902}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8725806881172214}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.653071495032936}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8841594851247558}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8093165371095978}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8256602083639062}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8286799382784213}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.873521315534646}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6602123426112263}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7794071151212485}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7480171271666625}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8471455346509641}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.882624851128977}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8988299548450556}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8908667884333363}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 1.0, 'edge_left_fire': 1.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8846136989584293}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7020756999541675}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6896403445599542}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7648184030962815}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8841455641303784}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.5308856955530379}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8131506915543556}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8798171812715339}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6176727507802304}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7647218435824742}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7914280593345993}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7863120475417305}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7386542451689917}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.5916936056497664}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8058124201690288}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.9257977952222455}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.8466683399179139}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 1.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 1.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.802165954107076}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6792257188538671}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.6731279524432483}\n",
      "{'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'edge_partial': 0.0, 'edge_left_fire': 0.0, 'edge_right_fire': 0.0, 'edge_top_fire': 0.0, 'edge_bottom_fire': 0.0, 'edge_color_partial': 1.0, 'edge_color_max_sim': 0.7900712061062172}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     16\u001b[39m     metrics = [\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# new edge-partial metrics\u001b[39;00m\n\u001b[32m     18\u001b[39m     m_edge_partial,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# make_edge_score(\"left\"), make_edge_skin_frac(\"left\"), ...\u001b[39;00m\n\u001b[32m     24\u001b[39m ]\n\u001b[32m     26\u001b[39m     vf = visionframe_from_pil(Image.open(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     values = \u001b[43mevaluate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(values)\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# # Example hard reject logic:\u001b[39;00m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# if values.get(\"edge_partial\", 0.0) >= 0.5:\u001b[39;00m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m#     print(\"HARD REJECT: partial face at image edge\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m#     img = Image.open(in_path).convert(\"RGB\")\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m#     img.save(out_path, \"JPEG\", quality=95)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mevaluate_metrics\u001b[39m\u001b[34m(vision_frame, metric_fns, include_meta)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_metrics\u001b[39m(\n\u001b[32m     75\u001b[39m     vision_frame: VisionFrame,\n\u001b[32m     76\u001b[39m     metric_fns: List[MetricFn],\n\u001b[32m     77\u001b[39m     *,\n\u001b[32m     78\u001b[39m     include_meta: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     79\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m     80\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m    Run a list of metric functions on a single image.\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    Each metric_fn(ctx) may return either a float or ('name', float).\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    Returns {name: value}. Adds a few useful meta entries when include_meta=True.\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     ctx = \u001b[43m_build_metric_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     out: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] = {}\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Optional meta so you can filter in tests\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36m_build_metric_context\u001b[39m\u001b[34m(vision_frame)\u001b[39m\n\u001b[32m     27\u001b[39m ctx[\u001b[33m\"\u001b[39m\u001b[33mvision_frame\u001b[39m\u001b[33m\"\u001b[39m] = vision_frame\n\u001b[32m     28\u001b[39m ctx[\u001b[33m\"\u001b[39m\u001b[33mH\u001b[39m\u001b[33m\"\u001b[39m], ctx[\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m] = h, w\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m faces = \u001b[43mget_many_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvision_frame\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m ctx[\u001b[33m\"\u001b[39m\u001b[33mfaces\u001b[39m\u001b[33m\"\u001b[39m] = faces\n\u001b[32m     32\u001b[39m ctx[\u001b[33m\"\u001b[39m\u001b[33mnum_faces\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlen\u001b[39m(faces)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/quality_metrics.py:41\u001b[39m, in \u001b[36mget_many_faces\u001b[39m\u001b[34m(vision_frames)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m face_detector_angle \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0\u001b[39m]:\n\u001b[32m     40\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m face_detector_angle == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \t\tbounding_boxes, face_scores, face_landmarks_5 = \u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     43\u001b[39m \t\tbounding_boxes, face_scores, face_landmarks_5 = detect_faces_by_angle(vision_frame, face_detector_angle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/facefusion/face_detector.py:136\u001b[39m, in \u001b[36mdetect_faces\u001b[39m\u001b[34m(vision_frame)\u001b[39m\n\u001b[32m    133\u001b[39m all_face_landmarks_5 : List[FaceLandmark5] = []\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state_manager.get_item(\u001b[33m'\u001b[39m\u001b[33mface_detector_model\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m [ \u001b[33m'\u001b[39m\u001b[33mmany\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mretinaface\u001b[39m\u001b[33m'\u001b[39m ]:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \tbounding_boxes, face_scores, face_landmarks_5 = \u001b[43mdetect_with_retinaface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_item\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mface_detector_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \tall_bounding_boxes.extend(bounding_boxes)\n\u001b[32m    138\u001b[39m \tall_face_scores.extend(face_scores)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/facefusion/face_detector.py:187\u001b[39m, in \u001b[36mdetect_with_retinaface\u001b[39m\u001b[34m(vision_frame, face_detector_size)\u001b[39m\n\u001b[32m    185\u001b[39m detect_vision_frame = prepare_detect_frame(temp_vision_frame, face_detector_size)\n\u001b[32m    186\u001b[39m detect_vision_frame = normalize_detect_frame(detect_vision_frame, [ -\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m ])\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m detection = \u001b[43mforward_with_retinaface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetect_vision_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, feature_stride \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(feature_strides):\n\u001b[32m    190\u001b[39m \tface_scores_raw = detection[index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/facefusion/face_detector.py:366\u001b[39m, in \u001b[36mforward_with_retinaface\u001b[39m\u001b[34m(detect_vision_frame)\u001b[39m\n\u001b[32m    363\u001b[39m face_detector = get_inference_pool().get(\u001b[33m'\u001b[39m\u001b[33mretinaface\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m thread_semaphore():\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m \tdetection = \u001b[43mface_detector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetect_vision_frame\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m detection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:273\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    271\u001b[39m     output_names = [output.name \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._outputs_meta]\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m C.EPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 3) Run your pipeline\n",
    "from quality_metrics import _crop_from_bbox, _scale01, _has_68, _geom_symmetry_score, _estimate_pose_from_68, _clamp01, _laplacian_var, _centering_score, _occlusion_score\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "target_dir = \"/Users/adamsobieszek/PycharmProjects/_manipy/content/out_filtered5\"\n",
    "out_dir = \"/Users/adamsobieszek/PycharmProjects/_manipy/content/out_filtered6\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "passed =[]\n",
    "Dp = []\n",
    "for im in os.listdir(target_dir):\n",
    "    if im.endswith(\".pt\") or im.startswith(\".\"):\n",
    "        continue\n",
    "\n",
    "    metrics = [\n",
    "    # new edge-partial metrics\n",
    "    m_edge_partial,\n",
    "    make_edge_fire(\"left\"), make_edge_fire(\"right\"),\n",
    "    make_edge_fire(\"top\"),  make_edge_fire(\"bottom\"),\n",
    "      m_edge_color_partial, m_edge_color_max_sim \n",
    "    # (optional debug)\n",
    "    # make_edge_score(\"left\"), make_edge_skin_frac(\"left\"), ...\n",
    "]\n",
    "\n",
    "    vf = visionframe_from_pil(Image.open(f\"{target_dir}/{im}\"))\n",
    "    values = evaluate_metrics(vf, metrics)\n",
    "    print(values)\n",
    "\n",
    "    # # Example hard reject logic:\n",
    "    # if values.get(\"edge_partial\", 0.0) >= 0.5:\n",
    "    #     print(\"HARD REJECT: partial face at image edge\")\n",
    "    #     print(im)\n",
    "    #     metrics = [m_det, m_geom, m_pose, m_sharp, m_exposure, m_center, m_occl,     m_edge_color_partial, m_edge_color_max_sim ] # new color-similarity check]x\n",
    "    #     values = evaluate_metrics(vf, metrics)\n",
    "    #     # values -> {'meta_num_faces': 1.0, 'meta_has_faces': 1.0, 'det': 0.83, ...}\n",
    "    #     print(values)\n",
    "    # else:\n",
    "    #     # Load the .webp image and save as .jpg in out_dir\n",
    "    #     from PIL import Image\n",
    "    #     in_path = f\"{target_dir}/{im}\"\n",
    "    #     out_path = f\"{out_dir}/{os.path.splitext(im)[0]}.jpg\"\n",
    "    #     img = Image.open(in_path).convert(\"RGB\")\n",
    "    #     img.save(out_path, \"JPEG\", quality=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0e9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac2c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['134_f_36', '69_f_36', '210_m_44', '123_m_11', '135_f_36', '33_f_12', '195_f_45', '41_f_19', '48_m_54', '146_f_15', '224_m_52', '128_f_27', '204_f_30', '153_m_49', '94_f_10', '4_m_45', '163_m_55', '203_f_55', '208_f_42', '14_f_34', '28_m_53', '187_f_49', '81_f_45', '7_f_23', '6_f_37', '199_f_7', '163_f_48', '110_f_57', '139_f_50', '162_f_48', '223_f_16', '154_f_25', '138_f_50', '178_f_28', '48_f_49', '167_f_56', '182_f_57', '153_f_40', '26_f_53', '180_f_10', '62_f_21', '1_f_52', '0_f_46', '86_f_20', '181_f_39', '152_f_41', '26_f_52', '166_f_43', '57_f_22', '206_m_57', '48_f_48', '115_f_48', '110_f_42', '64_f_45', '142_m_17', '21_f_23', '186_f_48', '124_f_54', '70_m_19', '208_f_43', '216_f_29', '106_f_44', '97_f_56', '44_f_12', '96_f_56', '192_f_21', '172_f_16', '101_f_35', '180_m_30', '100_f_35', '101_f_21', '30_f_54', '40_f_18', '195_f_50', '194_f_50', '61_m_52', '134_f_23', '170_f_51', '122_m_8', '159_f_40', '158_f_40', '218_f_36', '194_f_52', '144_f_45', '101_f_37', '173_f_28', '121_m_41', '129_f_18', '74_f_18', '188_f_57', '108_m_52', '91_f_25', '47_f_43', '46_f_57', '162_m_56', '127_m_30', '53_m_37', '163_m_56', '72_f_55', '82_f_29', '105_m_20', '209_f_55', '208_f_55', '161_f_18', '44_m_31', '199_f_20', '7_f_20', '21_f_21', '126_f_39', '65_f_47', '12_f_46', '56_f_20', '123_f_27', '183_f_54', '178_f_17', '102_m_45', '181_f_13', '43_m_40', '212_f_35', '75_m_39', '38_f_13', '0_f_50', '62_f_23', '120_f_49', '183_f_55', '106_f_7', '206_m_54', '2_f_17', '20_f_34', '21_f_34', '127_f_10', '52_f_17', '143_m_28', '15_f_22', '200_m_31', '113_f_12', '34_m_54', '221_f_47', '37_f_26', '119_f_39', '127_m_19', '202_f_43', '105_f_28', '193_f_36', '104_f_14', '188_f_56', '74_f_25', '135_f_20', '211_m_52', '69_f_18', '135_f_18', '68_f_30', '159_f_51', '90_f_20', '91_f_20', '74_f_21', '145_f_40', '74_f_35', '201_f_14', '201_f_28', '23_m_56', '118_f_29', '106_f_57', '130_f_12', '14_f_32', '193_m_13', '208_f_50', '141_m_57', '215_f_41', '175_m_55', '150_f_15', '122_f_36', '1_f_40', '213_f_30', '180_f_16', '197_m_25', '26_f_55', '116_f_35', '172_m_19', '153_f_53', '212_f_31', '146_m_33', '182_f_44', '7_f_24', '119_m_35', '146_f_8', '21_f_31', '209_f_45', '162_m_52', '88_f_12', '119_f_28', '193_f_33', '59_f_11', '161_m_15', '62_m_13', '99_m_50', '145_f_55', '90_f_21', '109_m_56', '205_f_23', '189_f_47', '171_f_43', '69_f_25', '41_f_36', '134_f_19', '146_f_12', '122_m_28', '33_f_17', '170_f_41', '147_f_38', '218_f_18', '9_f_13', '129_f_36', '74_f_22', '99_m_52', '31_f_50', '136_f_48', '133_f_56', '23_m_41', '80_m_49', '126_m_22', '96_f_52', '51_f_43', '22_f_48', '160_f_22', '113_f_15', '220_f_54', '149_f_19', '175_m_56', '214_f_42', '81_f_54', '210_f_48', '123_f_21', '57_f_26', '167_f_53', '205_m_14', '204_m_14', '27_f_56', '39_f_29', '109_f_48', '26_f_57', '62_f_19', '86_f_24', '207_m_46', '123_f_20', '178_f_10', '214_f_43', '64_f_40', '139_f_54', '151_m_8', '221_f_41', '14_f_18', '220_f_55', '217_f_38', '209_f_46', '100_m_7', '113_f_14', '34_m_46', '148_f_30', '217_f_10', '201_m_23', '148_f_24', '50_f_56', '118_f_17', '126_m_23', '142_f_33', '177_f_19', '46_f_44', '4_m_55', '91_f_36', '90_f_22', '68_f_32', '9_m_8', '134_f_32', '41_f_21', '41_f_46', '61_m_30', '56_m_49', '194_f_32', '115_m_37', '219_f_56', '1_m_38', '180_m_52', '121_m_35', '54_m_32', '136_f_12', '35_f_14', '176_f_42', '175_f_39', '221_f_26', '71_m_47', '209_f_35', '67_f_48', '66_f_48', '5_f_13', '191_m_19', '154_f_52', '111_f_20', '138_f_33', '110_f_20', '166_f_35', '102_m_19', '211_f_12', '87_f_43', '1_f_30', '145_m_11', '62_f_43', '63_f_43', '26_f_31', '77_m_23', '167_f_34', '85_f_11', '21_f_40', '214_f_19', '7_f_41', '162_f_16', '67_f_49', '185_f_50', '209_f_34', '23_f_13', '157_f_14', '17_m_18', '28_f_38', '175_f_38', '191_f_39', '16_m_24', '17_m_30', '177_f_57', '132_f_25', '133_f_31', '168_f_29', '4_m_33', '105_f_48', '91_f_50', '144_f_18', '109_m_27', '129_f_45', '101_f_56', '31_f_23', '129_f_51', '77_f_16', '182_m_14', '159_f_21', '170_f_26', '207_f_15', '103_f_39', '210_m_26', '158_f_21', '207_f_17', '211_m_30', '179_m_55', '48_m_20', '151_m_52', '150_m_46', '48_m_34', '18_f_32', '75_f_47', '136_f_39', '11_m_43', '192_f_40', '141_f_10', '125_m_14', '4_m_25', '132_f_33', '5_m_25', '127_m_47', '162_m_21', '142_f_57', '73_f_22', '208_f_22', '221_f_31', '34_m_22', '209_f_22', '23_f_39', '64_f_18', '198_f_43', '187_f_29', '110_f_23', '21_f_56', '154_f_45', '138_f_30', '122_f_44', '37_m_8', '100_f_8', '27_f_33', '87_f_54', '121_f_17', '212_f_42', '30_m_29', '47_f_7', '144_m_12', '26_f_32', '117_f_53', '63_f_54', '99_f_17', '56_f_56', '207_m_37', '166_f_23', '187_f_14', '139_f_31', '138_f_31', '199_f_56', '81_f_30', '44_m_53', '133_m_13', '47_m_14', '161_f_52', '221_f_18', '184_f_53', '176_m_49', '191_f_12', '138_m_10', '17_m_27', '22_m_19', '46_f_35', '176_f_54', '133_f_32', '221_m_39', '208_m_16', '1_m_12', '224_m_33', '128_f_46', '180_m_44', '178_m_54', '195_f_24', '13_m_39', '61_m_36', '218_f_50', '24_m_44', '68_f_53', '92_f_10', '158_f_32', '158_f_26', '134_f_53', '135_f_47', '85_m_37', '41_f_40', '10_m_52', '189_f_31', '100_f_45', '181_m_54', '216_m_44', '133_f_36', '94_f_49', '169_f_12', '143_f_46', '222_m_52', '191_f_16', '50_f_23', '148_f_51', '125_f_24', '201_m_42', '22_f_14', '104_m_52', '70_m_55', '184_f_57', '208_f_33', '119_m_57', '44_m_57', '6_f_52', '162_f_11', '7_f_46', '7_f_52', '102_m_37', '77_m_30', '87_f_51', '213_f_47', '86_f_51', '116_f_43', '117_f_57', '26_f_36', '62_f_50', '210_f_29', '49_f_11', '168_m_32', '22_f_15', '185_f_56', '125_f_31', '78_f_31', '127_m_57', '96_f_27', '97_f_33', '191_f_17', '4_f_8', '142_f_53', '107_f_21', '190_f_17', '66_m_46', '192_f_44', '101_f_44', '129_f_57', '189_f_30', '93_f_11', '41_f_41', '170_f_20', '158_f_33', '77_f_38', '219_f_53', '167_m_11', '85_m_20', '99_m_31', '133_f_21', '192_f_52', '168_f_11', '23_m_22', '67_m_44', '73_f_24', '37_f_42', '143_f_51', '96_f_19', '96_f_31', '162_m_33', '202_f_33', '79_f_33', '5_f_16', '140_m_23', '23_f_17', '208_f_24', '111_f_25', '139_f_22', '198_f_45', '163_f_12', '72_m_39', '214_f_35', '85_f_29', '48_f_13', '60_f_14', '167_f_24', '207_m_30', '211_f_17', '87_f_46', '213_f_50', '99_f_38', '173_m_50', '120_f_11', '152_f_32', '9_m_50', '86_f_47', '206_m_25', '182_f_18', '122_f_43', '166_f_31', '206_m_31', '61_f_29', '76_m_26', '16_f_14', '215_f_20', '80_f_22', '119_m_55', '7_f_50', '155_f_56', '137_f_8', '220_f_36', '35_m_31', '200_m_54', '169_m_19', '160_f_54', '209_f_19', '73_f_25', '139_m_16', '97_f_18', '50_m_28', '177_f_52', '133_f_20', '23_m_7', '47_f_27', '144_f_21', '98_m_30', '109_m_36', '188_f_27', '32_f_49', '167_m_10', '93_f_12', '56_m_54', '103_f_19', '194_f_13', '19_f_10', '8_f_40', '108_m_13', '1_m_25', '185_m_45', '132_f_39', '79_m_36', '200_f_50', '79_m_22', '133_f_11', '175_f_24', '198_m_54', '143_f_49', '175_f_18', '113_f_52', '157_f_20', '112_f_52', '58_m_49', '132_m_30', '124_f_17', '64_f_12', '186_f_23', '138_f_12', '215_f_39', '166_f_14', '68_m_55', '206_m_28', '114_f_23', '57_f_49', '172_m_48', '39_f_47', '39_f_52', '90_m_50', '27_f_38', '180_f_47', '213_f_49', '181_f_53', '115_f_22', '167_f_15', '171_m_26', '130_m_7', '7_f_48', '29_m_10', '127_f_45', '34_m_29', '186_m_17', '97_f_14', '174_f_25', '148_m_56', '161_m_51', '58_f_41', '168_f_34', '35_f_34', '169_f_20', '48_m_8', '42_f_35', '153_m_22', '92_f_22', '159_f_8', '146_f_54', '60_m_12', '146_f_40', '137_f_18', '173_f_56', '168_f_22', '47_f_29', '140_f_25', '70_f_44', '35_f_36', '164_m_7', '118_f_46', '130_f_55', '15_f_49', '192_f_8', '124_f_14', '4_f_31', '127_f_47', '126_f_53', '163_f_21', '163_f_35', '215_f_12', '127_f_53', '223_f_43', '186_f_20', '69_m_42', '49_f_34', '60_f_27', '108_f_19', '145_m_26', '180_f_45', '153_f_29', '1_f_13', '152_f_29', '91_m_47', '10_f_42', '54_f_30', '165_f_45', '181_f_50', '144_m_33', '128_m_46', '128_m_52', '180_f_50', '98_f_22', '224_f_27', '115_f_35', '182_f_17', '76_m_15', '114_f_35', '166_f_16', '171_m_25', '41_m_50', '207_m_16', '222_f_42', '53_f_55', '162_f_34', '50_f_12', '83_f_56', '176_f_8', '113_f_50', '208_f_16', '148_f_48', '15_f_48', '190_f_27', '21_m_57', '20_m_43', '44_f_47', '16_m_12', '97_f_17', '176_f_49', '168_f_23', '0_m_33', '9_f_56', '8_f_42', '60_m_13', '122_m_51', '195_f_15', '33_f_42', '123_m_55', '76_f_18', '54_m_15', '124_m_8', '196_f_46', '19_f_16', '153_m_25', '124_m_24', '112_m_49', '51_m_23', '104_f_52', '70_f_55', '47_f_38', '141_f_34', '168_f_33', '89_f_51', '191_f_37', '155_m_40', '162_m_11', '28_f_36', '83_f_46', '112_f_54', '221_f_15', '222_f_46', '203_m_18', '142_m_52', '223_f_46', '64_f_28', '126_f_42', '114_f_31', '48_f_25', '61_f_36', '211_f_21', '39_f_55', '181_f_40', '101_m_51', '98_f_32', '18_m_36', '153_f_11', '121_f_32', '1_f_17', '121_f_26', '3_f_44', '177_f_8', '178_f_45', '150_f_56', '80_f_14', '111_f_12', '112_f_41', '113_f_41', '209_f_13', '157_f_27', '216_f_51', '202_f_10', '119_f_56', '45_f_42', '21_m_52', '168_f_26', '94_f_41', '196_f_47', '205_f_49', '159_f_12', '57_m_47', '41_f_48', '76_f_31', '102_f_20', '33_f_55', '76_f_27', '60_m_14', '183_m_31', '170_f_17', '158_f_10', '18_f_29', '43_f_25', '26_m_21', '153_m_32', '90_f_7', '168_f_18', '14_m_46', '104_f_45', '168_f_30', '208_m_24', '140_f_37', '71_f_42', '191_f_34', '16_m_29', '131_f_53', '82_f_45', '83_f_51', '186_f_32', '202_m_27', '80_f_16', '215_f_14', '150_f_40', '115_f_32', '150_f_54', '49_f_32', '183_f_10', '171_m_36', '11_f_51', '39_f_56', '100_m_46', '128_m_40', '11_f_44', '164_f_57', '2_f_47', '68_m_51', '211_f_37', '194_m_22', '52_f_53', '106_m_22', '223_f_44', '142_m_50', '45_m_48', '156_f_30', '176_m_46', '50_f_14', '7_m_44', '119_f_55', '119_f_41', '126_m_49', '187_m_12', '132_f_29', '208_m_19', '116_m_41', '224_m_14', '9_f_50', '19_f_14', '172_f_51', '137_f_23', '158_f_11', '76_f_32', '147_f_53', '167_m_19', '134_f_17', '41_f_10', '179_m_14', '32_f_33', '77_f_41', '74_f_12', '169_f_7', '63_m_35', '75_f_12', '34_f_42', '201_f_27', '144_m_8', '202_f_48', '111_m_57', '184_f_13', '66_f_36', '149_f_15', '217_f_35', '216_f_21', '127_f_27', '154_f_38', '6_f_16', '223_f_37', '25_f_35', '24_f_21', '32_m_12', '25_f_21', '129_m_33', '224_f_52', '213_f_17', '109_f_45', '151_f_27', '158_m_56', '183_m_7', '222_f_36', '126_f_32', '20_m_8', '16_f_47', '217_f_20', '125_f_49', '23_f_45', '22_f_51', '149_f_28', '209_m_57', '213_f_8', '209_m_43', '86_m_21', '172_f_23', '136_f_45', '128_f_13', '8_f_36', '38_m_38', '101_f_14', '100_f_14', '9_f_36', '219_f_15', '33_f_32', '182_m_56', '219_f_29', '12_m_50', '134_f_14', '32_f_18', '208_m_7', '134_f_28', '178_m_17', '26_m_44', '63_m_36', '26_m_50', '74_f_11', '104_f_34', '125_m_42', '94_f_26', '118_f_31', '89_f_23', '110_m_54', '64_m_53', '35_m_7', '92_f_7', '161_f_11', '217_f_36', '131_f_8', '202_m_42', '6_f_29', '186_f_57', '187_f_43', '20_f_14', '134_m_21', '171_m_53', '84_f_45', '100_m_23', '98_f_40', '116_f_10', '120_f_55', '38_f_33', '165_f_32', '188_m_56', '213_f_29', '172_m_29', '150_f_24', '179_f_37', '84_f_44', '159_m_55', '89_m_17', '194_f_7', '199_f_14', '141_m_7', '217_f_23', '200_m_10', '132_m_45', '82_f_35', '133_m_51', '7_f_7', '119_f_18', '44_f_18', '36_f_13', '139_m_46', '29_f_45', '72_f_49', '119_f_24', '130_f_37', '45_f_24', '131_f_23', '94_f_27', '173_f_34', '172_f_20', '8_f_21', '100_f_17', '153_m_42', '103_f_50', '146_f_22', '158_f_48', '135_f_15', '207_f_54', '218_f_12', '173_f_24', '172_f_30', '1_m_40', '8_f_31', '196_f_25', '8_f_25', '58_f_7', '201_f_21', '220_m_57', '71_f_22', '59_f_25', '65_m_56', '142_f_10', '45_f_34', '67_f_18', '15_f_13', '4_f_43', '4_f_57', '82_f_25', '149_f_13', '200_m_28', '186_f_52', '143_m_25', '106_m_57', '202_m_53', '163_f_53', '61_f_41', '211_f_56', '85_f_54', '210_f_56', '103_m_49', '39_f_36', '224_f_40', '1_f_49', '185_f_7', '180_f_36', '91_m_21', '205_m_37', '17_m_9', '26_f_49', '8_m_39', '165_f_37', '121_f_45', '158_m_44', '103_m_48', '210_f_57', '219_m_32', '126_f_20', '162_f_52', '17_f_41', '45_m_28', '215_f_49', '73_m_51', '36_m_23', '132_m_54', '15_f_12', '222_m_39', '142_f_39', '88_f_33', '190_f_55', '152_m_47', '172_f_31', '91_f_14', '207_f_45', '13_m_56', '93_f_47', '103_f_55', '40_f_29', '41_f_29', '146_f_31', '24_m_39', '194_f_49', '207_f_53', '3_m_10', '92_f_51', '204_f_14', '55_m_49', '153_m_45', '197_f_32', '108_m_9', '200_f_22', '71_f_21', '110_m_52', '178_f_7', '198_m_26', '130_f_30', '107_f_49', '221_f_49', '216_f_24', '156_f_52', '20_f_12', '16_f_43', '162_f_44', '138_f_48', '194_m_40', '159_m_52', '3_f_31', '178_f_18', '159_m_46', '200_m_7', '171_m_55', '10_f_26', '91_m_37', '212_f_12', '101_m_25', '99_f_46', '116_f_16', '137_m_48', '62_f_11', '98_f_46', '63_f_10', '98_f_53', '197_m_12', '62_f_38', '54_f_55', '158_m_47', '56_f_12', '84_f_42', '159_m_53', '115_f_44', '13_f_48', '179_f_31', '151_f_36', '52_f_30', '53_f_18', '222_f_33', '223_f_27', '14_f_39', '82_f_33', '83_f_27', '160_f_16', '82_f_27', '81_m_41', '110_m_47', '191_f_42', '58_f_27', '104_f_27', '51_m_42', '79_m_51', '129_f_16', '41_f_14', '212_f_8', '92_f_50', '122_m_34', '12_m_55', '40_f_28', '86_f_8']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print([m.split('.')[0] for m in os.listdir(\"/Users/adamsobieszek/PycharmProjects/_manipy/content/out_filtered6\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8790e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024, 1024])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 3, 1, 224, 224]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m img_bchw = G(sample_w(\u001b[32m1\u001b[39m, G=G), \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# or your already-rendered tensor\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(img_bchw.shape)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m res = \u001b[43mfeature_sensitivity_at\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_bchw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_bchw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenter_xy\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# pixels; set coords_mode=\"normaliz    ed\" if using [-1,1]\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextractor_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresnet50\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# or \"vgg16\", \"lpips\", \"clip-vitb32\" if installed\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoords_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpixels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_px\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_res\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ml2_half\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgrad (âˆ‚g/âˆ‚x, âˆ‚g/âˆ‚y):\u001b[39m\u001b[33m\"\u001b[39m, res.grad_xy.cpu().numpy())\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33munit dir (fastest change):\u001b[39m\u001b[33m\"\u001b[39m, res.unit_dir.cpu().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/sg_opt/sg_output_analysis.py:328\u001b[39m, in \u001b[36mfeature_sensitivity_at\u001b[39m\u001b[34m(image_bchw, center_xy, extractor_name, coords_mode, patch_px, out_res, reduction)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m g, feats, patch\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Autograd path (now MPS-safe via custom bilinear sampler)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m g, fvec, patch = \u001b[43menergy_from_xy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m grad = torch.autograd.grad(g, xy, create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_scalar_fn\u001b[39m(xy_in: torch.Tensor) -> torch.Tensor:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/sg_opt/sg_output_analysis.py:315\u001b[39m, in \u001b[36mfeature_sensitivity_at.<locals>.energy_from_xy\u001b[39m\u001b[34m(xy_in)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34menergy_from_xy\u001b[39m(xy_in: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     feats, patch = \u001b[43mcoords_to_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxy_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m     feats = feats.view(-\u001b[32m1\u001b[39m)\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m reduction == \u001b[33m\"\u001b[39m\u001b[33ml2_half\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/sg_opt/sg_output_analysis.py:256\u001b[39m, in \u001b[36mcoords_to_features\u001b[39m\u001b[34m(image_bchw, center_xy, cfg, extractor)\u001b[39m\n\u001b[32m    254\u001b[39m     extractor = build_feature_extractor(cfg.extractor_name, device=device)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.set_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     feats = \u001b[43mextractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (1,D)\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m feats, patch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/_manipy/deixis/face_utils/sg_opt/sg_output_analysis.py:30\u001b[39m, in \u001b[36mFeatureExtractor.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m     32\u001b[39m         y = y[-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/manip311/lib/python3.11/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 3, 1, 224, 224]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sg_opt.sg_output_analysis import feature_sensitivity_at\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append('/Users/adamsobieszek/PycharmProjects/_manipy/')\n",
    "from manipy.stylegan.utils import sample_w\n",
    "device = torch.device(\"mps\")\n",
    "# --- 2. Load Models ---\n",
    "sys.path.append('/Users/adamsobieszek/PycharmProjects/psychGAN/content/psychGAN/stylegan3')\n",
    "with open('/Users/adamsobieszek/PycharmProjects/psychGAN/stylegan2-ffhq-1024x1024.pkl', 'rb') as fp:\n",
    "    G = pickle.load(fp)['G_ema'].to(device)\n",
    "    G.eval()\n",
    "with open('/Users/adamsobieszek/PycharmProjects/psychGAN/stylegan2-ffhq-1024x1024.pkl', 'rb') as fp:\n",
    "    D = pickle.load(fp)['D'].to(device)\n",
    "    D.eval()\n",
    "# img_bchw: (1,3,H,W), RGB in [0,1] from your StyleGAN pipeline (no need to convert to BGR)\n",
    "img_bchw = G(sample_w(1, G=G), None)  # or your already-rendered tensor\n",
    "print(img_bchw.shape)\n",
    "res = feature_sensitivity_at(\n",
    "    image_bchw=img_bchw,\n",
    "    center_xy=(256, 256),            # pixels; set coords_mode=\"normaliz    ed\" if using [-1,1]\n",
    "    extractor_name=\"resnet50\",       # or \"vgg16\", \"lpips\", \"clip-vitb32\" if installed\n",
    "    coords_mode=\"pixels\",\n",
    "    patch_px=128,\n",
    "    out_res=224,\n",
    "    reduction=\"l2_half\"\n",
    ")\n",
    "\n",
    "print(\"grad (âˆ‚g/âˆ‚x, âˆ‚g/âˆ‚y):\", res.grad_xy.cpu().numpy())\n",
    "print(\"unit dir (fastest change):\", res.unit_dir.cpu().numpy())\n",
    "print(\"Hessian:\\n\", res.hess_xy.cpu().numpy())\n",
    "print(\"eigvals:\", res.eigvals.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52083e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set coords_mode=\"normaliz    ed\" if using [-1,1]\n",
    "    extractor_name=\"resnet50\",       # or \"vgg16\", \"lpips\", \"clip-vitb32\" if installed\n",
    "    coords_mode=\"pixels\",\n",
    "    patch_px=128,\n",
    "    out_res=224,\n",
    "    reduction=\"l2_half\"\n",
    ")\n",
    "\n",
    "print(\"grad (âˆ‚g/âˆ‚x, âˆ‚g/âˆ‚y):\", res.grad_xy.cpu().numpy())\n",
    "print(\"unit dir (fastest change):\", res.unit_dir.cpu().numpy())\n",
    "print(\"Hessian:\\n\", res.hess_xy.cpu().numpy())\n",
    "print(\"eigvals:\", res.eigvals.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8875055",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_partial_face_edges' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     53\u001b[39m     metrics = [\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# new edge-partial metrics\u001b[39;00m\n\u001b[32m     55\u001b[39m     m_edge_partial,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# make_edge_score(\"left\"), make_edge_skin_frac(\"left\"), ...\u001b[39;00m\n\u001b[32m     61\u001b[39m ]\n\u001b[32m     63\u001b[39m     vf = visionframe_from_pil(Image.open(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     values = \u001b[43mevaluate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     reject, score, subs, reasons = should_reject(vf)\n\u001b[32m     66\u001b[39m     passed.append(reject)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mevaluate_metrics\u001b[39m\u001b[34m(vision_frame, metric_fns, include_meta)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m metric_fns:\n\u001b[32m     94\u001b[39m     name = \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmetric\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     val = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) == \u001b[32m2\u001b[39m:\n\u001b[32m     97\u001b[39m         name, score = val  \u001b[38;5;66;03m# ('custom_name', value)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mm_edge_partial\u001b[39m\u001b[34m(ctx)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mm_edge_partial\u001b[39m(ctx):\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     has, _ = \u001b[43m_edge_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33medge_partial\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36m_edge_info\u001b[39m\u001b[34m(ctx)\u001b[39m\n\u001b[32m    156\u001b[39m bbox = ctx.get(\u001b[33m\"\u001b[39m\u001b[33mbbox\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    157\u001b[39m main_bbox = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m bbox \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np.array(bbox, dtype=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m has_partial, details = \u001b[43mdetect_partial_face_edges\u001b[49m(\n\u001b[32m    159\u001b[39m     ctx[\u001b[33m\"\u001b[39m\u001b[33mvision_frame\u001b[39m\u001b[33m\"\u001b[39m], main_bbox=main_bbox, debug=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    160\u001b[39m )\n\u001b[32m    161\u001b[39m ctx[key] = (has_partial, details)  \u001b[38;5;66;03m# cache for sibling metrics\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ctx[key]\n",
      "\u001b[31mNameError\u001b[39m: name 'detect_partial_face_edges' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# torch >=1.10, stylegan2-ada-pytorch discriminator D\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def d_realness_score_bgr_uint8(img_bgr: VisionFrame, D, device=\"mps\", n_tta=5):\n",
    "    \"\"\"\n",
    "    img_bgr: HxWx3 uint8, BGR. Returns avg sigmoid(logit) in [0,1].\n",
    "    \"\"\"\n",
    "    # to RGB, [-1,1], NCHW, and resize to D's resolution\n",
    "    rgb = img_bgr[..., ::-1].astype('float32') / 127.5 - 1.0\n",
    "    x = torch.from_numpy(rgb).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "    # D may expect a specific res (e.g., 1024). Use D.img_resolution\n",
    "    res = getattr(D, 'img_resolution', x.shape[-1])\n",
    "    x = F.interpolate(x, size=(res, res), mode='bicubic', align_corners=False)\n",
    "\n",
    "    scores = []\n",
    "    for i in range(n_tta):\n",
    "        z = x\n",
    "        if i % 2 == 1:\n",
    "            z = torch.flip(z, dims=[3])  # horizontal flip\n",
    "        # mild crop-jitter\n",
    "        pad = int(0.03 * res)\n",
    "        if pad > 0:\n",
    "            off = torch.randint(0, pad+1, ()).item()\n",
    "            z = z[..., off:res-off, off:res-off]\n",
    "            z = F.interpolate(z, size=(res, res), mode='bilinear', align_corners=False)\n",
    "        out = D(z, None)  # StyleGAN2-ADA API: D(image, c)\n",
    "        scores.append(torch.sigmoid(out).mean().item())\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "\n",
    "\n",
    "# 3) Run your pipeline\n",
    "from quality_metrics import _crop_from_bbox, _scale01, _has_68, _geom_symmetry_score, _estimate_pose_from_68, _clamp01, _laplacian_var, _centering_score, _occlusion_score\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "target_dir = \"/Users/adamsobieszek/PycharmProjects/_manipy/selected_age_additional3\"\n",
    "out_dir = \"/Users/adamsobieszek/PycharmProjects/_manipy/selected_age_additional3/rejected\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "passed =[]\n",
    "Dp = []\n",
    "Dp_dict={}\n",
    "for im in os.listdir(target_dir):\n",
    "    if im.endswith(\".pt\") or im.startswith(\".\"):\n",
    "        continue\n",
    "\n",
    "\n",
    "    metrics = [\n",
    "    # new edge-partial metrics\n",
    "    m_edge_partial,\n",
    "    make_edge_fire(\"left\"), make_edge_fire(\"right\"),\n",
    "    make_edge_fire(\"top\"),  make_edge_fire(\"bottom\"),\n",
    "      m_edge_color_partial, m_edge_color_max_sim \n",
    "    # (optional debug)\n",
    "    # make_edge_score(\"left\"), make_edge_skin_frac(\"left\"), ...\n",
    "]\n",
    "\n",
    "    vf = visionframe_from_pil(Image.open(f\"{target_dir}/{im}\"))\n",
    "    values = evaluate_metrics(vf, metrics)\n",
    "    reject, score, subs, reasons = should_reject(vf)\n",
    "    passed.append(reject)\n",
    "    # dp = d_realness_score_bgr_uint8(vf, D, device=\"mps\", n_tta=5)\n",
    "    # Dp.append(dp)\n",
    "    # Dp_dict[im] = dp\n",
    "    # # Example hard reject logic:\n",
    "    if values.get(\"edge_partial\", 0.0) >= 0.5 or reject:\n",
    "        print(\"HARD REJECT: partial face at image edge\")\n",
    "        # move the file to out_dir\n",
    "        shutil.move(f\"{target_dir}/{im}\", f\"{out_dir}/{im}\")\n",
    "    else:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179_f_37.jpg\n"
     ]
    }
   ],
   "source": [
    "np.argmax(Dp)\n",
    "for i,im in enumerate(os.listdir(target_dir)    ):\n",
    "    if im.endswith(\".pt\") or im.startswith(\".\"):\n",
    "        continue\n",
    "    if i ==946:\n",
    "        print(im)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e683d694",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 349\u001b[39m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SensitivityResult(\n\u001b[32m    339\u001b[39m         feats=fvec.detach(),\n\u001b[32m    340\u001b[39m         energy=g.detach(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    346\u001b[39m         patch=patch.detach()\n\u001b[32m    347\u001b[39m     )\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# img_bchw: (1,3,H,W), RGB in [0,1] from your StyleGAN pipeline (no need to convert to BGR)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m img_bchw = \u001b[43mG\u001b[49m(sample_w(\u001b[32m1\u001b[39m, G=G), \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# or your already-rendered tensor\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;28mprint\u001b[39m(img_bchw.shape)\n\u001b[32m    351\u001b[39m res = feature_sensitivity_at(\n\u001b[32m    352\u001b[39m     image_bchw=img_bchw,\n\u001b[32m    353\u001b[39m     center_xy=(\u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m),            \u001b[38;5;66;03m# pixels; set coords_mode=\"normalized\" if using [-1,1]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    358\u001b[39m     reduction=\u001b[33m\"\u001b[39m\u001b[33ml2_half\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    359\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "# sg_output_analysis.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional, Literal, Callable\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================\n",
    "# Feature extractors\n",
    "# ============================\n",
    "\n",
    "class _IdentityPool(nn.Module):\n",
    "    def forward(self, x): return x\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a backbone and produces a single flat feature vector per image.\n",
    "    Expect input: float tensor in [0,1], shape (N,3,H,W), RGB.\n",
    "    \"\"\"\n",
    "    def __init__(self, trunk: nn.Module, pool: nn.Module | None = None, proj: nn.Module | None = None):\n",
    "        super().__init__()\n",
    "        self.trunk = trunk\n",
    "        self.pool = pool if pool is not None else _IdentityPool()\n",
    "        self.proj = proj if proj is not None else _IdentityPool()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.trunk(x)\n",
    "        if isinstance(y, (list, tuple)):\n",
    "            y = y[-1]\n",
    "        # if feature map, global-average pool\n",
    "        if y.ndim == 4:\n",
    "            y = torch.flatten(F.adaptive_avg_pool2d(y, 1), 1)\n",
    "        y = self.pool(y)\n",
    "        y = self.proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "def build_feature_extractor(name: str, device: torch.device | str = \"cpu\") -> FeatureExtractor:\n",
    "    \"\"\"\n",
    "    name âˆˆ {\"vgg16\",\"resnet50\",\"lpips\",\"clip-vitb32\", ...}\n",
    "    (LPIPS and CLIP are optional; we fall back gracefully if not installed.)\n",
    "    \"\"\"\n",
    "    n = name.lower()\n",
    "\n",
    "    if n == \"vgg16\":\n",
    "        from torchvision.models import vgg16, VGG16_Weights\n",
    "        m = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:23]  # conv3_3\n",
    "        return FeatureExtractor(nn.Sequential(m)).to(device).eval()\n",
    "\n",
    "    if n == \"resnet50\":\n",
    "        from torchvision.models import resnet50, ResNet50_Weights\n",
    "        m = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        trunk = nn.Sequential(*(list(m.children())[:-1]))  # â†’ (N,2048,1,1)\n",
    "        proj = nn.Flatten(1)\n",
    "        return FeatureExtractor(trunk, proj=proj).to(device).eval()\n",
    "\n",
    "    if n == \"lpips\":\n",
    "        try:\n",
    "            import lpips  # pip install lpips\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"LPIPS not available; pip install lpips\") from e\n",
    "        net = lpips.LPIPS(net='vgg')  # returns (N,1,1,1) if called on image pairs\n",
    "        # Wrap to behave like a feature extractor on single images:\n",
    "        class LPIPSFeat(nn.Module):\n",
    "            def __init__(self, net):\n",
    "                super().__init__()\n",
    "                self.net = net\n",
    "            def forward(self, x):\n",
    "                # Compare to black image to get a (pseudo) embedding; not a true embedding but useful for sensitivity\n",
    "                z = torch.zeros_like(x)\n",
    "                d = self.net(x*2-1, z*2-1)  # LPIPS expects [-1,1]\n",
    "                return d.view(x.shape[0], -1)\n",
    "        return FeatureExtractor(LPIPSFeat(net)).to(device).eval()\n",
    "\n",
    "    if n in {\"clip\", \"clip-vitb32\"}:\n",
    "        try:\n",
    "            import clip  # pip install git+https://github.com/openai/CLIP.git\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"CLIP not available; install openai-clip\") from e\n",
    "        model, _ = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "        class CLIPImageFeat(nn.Module):\n",
    "            def __init__(self, m): super().__init__(); self.m = m\n",
    "            def forward(self, x):\n",
    "                # CLIP expects normalized [-1,1] w/ mean/std; we accept [0,1] and convert\n",
    "                mean = torch.tensor([0.48145466,0.4578275,0.40821073], device=x.device)[None,:,None,None]\n",
    "                std  = torch.tensor([0.26862954,0.26130258,0.27577711], device=x.device)[None,:,None,None]\n",
    "                x_n = (x - mean) / std\n",
    "                return self.m.encode_image(x_n).float()\n",
    "        return FeatureExtractor(CLIPImageFeat(model)).to(device).eval()\n",
    "\n",
    "    raise ValueError(f\"Unknown feature extractor: {name}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Differentiable coords â†’ patch â†’ features\n",
    "# ============================\n",
    "\n",
    "@dataclass\n",
    "class CoordsToFeaturesConfig:\n",
    "    extractor_name: str = \"resnet50\"\n",
    "    out_res: int = 224            # feature extractor input resolution (square)\n",
    "    patch_px: int = 128           # physical patch width/height in pixels (on the source image)\n",
    "    coords_mode: Literal[\"pixels\",\"normalized\"] = \"pixels\"  # input coords convention\n",
    "    clamp: bool = True            # clamp sampling grid to [-1,1]\n",
    "\n",
    "\n",
    "def _to_norm_xy(xy: torch.Tensor, H: int, W: int, mode: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert (x,y) in pixels or normalized to normalized coords in [-1,1] (align_corners=True).\n",
    "    xy: (...,2)\n",
    "    \"\"\"\n",
    "    if mode == \"normalized\":\n",
    "        return xy\n",
    "    # pixels â†’ normalized\n",
    "    x, y = xy[..., 0], xy[..., 1]\n",
    "    xn = 2.0 * x / max(W-1, 1) - 1.0\n",
    "    yn = 2.0 * y / max(H-1, 1) - 1.0\n",
    "    return torch.stack([xn, yn], dim=-1)\n",
    "\n",
    "\n",
    "def _make_patch_grid(\n",
    "    center_xy_norm: torch.Tensor,  # (2,), requires_grad=True\n",
    "    H: int, W: int,\n",
    "    out_res: int,\n",
    "    patch_px: int,\n",
    "    clamp: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a sampling grid (1, out_res, out_res, 2) in normalized coords so that\n",
    "    grid_sample(image, grid) returns a patch centered at center_xy_norm with physical\n",
    "    size ~ patch_px Ã— patch_px (in source pixel units).\n",
    "    \"\"\"\n",
    "    device = center_xy_norm.device\n",
    "    # Base grid in [-1,1]\n",
    "    yy, xx = torch.meshgrid(\n",
    "        torch.linspace(-1, 1, out_res, device=device),\n",
    "        torch.linspace(-1, 1, out_res, device=device),\n",
    "        indexing=\"ij\"\n",
    "    )\n",
    "    base = torch.stack([xx, yy], dim=-1)  # (H,W,2)\n",
    "\n",
    "    # Half-size of patch in normalized coords: (patch_px/2) * (2/W or 2/H) = patch_px/W (or /H)\n",
    "    sx = (patch_px / float(W))\n",
    "    sy = (patch_px / float(H))\n",
    "    scaled = torch.stack([sx * base[..., 0], sy * base[..., 1]], dim=-1)\n",
    "\n",
    "    # Shift to center\n",
    "    grid = scaled + center_xy_norm[None, None, :]\n",
    "    if clamp:\n",
    "        grid = torch.clamp(grid, -1.0, 1.0)\n",
    "    return grid.unsqueeze(0)  # (1, out_res, out_res, 2)\n",
    "\n",
    "\n",
    "def _bilinear_sample_bchw(\n",
    "    image_bchw: torch.Tensor,         # (1,C,H,W)\n",
    "    grid: torch.Tensor,               # (1, out_h, out_w, 2) in normalized [-1,1]\n",
    "    padding_mode: Literal[\"border\",\"zeros\"] = \"border\",\n",
    "    align_corners: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Differentiable bilinear sampler equivalent to grid_sample for N=1 using\n",
    "    primitive ops that are supported on MPS backward. Grad flows w.r.t. image and grid.\n",
    "    \"\"\"\n",
    "    assert image_bchw.ndim == 4 and image_bchw.shape[0] == 1\n",
    "    assert grid.ndim == 4 and grid.shape[0] == 1\n",
    "    _, C, H, W = image_bchw.shape\n",
    "    _, out_h, out_w, _ = grid.shape\n",
    "\n",
    "    gx = grid[..., 0]\n",
    "    gy = grid[..., 1]\n",
    "\n",
    "    if align_corners:\n",
    "        x = (gx + 1) * (W - 1) * 0.5\n",
    "        y = (gy + 1) * (H - 1) * 0.5\n",
    "    else:\n",
    "        x = ((gx + 1) * W - 1) * 0.5\n",
    "        y = ((gy + 1) * H - 1) * 0.5\n",
    "\n",
    "    x0 = torch.floor(x)\n",
    "    y0 = torch.floor(y)\n",
    "    x1 = x0 + 1\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    wa = (x1 - x) * (y1 - y)\n",
    "    wb = (x - x0) * (y1 - y)\n",
    "    wc = (x1 - x) * (y - y0)\n",
    "    wd = (x - x0) * (y - y0)\n",
    "\n",
    "    def sample_at(ix: torch.Tensor, iy: torch.Tensor) -> torch.Tensor:\n",
    "        ix_l = ix.long()\n",
    "        iy_l = iy.long()\n",
    "        if padding_mode == \"border\":\n",
    "            ix_l = ix_l.clamp(0, W - 1)\n",
    "            iy_l = iy_l.clamp(0, H - 1)\n",
    "            vals = image_bchw[:, :, iy_l, ix_l]  # (1,C,out_h,out_w)\n",
    "            return vals\n",
    "        elif padding_mode == \"zeros\":\n",
    "            in_x = (ix_l >= 0) & (ix_l < W)\n",
    "            in_y = (iy_l >= 0) & (iy_l < H)\n",
    "            inb = (in_x & in_y).unsqueeze(1)  # (1,1,out_h,out_w)\n",
    "            ix_c = ix_l.clamp(0, W - 1)\n",
    "            iy_c = iy_l.clamp(0, H - 1)\n",
    "            vals = image_bchw[:, :, iy_c, ix_c]\n",
    "            return vals * inb\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported padding_mode\")\n",
    "\n",
    "    Ia = sample_at(x0, y0)\n",
    "    Ib = sample_at(x1, y0)\n",
    "    Ic = sample_at(x0, y1)\n",
    "    Id = sample_at(x1, y1)\n",
    "\n",
    "    wa = wa.unsqueeze(1)\n",
    "    wb = wb.unsqueeze(1)\n",
    "    wc = wc.unsqueeze(1)\n",
    "    wd = wd.unsqueeze(1)\n",
    "\n",
    "    out = Ia * wa + Ib * wb + Ic * wc + Id * wd  # (1,C,out_h,out_w)\n",
    "    return out\n",
    "\n",
    "\n",
    "def coords_to_features(\n",
    "    image_bchw: torch.Tensor,        # (1,3,H,W), RGB, float in [0,1]\n",
    "    center_xy: torch.Tensor,         # (2,) x,y in pixels or normalized\n",
    "    cfg: CoordsToFeaturesConfig,\n",
    "    extractor: Optional[FeatureExtractor] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns (features, patch_bchw). Differentiable w.r.t. center_xy (and image if you want).\n",
    "    \"\"\"\n",
    "    assert image_bchw.ndim == 4 and image_bchw.shape[0] == 1 and image_bchw.shape[1] == 3\n",
    "    _, _, H, W = image_bchw.shape\n",
    "    device = image_bchw.device\n",
    "\n",
    "    center_xy = center_xy.to(device).float()\n",
    "    center_xy_norm = _to_norm_xy(center_xy, H, W, cfg.coords_mode)\n",
    "    center_xy_norm.requires_grad_(True)\n",
    "\n",
    "    # Build sampling grid\n",
    "    is_mps = (device.type == \"mps\")\n",
    "    effective_clamp = (cfg.clamp or is_mps)\n",
    "    grid = _make_patch_grid(center_xy_norm, H, W, cfg.out_res, cfg.patch_px, effective_clamp)\n",
    "\n",
    "    # Use a custom bilinear sampler on MPS to avoid grid_sample backward limitations\n",
    "    if is_mps:\n",
    "        patch = _bilinear_sample_bchw(image_bchw, grid, padding_mode=\"border\", align_corners=True).squeeze(2)\n",
    "    else:\n",
    "        patch = F.grid_sample(image_bchw, grid, mode=\"bilinear\", padding_mode=\"border\", align_corners=True)\n",
    "\n",
    "    if extractor is None:\n",
    "        extractor = build_feature_extractor(cfg.extractor_name, device=device)\n",
    "    \n",
    "    with torch.set_grad_enabled(True):\n",
    "        feats = extractor(patch)  # (1,D)\n",
    "    return feats, patch\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Sensitivity (grad/Hessian) at a coordinate\n",
    "# ============================\n",
    "\n",
    "@dataclass\n",
    "class SensitivityResult:\n",
    "    feats: torch.Tensor             # (D,)\n",
    "    energy: torch.Tensor            # scalar g = 0.5||f||^2 or other reduction\n",
    "    grad_xy: torch.Tensor           # (2,) âˆ‚g/âˆ‚(x,y)   in chosen coords_mode\n",
    "    unit_dir: torch.Tensor          # (2,) normalized gradient direction (steepest ascent)\n",
    "    hess_xy: torch.Tensor           # (2,2) Hessian of g wrt (x,y)\n",
    "    eigvals: torch.Tensor           # (2,) principal curvatures\n",
    "    eigvecs: torch.Tensor           # (2,2) columns = principal directions\n",
    "    patch: torch.Tensor             # (1,3,out_res,out_res)\n",
    "\n",
    "def feature_sensitivity_at(\n",
    "    image_bchw: torch.Tensor,            # (1,3,H,W), RGB, [0,1]\n",
    "    center_xy: Tuple[float,float] | torch.Tensor,  # (x,y) in pixels by default\n",
    "    extractor_name: str = \"resnet50\",\n",
    "    coords_mode: Literal[\"pixels\",\"normalized\"] = \"pixels\",\n",
    "    patch_px: int = 128,\n",
    "    out_res: int = 224,\n",
    "    reduction: Literal[\"l2\",\"l2_half\",\"l1\"] = \"l2_half\",\n",
    ") -> SensitivityResult:\n",
    "    \"\"\"\n",
    "    Compute âˆ‚g/âˆ‚(x,y) and âˆ‚Â²g/âˆ‚(x,y)Â² where g is a scalar energy of features f(patch(x,y)).\n",
    "    - Default g = 0.5*||f||^2 (smooth and convenient): 'l2_half'\n",
    "    - 'l2' uses ||f||^2, 'l1' uses ||f||_1 (subgradient-friendly).\n",
    "    Returns gradient direction (steepest ascent), Hessian, and eigendecomposition.\n",
    "    \"\"\"\n",
    "    if isinstance(center_xy, tuple):\n",
    "        center_xy = torch.tensor(center_xy, dtype=torch.float32, device=image_bchw.device)\n",
    "\n",
    "    cfg = CoordsToFeaturesConfig(\n",
    "        extractor_name=extractor_name,\n",
    "        out_res=out_res,\n",
    "        patch_px=patch_px,\n",
    "        coords_mode=coords_mode,\n",
    "        clamp=True,\n",
    "    )\n",
    "\n",
    "    orig_device = image_bchw.device\n",
    "\n",
    "    # Build extractor once on the original device\n",
    "    extractor = build_feature_extractor(extractor_name, device=orig_device)\n",
    "\n",
    "    # Prepare coordinates tensor on original device\n",
    "    xy = center_xy.detach().clone().to(orig_device)\n",
    "    xy.requires_grad_(True)\n",
    "\n",
    "    # Keep image on original device\n",
    "    image_in = image_bchw\n",
    "\n",
    "    # Scalar energy helper\n",
    "    def energy_from_xy(xy_in: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        feats, patch = coords_to_features(image_in, xy_in, cfg, extractor)\n",
    "        feats = feats.view(-1)\n",
    "        if reduction == \"l2_half\":\n",
    "            g = 0.5 * torch.dot(feats, feats)\n",
    "        elif reduction == \"l2\":\n",
    "            g = torch.dot(feats, feats)\n",
    "        elif reduction == \"l1\":\n",
    "            g = feats.abs().sum()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown reduction\")\n",
    "        return g, feats, patch\n",
    "\n",
    "    # Autograd path (now MPS-safe via custom bilinear sampler)\n",
    "    g, fvec, patch = energy_from_xy(xy)\n",
    "    grad = torch.autograd.grad(g, xy, create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    def _scalar_fn(xy_in: torch.Tensor) -> torch.Tensor:\n",
    "        g2, _, _ = energy_from_xy(xy_in)\n",
    "        return g2\n",
    "    H = torch.autograd.functional.hessian(_scalar_fn, xy, create_graph=False)\n",
    "    evals, evecs = torch.linalg.eigh(H)\n",
    "    grad_dir = grad / (grad.norm() + 1e-8)\n",
    "    return SensitivityResult(\n",
    "        feats=fvec.detach(),\n",
    "        energy=g.detach(),\n",
    "        grad_xy=grad.detach(),\n",
    "        unit_dir=grad_dir.detach(),\n",
    "        hess_xy=H.detach(),\n",
    "        eigvals=evals.detach(),\n",
    "        eigvecs=evecs.detach(),\n",
    "        patch=patch.detach()\n",
    "    )\n",
    "# img_bchw: (1,3,H,W), RGB in [0,1] from your StyleGAN pipeline (no need to convert to BGR)\n",
    "img_bchw = G(sample_w(1, G=G), None)  # or your already-rendered tensor\n",
    "print(img_bchw.shape)\n",
    "res = feature_sensitivity_at(\n",
    "    image_bchw=img_bchw,\n",
    "    center_xy=(256, 256),            # pixels; set coords_mode=\"normalized\" if using [-1,1]\n",
    "    extractor_name=\"resnet50\",       # or \"vgg16\", \"lpips\", \"clip-vitb32\" if installed\n",
    "    coords_mode=\"pixels\",\n",
    "    patch_px=128,\n",
    "    out_res=224,\n",
    "    reduction=\"l2_half\"\n",
    ")\n",
    "\n",
    "print(\"grad (âˆ‚g/âˆ‚x, âˆ‚g/âˆ‚y):\", res.grad_xy.cpu().numpy())\n",
    "print(\"unit dir (fastest change):\", res.unit_dir.cpu().numpy())\n",
    "print(\"Hessian:\\n\", res.hess_xy.cpu().numpy())\n",
    "print(\"eigvals:\", res.eigvals.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47111ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18709dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manip311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
