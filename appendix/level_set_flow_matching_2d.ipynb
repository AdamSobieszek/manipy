{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adamsobieszek/PycharmProjects/manipy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import ot # Python Optimal Transport library\n",
    "import functools # For passing arguments to collate_fn\n",
    "%cd /Users/adamsobieszek/PycharmProjects/manipy\n",
    "import sys\n",
    "sys.path.append('/Users/adamsobieszek/PycharmProjects/manipy/levelset_fm')\n",
    "from levelset_fm.models.flow_model import VectorFieldTransformer, VectorFieldModel2, VectorFieldTransformer3\n",
    "from lamb_optimizer import Lamb\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "DTYPE = torch.float32\n",
    "\n",
    "class PoolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that holds a pre-sampled pool of points.\n",
    "    During initialization, it samples points such that the distribution of their\n",
    "    f(x) values is approximately uniform.\n",
    "    Used with a custom collate_fn for dynamic minibatch OT pairing.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 distribution,    # PyTorch distribution to sample P_X from\n",
    "                 f_func,          # Function f(x) -> scalar\n",
    "                 pool_size=10000, # Desired final size of the pool\n",
    "                 oversample_factor=10, # How many times more samples to draw initially\n",
    "                 num_f_bins=100,    # Number of bins for f-value histogram re-weighting\n",
    "                 device=None,\n",
    "                 dtype=None):\n",
    "\n",
    "        self.distribution = distribution\n",
    "        self.f_func = f_func\n",
    "        self.pool_size = pool_size\n",
    "        self.oversample_factor = oversample_factor\n",
    "        self.num_f_bins = num_f_bins\n",
    "        self.device = device if device is not None else torch.device(\"cpu\")\n",
    "        self.dtype = dtype if dtype is not None else torch.float32\n",
    "\n",
    "        print(f\"Initializing PoolDataset on {self.device} with {self.dtype}.\")\n",
    "        print(f\"Target pool size: {self.pool_size}. Oversampling factor: {self.oversample_factor}.\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 1. Oversample initially\n",
    "            num_initial_samples = self.pool_size * self.oversample_factor\n",
    "            print(f\"Generating initial {num_initial_samples} samples from the distribution...\")\n",
    "            initial_points = self.distribution.sample((num_initial_samples,)).to(self.device, self.dtype)\n",
    "            initial_f_values = self.f_func(initial_points)\n",
    "            print(\"Initial sampling and f(x) evaluation complete.\")\n",
    "\n",
    "            if torch.isnan(initial_f_values).any() or torch.isinf(initial_f_values).any():\n",
    "                print(\"Warning: NaN or Inf found in initial_f_values. Filtering them out.\")\n",
    "                valid_mask = ~(torch.isnan(initial_f_values) | torch.isinf(initial_f_values))\n",
    "                initial_points = initial_points[valid_mask]\n",
    "                initial_f_values = initial_f_values[valid_mask]\n",
    "                if initial_points.shape[0] == 0:\n",
    "                    raise ValueError(\"All initial samples resulted in NaN/Inf f-values. Check f_func or distribution.\")\n",
    "\n",
    "            if initial_points.shape[0] < self.pool_size:\n",
    "                 print(f\"Warning: After filtering NaN/Inf, only {initial_points.shape[0]} samples remain, \"\n",
    "                       f\"which is less than the target pool_size {self.pool_size}. \"\n",
    "                       \"The final pool will be smaller.\")\n",
    "                 self.pool_size = initial_points.shape[0]\n",
    "\n",
    "\n",
    "            if self.pool_size == 0: # If target pool size becomes 0 after filtering\n",
    "                self.pool_points = torch.empty((0, initial_points.shape[-1] if initial_points.nelement() > 0 else 2), device=self.device, dtype=self.dtype)\n",
    "                self.pool_f_values = torch.empty((0,), device=self.device, dtype=self.dtype)\n",
    "                print(\"Pool dataset is empty after NaN/Inf filtering and pool size adjustment.\")\n",
    "                return\n",
    "\n",
    "\n",
    "            # 2. Stratified resampling to make f(x) distribution more uniform\n",
    "            print(f\"Performing stratified resampling for f(x) values using {self.num_f_bins} bins...\")\n",
    "\n",
    "            # Move f_values to CPU for histogramming with NumPy if it's large\n",
    "            f_values_np = initial_f_values.cpu().numpy()\n",
    "            # Determine bins based on the range of f_values_np using 1st and 99th percentiles\n",
    "            f_min, f_max = np.percentile(f_values_np, 1), np.percentile(f_values_np, 99.9)\n",
    "            initial_points = initial_points[(f_values_np > f_min) & (f_values_np < f_max)]\n",
    "            initial_f_values = initial_f_values[(f_values_np > f_min) & (f_values_np < f_max)]\n",
    "            f_values_np = f_values_np[(f_values_np > f_min) & (f_values_np < f_max)]\n",
    "\n",
    "            hist, bin_edges = np.histogram(f_values_np, bins=self.num_f_bins)\n",
    "            \n",
    "            # Assign each f_value to a bin\n",
    "            # np.digitize returns bin index (1-based), so subtract 1 for 0-based\n",
    "            bin_indices = np.digitize(f_values_np, bin_edges[:-1]) - 1\n",
    "            # Ensure indices are within [0, num_f_bins-1]\n",
    "            bin_indices = np.clip(bin_indices, 0, self.num_f_bins - 1)\n",
    "\n",
    "            # Calculate weights: inverse of bin counts (add 1 to avoid division by zero for empty bins, though unlikely here)\n",
    "            # Add a small epsilon to counts for bins that might be empty after all due to edge cases or if some bins become 0 after clip\n",
    "            bin_counts_epsilon = hist.astype(np.float32) + 1e-6\n",
    "            sample_weights_for_f_val = 1.0 / bin_counts_epsilon[bin_indices]\n",
    "\n",
    "            # Normalize weights to get probabilities\n",
    "            probabilities = sample_weights_for_f_val / np.sum(sample_weights_for_f_val)\n",
    "            \n",
    "            # Resample `pool_size` indices based on these probabilities\n",
    "            # `replace=True` is important if pool_size > number of unique low-density samples\n",
    "            # and allows oversampling from underrepresented regions.\n",
    "            if np.sum(probabilities) == 0 or np.any(np.isnan(probabilities)) or np.any(np.isinf(probabilities)):\n",
    "                print(\"Warning: Invalid probabilities for resampling. Falling back to uniform random sampling.\")\n",
    "                indices = np.random.choice(len(initial_points), self.pool_size, replace=True)\n",
    "            else:\n",
    "                indices = np.random.choice(len(initial_points), self.pool_size, replace=True, p=probabilities)\n",
    "\n",
    "            # plot the histogram of the resampled points\n",
    "            # plt.hist(initial_f_values[indices].cpu().numpy(), bins=self.num_f_bins, color='green', alpha=0.7)\n",
    "            # plt.show()\n",
    "\n",
    "            # 3. Create the final pool with resampled points\n",
    "            self.pool_points = initial_points[indices]\n",
    "            self.pool_f_values = initial_f_values[indices] # f-values corresponding to the resampled points\n",
    "\n",
    "        print(f\"Final data pool generated with {len(self.pool_points)} samples after resampling for uniform f(x) distribution.\")\n",
    "\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt_debug\n",
    "            plt_debug.figure(figsize=(12, 4))\n",
    "            plt_debug.subplot(1, 2, 1)\n",
    "            plt_debug.hist(f_values_np, bins=self.num_f_bins, color='blue', alpha=0.7)\n",
    "            plt_debug.title(\"f(x) distribution in initial oversample\")\n",
    "            plt_debug.subplot(1, 2, 2)\n",
    "            plt_debug.hist(self.pool_f_values.cpu().numpy(), bins=self.num_f_bins, color='green', alpha=0.7)\n",
    "            plt_debug.title(f\"f(x) distribution in final pool (size {self.pool_size})\")\n",
    "            plt_debug.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Debug plot failed: {e}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if not hasattr(self, 'pool_points'): # If init failed very early\n",
    "            return 0\n",
    "        return len(self.pool_points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not hasattr(self, 'pool_points') or len(self.pool_points) == 0:\n",
    "            raise IndexError(\"PoolDataset is empty or not properly initialized.\")\n",
    "        # Returns a single raw sample (point and its f-value)\n",
    "        return self.pool_points[idx], self.pool_f_values[idx]\n",
    "    \n",
    "def minibatch_ot_lst_collate_fn(\n",
    "    raw_batch_item_tuples, # List of (point_tensor, f_value_tensor) from PoolDataset __getitem__\n",
    "    min_f_diff,\n",
    "    min_norm_diff,\n",
    "    ot_computation_device, # e.g., 'cpu' as POT usually expects numpy cpu arrays\n",
    "    target_device,         # e.g., global DEVICE for training\n",
    "    target_dtype           # e.g., global DTYPE for training\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Custom collate_fn for dynamic minibatch OT pairing for Level-Set Transport.\n",
    "    Takes a batch of raw samples, splits them, computes OT with LST cost, and returns paired data.\n",
    "    \"\"\"\n",
    "    num_raw_samples_in_this_collate_batch = len(raw_batch_item_tuples)\n",
    "\n",
    "    if num_raw_samples_in_this_collate_batch == 0: # Can happen if dataloader num_workers > 0 and dataset is small near end\n",
    "         dim = 2 # Fallback dimension\n",
    "         # Try to get dim from a sample if possible, otherwise default\n",
    "         if hasattr(raw_batch_item_tuples, '__iter__') and len(raw_batch_item_tuples) > 0 and hasattr(raw_batch_item_tuples[0][0], 'shape'):\n",
    "            dim = raw_batch_item_tuples[0][0].shape[-1]\n",
    "\n",
    "         return (torch.empty((0, dim), device=target_device, dtype=target_dtype),\n",
    "                torch.empty((0, dim), device=target_device, dtype=target_dtype),\n",
    "                torch.empty((0, dim), device=target_device, dtype=target_dtype))\n",
    "\n",
    "\n",
    "    if num_raw_samples_in_this_collate_batch % 2 != 0:\n",
    "        # This can happen if batch_size for DataLoader is odd, or drop_last=False and last batch is odd.\n",
    "        # Handle by trimming or raising error. Trimming is safer for continuation.\n",
    "        print(f\"Collate Warning: Number of samples {num_raw_samples_in_this_collate_batch} is odd. Trimming last sample.\")\n",
    "        raw_batch_item_tuples = raw_batch_item_tuples[:-1]\n",
    "        num_raw_samples_in_this_collate_batch -=1\n",
    "        if num_raw_samples_in_this_collate_batch == 0:\n",
    "            dim = 2 \n",
    "            return (torch.empty((0, dim), device=target_device, dtype=target_dtype),\n",
    "                    torch.empty((0, dim), device=target_device, dtype=target_dtype),\n",
    "                    torch.empty((0, dim), device=target_device, dtype=target_dtype))\n",
    "\n",
    "\n",
    "    # Unzip points and their f_values from the list of tuples\n",
    "    # Ensure they are on the target_device and target_dtype for initial processing\n",
    "    raw_points_batch = torch.stack([item[0] for item in raw_batch_item_tuples]).to(target_device, target_dtype)\n",
    "    raw_f_values_batch = torch.stack([item[1] for item in raw_batch_item_tuples]).to(target_device, target_dtype)\n",
    "    \n",
    "    _, f_sorted_indices = torch.sort(raw_f_values_batch)\n",
    "    \n",
    "    N = num_raw_samples_in_this_collate_batch\n",
    "    margin_percentage = 0.1\n",
    "    n_extreme = int(N*margin_percentage)\n",
    "\n",
    "    x0_indices = f_sorted_indices[:n_extreme]\n",
    "    x1_indices = f_sorted_indices[N - n_extreme:]\n",
    "    middle_indices = f_sorted_indices[n_extreme : N - n_extreme]\n",
    "\n",
    "    middle_indices = middle_indices[torch.randperm(len(middle_indices))]\n",
    "    x0_indices = x0_indices[torch.randperm(len(x0_indices))]\n",
    "    x1_indices = x1_indices[torch.randperm(len(x1_indices))]\n",
    "    x0_indices = x0_indices[len(x0_indices)//2:]\n",
    "    x1_indices = x1_indices[len(x1_indices)//2:]\n",
    "\n",
    "    x0_indices = torch.cat([x0_indices, middle_indices[:len(middle_indices)//2]])\n",
    "    x1_indices = torch.cat([x1_indices, middle_indices[len(middle_indices)//2:]])\n",
    "    ot_minibatch_internal_size = len(x0_indices)\n",
    "\n",
    "    x0_mb = raw_points_batch[x0_indices]\n",
    "    f_v_x0_mb = raw_f_values_batch[x0_indices]\n",
    "    \n",
    "    x1_mb = raw_points_batch[x1_indices]\n",
    "    f_v_x1_mb = raw_f_values_batch[x1_indices]\n",
    "\n",
    "    final_paired_x0_list = []\n",
    "    final_paired_x1_list = []\n",
    "    final_u_hat_target_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Construct the m x m cost matrix C_mini on target_device first\n",
    "        x0_b_exp = x0_mb.unsqueeze(1)       # (m, 1, d)\n",
    "        f_v_x0_b_exp = f_v_x0_mb.unsqueeze(1) # (m, 1)\n",
    "        x1_b_exp = x1_mb.unsqueeze(0)       # (1, m, d)\n",
    "        f_v_x1_b_exp = f_v_x1_mb.unsqueeze(0) # (1, m)\n",
    "\n",
    "        # Cost components:\n",
    "        # diff_x[i,j,:] = x1_mb[j] - x0_mb[i] (where i is row for x0, j is col for x1)\n",
    "        diff_x_abs = x1_b_exp - x0_b_exp      # Shape (m, m, d)\n",
    "        dist_cost_x = torch.sum(diff_x_abs**2,dim=2) # Shape (m, m)\n",
    "        \n",
    "        # f_diff[i,j] = f_v_x1_mb[j] - f_v_x0_mb[i]\n",
    "        f_diff = f_v_x1_b_exp - f_v_x0_b_exp # Shape (m, m)\n",
    "        dist_cost_y = (f_diff)**2 # Shape (m, m)\n",
    "\n",
    "        # Validity mask for the m x m cost matrix\n",
    "        mask = (f_diff > min_f_diff) #& \\\n",
    "               #(norm_diff_x > (min_norm_diff))\n",
    "        \n",
    "        large_cost_val = 1e18 # Represents infinity for OT\n",
    "        cost_matrix_mb_torch = torch.full_like(dist_cost_x, large_cost_val, \n",
    "                                               device=target_device, dtype=target_dtype)\n",
    "        \n",
    "        # Calculate LST cost where valid: ||x1-x0|| / (f(x1)-f(x0))\n",
    "        cost_matrix_mb_torch[mask] = (dist_cost_x[mask] / dist_cost_y[mask])\n",
    "        \n",
    "        # Move to OT computation device (likely CPU) and convert to NumPy\n",
    "        cost_matrix_mb_np = cost_matrix_mb_torch.to(ot_computation_device).cpu().numpy()\n",
    "\n",
    "    # Uniform weights for OT problem\n",
    "    a_unif = np.ones(ot_minibatch_internal_size) / ot_minibatch_internal_size\n",
    "    b_unif = np.ones(ot_minibatch_internal_size) / ot_minibatch_internal_size\n",
    "    \n",
    "    try:\n",
    "        if np.all(cost_matrix_mb_np >= large_cost_val * 0.99): # Check if all costs are effectively infinite\n",
    "            # print(f\"Collate Info: No valid paths in cost matrix for OT solve in this batch. Returning 0 pairs.\")\n",
    "            pass # Fall through to return empty/few pairs\n",
    "        else:\n",
    "            # Solve OT problem\n",
    "            # For stability, replace any true inf with the large_cost_val\n",
    "            cost_matrix_mb_np[np.isinf(cost_matrix_mb_np)] = large_cost_val \n",
    "            pi_minibatch_np = ot.emd(a_unif, b_unif, cost_matrix_mb_np, numItermax=200000) # Transport plan (m x m)\n",
    "            \n",
    "            # Extract pairs (indices are relative to the minibatches x0_mb, x1_mb)\n",
    "            # For EMD with uniform marginals, pi_minibatch_np[i,j] will be 0 or 1/m.\n",
    "            # We look for entries > threshold (e.g., > 0 or > 1/(2*m) for safety)\n",
    "            paired_indices_in_mb = np.argwhere(pi_minibatch_np > 0)# (1.0 / (ot_minibatch_internal_size * 2.0 + 1e-9)) ) \n",
    "\n",
    "            for r_idx_mb, c_idx_mb in paired_indices_in_mb:\n",
    "                # r_idx_mb is index in x0_mb (row of cost matrix), c_idx_mb is index in x1_mb (col of cost matrix)\n",
    "                x0_selected = x0_mb[r_idx_mb] # This is already on target_device, target_dtype\n",
    "                x1_selected = x1_mb[c_idx_mb] # This is already on target_device, target_dtype\n",
    "\n",
    "                # Final check on validity (should be redundant if mask was correct)\n",
    "                f0_val_check = f_v_x0_mb[r_idx_mb]\n",
    "                f1_val_check = f_v_x1_mb[c_idx_mb]\n",
    "                f_diff = f1_val_check - f0_val_check\n",
    "                assert f_diff > 0, \"f_diff is not positive\"\n",
    "\n",
    "                # if not (f1_val_check > f0_val_check + min_f_diff and norm_sq_check > min_norm_diff**2):\n",
    "                #     # This pair, despite OT, doesn't meet strict criteria. Skip.\n",
    "                #     # Could happen if OT solver forces a pairing due to limited options.\n",
    "                #     continue    \n",
    "\n",
    "                final_paired_x0_list.append(x0_selected)\n",
    "                final_paired_x1_list.append(x1_selected)\n",
    "                \n",
    "                diff_vec = (x1_selected - x0_selected)/f_diff\n",
    "                \n",
    "                # DONT normalize\n",
    "                u_hat = diff_vec \n",
    "                final_u_hat_target_list.append(u_hat)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Collate Warning: OT computation or pair extraction failed: {e}\")\n",
    "        # This batch might be smaller or empty if an error occurs.\n",
    "        pass\n",
    "\n",
    "    if not final_paired_x0_list: # No pairs successfully formed for this batch\n",
    "        dim = raw_points_batch.shape[-1] if raw_points_batch.nelement() > 0 else 2\n",
    "        return (torch.empty((0, dim), device=target_device, dtype=target_dtype),\n",
    "                torch.empty((0, dim), device=target_device, dtype=target_dtype),\n",
    "                torch.empty((0, dim), device=target_device, dtype=target_dtype))\n",
    "\n",
    "    # Stack the collected pairs and targets into batch tensors\n",
    "    batch_x0_final = torch.stack(final_paired_x0_list)\n",
    "    batch_x1_final = torch.stack(final_paired_x1_list)\n",
    "    batch_u_hat_target_final = torch.stack(final_u_hat_target_list)\n",
    "    \n",
    "    return batch_x0_final, batch_x1_final, batch_u_hat_target_final\n",
    "\n",
    "\n",
    "# Configuration for data loading and OT\n",
    "data_config = {\n",
    "    'pool_size': 200000,  # Size of the static pool of raw P_X samples\n",
    "    'training_pairs_per_batch': 2048, # Number of (x0,x1) pairs desired for model training batch\n",
    "    'min_f_diff_collate': 0.0001,\n",
    "    'min_norm_diff_collate': 1e-5,\n",
    "    'ot_computation_device': 'cpu' # POT library usually runs on CPU\n",
    "}\n",
    "\n",
    "custom_collate = functools.partial(\n",
    "    minibatch_ot_lst_collate_fn,\n",
    "    min_f_diff=data_config['min_f_diff_collate'],\n",
    "    min_norm_diff=data_config['min_norm_diff_collate'],\n",
    "    ot_computation_device=torch.device(data_config['ot_computation_device']),\n",
    "    target_device=DEVICE, # Final batch tensors should be on this device\n",
    "    target_dtype=DTYPE\n",
    ")\n",
    "\n",
    "\n",
    "# 1. Create the PoolDataset instance\n",
    "\n",
    "batches = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps, Dtype: torch.float32\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::linalg_cholesky_ex.L' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 376\u001b[0m\n\u001b[1;32m    374\u001b[0m p_dist_param_device \u001b[38;5;241m=\u001b[39m DEVICE\n\u001b[1;32m    375\u001b[0m mix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(mix_probs\u001b[38;5;241m.\u001b[39mto(p_dist_param_device))\n\u001b[0;32m--> 376\u001b[0m comp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_dist_param_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_mats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_dist_param_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m p_distribution_base \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mMixtureSameFamily(mix, comp)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSkewedDistribution\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mDistribution):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/manip39/lib/python3.9/site-packages/torch/distributions/multivariate_normal.py:177\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m=\u001b[39m loc\u001b[38;5;241m.\u001b[39mexpand(batch_shape \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m    176\u001b[0m event_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_tril \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m scale_tril\n",
      "File \u001b[0;32m/opt/anaconda3/envs/manip39/lib/python3.9/site-packages/torch/distributions/distribution.py:66\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[1;32m     65\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[0;32m---> 66\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/manip39/lib/python3.9/site-packages/torch/distributions/constraints.py:557\u001b[0m, in \u001b[0;36m_PositiveDefinite.check\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sym_check\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sym_check\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::linalg_cholesky_ex.L' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchdiffeq import odeint\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "# --- Global Settings & Provided Functions ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "DTYPE = torch.float32\n",
    "print(f\"Using device: {DEVICE}, Dtype: {DTYPE}\")\n",
    "\n",
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "        if delta <= 0:\n",
    "            raise ValueError(\"HuberLoss delta must be positive.\")\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        if predictions.shape != targets.shape:\n",
    "            raise ValueError(f\"Predictions shape {predictions.shape} must match targets shape {targets.shape}\")\n",
    "        targets = targets.to(dtype=predictions.dtype)\n",
    "        error = torch.abs(targets - predictions)\n",
    "        sum_error = (error**2).sum(dim=-1)\n",
    "        quadratic_part_condition = (sum_error <= self.delta)\n",
    "        linear_part_condition = ~quadratic_part_condition\n",
    "        loss_values = torch.zeros(predictions.shape[0], dtype=predictions.dtype, device=predictions.device)\n",
    "        loss_values[quadratic_part_condition] = 0.5 * sum_error[quadratic_part_condition]\n",
    "        loss_values[linear_part_condition] = self.delta * (error[linear_part_condition].sum(dim=-1) - 0.5 * self.delta)\n",
    "        if self.reduction == 'mean':\n",
    "            return loss_values.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss_values.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return loss_values\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid reduction: {self.reduction}\")\n",
    "\n",
    "huber = HuberLoss(1.)\n",
    "\n",
    "# --- User Provided Plotting Helper ---\n",
    "def _plot_current_rescaled_field_and_contours(\n",
    "    model_V_hat, p_distribution, f_func, grad_f_func,\n",
    "    ax, grid_extents, grid_points, device, dtype, stability_eps=1e-4\n",
    "):\n",
    "    # model_V_hat.eval() # Eval mode should be handled by the caller if necessary\n",
    "\n",
    "    # 1. Plot p(x) contours\n",
    "    contour_grid_points = grid_points * 2\n",
    "    x_lin_dist = torch.linspace(grid_extents[0], grid_extents[1], contour_grid_points, device=device, dtype=dtype)\n",
    "    y_lin_dist = torch.linspace(grid_extents[0], grid_extents[1], contour_grid_points, device=device, dtype=dtype)\n",
    "    X_grid_dist, Y_grid_dist = torch.meshgrid(x_lin_dist, y_lin_dist, indexing='ij')\n",
    "    grid_coords_dist = torch.stack([X_grid_dist.flatten(), Y_grid_dist.flatten()], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        log_probs = p_distribution.log_prob(grid_coords_dist)\n",
    "        probs = torch.exp(log_probs).cpu().numpy().reshape(contour_grid_points, contour_grid_points)\n",
    "\n",
    "    ax.contourf(X_grid_dist.cpu().numpy(), Y_grid_dist.cpu().numpy(), probs, levels=10, cmap='viridis', alpha=0.4)\n",
    "    ax.contour(X_grid_dist.cpu().numpy(), Y_grid_dist.cpu().numpy(), probs, levels=10, colors='white', linewidths=0.5, alpha=0.3)\n",
    "\n",
    "    # 2. Plot rescaled vector field V(x) (or w_hat(x) from model)\n",
    "    x_lin_vf = torch.linspace(grid_extents[0], grid_extents[1], grid_points, device=device, dtype=dtype)\n",
    "    y_lin_vf = torch.linspace(grid_extents[0], grid_extents[1], grid_points, device=device, dtype=dtype)\n",
    "    X_grid_vf, Y_grid_vf = torch.meshgrid(x_lin_vf, y_lin_vf, indexing='ij')\n",
    "    grid_coords_vf = torch.stack([X_grid_vf.flatten(), Y_grid_vf.flatten()], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w_hat_x = model_V_hat(grid_coords_vf)*2 # Output of the model\n",
    "        w_hat_norm_too_high = w_hat_x.norm(dim=-1, keepdim=True).clamp(min=3)\n",
    "        w_hat_x = w_hat_x / w_hat_norm_too_high\n",
    "\n",
    "    vf_np_x = w_hat_x[:, 0].cpu().numpy().reshape(grid_points, grid_points)\n",
    "    vf_np_y = w_hat_x[:, 1].cpu().numpy().reshape(grid_points, grid_points)\n",
    "\n",
    "    ax.quiver(X_grid_vf.cpu().numpy(), Y_grid_vf.cpu().numpy(), vf_np_x, vf_np_y,\n",
    "              color='deepskyblue', scale=None, scale_units='xy', angles='xy',\n",
    "              headwidth=4, headlength=6, width=0.004, zorder=5, alpha=0.9)\n",
    "    # model_V_hat.train() # Train mode should be handled by the caller\n",
    "\n",
    "# --- User Provided Final Visualization ---\n",
    "def visualize_final_lst_flow(\n",
    "    model_V_hat, p_distribution, f_func, grad_f_func,\n",
    "    grid_extents, grid_points=30, num_trajectories=15,\n",
    "    S_final=2.0, stability_eps=1e-4,\n",
    "    writer=None, current_step=None, # For TensorBoard logging\n",
    "    device=DEVICE, dtype=DTYPE     # Added device and dtype args\n",
    "):\n",
    "    # print(\"Starting detailed final LST flow visualization...\")\n",
    "    original_training_state = model_V_hat.training\n",
    "    model_V_hat.eval()\n",
    "\n",
    "    # Assuming model_V_hat might have this attribute, as per user's code\n",
    "    if hasattr(model_V_hat, 'add_rating_gradient'):\n",
    "        original_add_rating_gradient = model_V_hat.add_rating_gradient\n",
    "        model_V_hat.add_rating_gradient = False\n",
    "\n",
    "        # --- Plot 1: Learned Field w_hat(x) ---\n",
    "        fig_field, ax_field = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax_field.set_xlabel(\"$x_1$\"); ax_field.set_ylabel(\"$x_2$\")\n",
    "        ax_field.set_title(\"Learned Field $w_{hat}(x)$ with $P_X$ Contours\")\n",
    "        ax_field.set_aspect('equal', adjustable='box')\n",
    "        ax_field.set_xlim(grid_extents); ax_field.set_ylim(grid_extents)\n",
    "        ax_field.grid(True, linestyle='--', alpha=0.5)\n",
    "        _plot_current_rescaled_field_and_contours(\n",
    "            model_V_hat, p_distribution, f_func, grad_f_func, ax_field,\n",
    "            grid_extents, grid_points, device, dtype, stability_eps\n",
    "        )\n",
    "        if writer and current_step is not None:\n",
    "            writer.add_figure('visualization/learned_field_w_hat', fig_field, current_step)\n",
    "            plt.close(fig_field)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        model_V_hat.add_rating_gradient = True\n",
    "\n",
    "    # --- Plot 2: Trajectories using V(x) ---\n",
    "    # V(x) = w_hat(x) / (<w_hat(x), grad f(x)> + eps)\n",
    "    def V_rescaled_field_func_for_ode(s_scalar, current_x_batch_ode): # s is not used by V(x) directly\n",
    "        with torch.no_grad():\n",
    "            w_hat_val = model_V_hat(current_x_batch_ode)\n",
    "            grad_f_val = grad_f_func(current_x_batch_ode, f_func)\n",
    "            dot_product = torch.sum(w_hat_val * grad_f_val, dim=1, keepdim=True)\n",
    "            # Ensure epsilon adds constructively, prevent division by zero if dot_product is small positive\n",
    "            # If dot_product is negative, V(x) points against f increase.\n",
    "            denominator = dot_product + stability_eps * torch.sign(dot_product).clamp(min=0) + 1e-9\n",
    "            return w_hat_val / denominator\n",
    "\n",
    "    x_initial_trajectories = config['x_initial_trajectories']\n",
    "\n",
    "    s_span = torch.linspace(0, S_final, 100, device=device, dtype=dtype)\n",
    "    # print(f\"Generating {num_trajectories} final trajectories (target integral of <V,grad_f> ds = {S_final:.2f})...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        trajectories = odeint(V_rescaled_field_func_for_ode, x_initial_trajectories, s_span, method='rk4', options=dict(step_size=S_final/100.0))\n",
    "\n",
    "    trajectories_np = trajectories.permute(1, 0, 2).cpu().numpy() # (num_trajectories, num_timesteps, dim)\n",
    "\n",
    "    fig_traj = plt.figure(figsize=(9, 8.5))\n",
    "    ax_traj = fig_traj.gca()\n",
    "    num_bg_samples = 1000\n",
    "    bg_samples = p_distribution.sample((num_bg_samples,)).cpu().numpy()\n",
    "    ax_traj.scatter(bg_samples[:, 0], bg_samples[:, 1], alpha=0.1, color='gray', label=f'$P_X$ Samples', s=5)\n",
    "    _plot_current_rescaled_field_and_contours(\n",
    "        model_V_hat, p_distribution, f_func, grad_f_func, ax_traj,\n",
    "        grid_extents, grid_points, device, dtype, stability_eps\n",
    "    )\n",
    "    for i in range(num_trajectories):\n",
    "        ax_traj.plot(trajectories_np[i, :, 0], trajectories_np[i, :, 1], '-', alpha=0.8, lw=1.5)\n",
    "        ax_traj.scatter(trajectories_np[i, 0, 0], trajectories_np[i, 0, 1],\n",
    "                     color='red', marker='o', s=15,\n",
    "                     label='Start ($x_0$)' if i == 0 else None, zorder=5)\n",
    "        ax_traj.scatter(trajectories_np[i, -1, 0], trajectories_np[i, -1, 1],\n",
    "                     color='lime', marker='X', s=20,\n",
    "                     label='End ($x_S$)' if i == 0 else None, zorder=5)\n",
    "\n",
    "    ax_traj.set_title(f\"Final LST Trajectories (Target $\\int <V, grad_f> ds = {S_final:.2f}$)\")\n",
    "    ax_traj.set_xlabel(\"$x_1$\"); ax_traj.set_ylabel(\"$x_2$\")\n",
    "    ax_traj.legend(loc='best'); ax_traj.set_xlim(grid_extents); ax_traj.set_ylim(grid_extents)\n",
    "    ax_traj.set_aspect('equal', adjustable='box')\n",
    "    ax_traj.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    if writer and current_step is not None:\n",
    "        writer.add_figure('visualization/final_trajectories', fig_traj, current_step)\n",
    "        plt.close(fig_traj)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    if hasattr(model_V_hat, 'add_rating_gradient'):\n",
    "        model_V_hat.add_rating_gradient = original_add_rating_gradient\n",
    "    \n",
    "    if original_training_state:\n",
    "        model_V_hat.train()\n",
    "    # print(\"Detailed final LST flow visualization finished.\")\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_level_set_transport_model(p_distribution, f_func, grad_f_func, config, dataset): # Changed dataset to dataset_class\n",
    "    print(\"Starting Level-Set Transport model training...\")\n",
    "\n",
    "    if isinstance(dataset, type):\n",
    "        # Initialize Dataset\n",
    "        train_dataset = dataset( # Use the passed class\n",
    "            distribution=p_distribution,\n",
    "            f_func=f_func,\n",
    "            pool_size=config.get('dataset_pool_size', 30000),               # Configurable\n",
    "            x1_batch_size=config.get('batch_size', 1024),        # Configurable\n",
    "            min_f_diff=config.get('dataset_min_f_diff', 0.0001),             # Configurable\n",
    "            min_norm_diff=config.get('dataset_min_norm_diff', 1e-10),        # Configurable\n",
    "            device=DEVICE, dtype=DTYPE\n",
    "        )\n",
    "        dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0, drop_last=True)\n",
    "    elif isinstance(dataset, tuple):    \n",
    "        train_dataset, dataloader = dataset\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataset class: {type(dataset)}\")\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=config.get('tensorboard_log_dir', f'runs/lst_experiment_{timestamp}'))\n",
    "\n",
    "\n",
    "    model_V_hat = VectorFieldModel2( # Make sure this matches your class structure\n",
    "        f_module(f_func),            # Pass f_module instance if needed by your model\n",
    "        input_dim=3,                 # Assuming model takes 2D xt input\n",
    "        hidden_dims=[config.get('model_hidden_dim', 512)] * 4,\n",
    "        output_dim=2,\n",
    "        activation=torch.nn.ReLU(),\n",
    "        dropout_rate=0.1,\n",
    "        add_rating_gradient=False, # If your model uses this attribute\n",
    "    ).to(DEVICE, DTYPE)\n",
    "\n",
    "    optimizer = optim.AdamW(model_V_hat.parameters(), lr=config['lr'], weight_decay=0.001)\n",
    "\n",
    "    # Plotting setup (loss curve and one dynamic vector field plot)\n",
    "    # fig_dynamic will contain ax_loss and ax_vf\n",
    "    fig_dynamic, (ax_loss, ax_vf) = plt.subplots(2, 1, figsize=(8, 13), gridspec_kw={'height_ratios': [1, 2]})\n",
    "    plt.subplots_adjust(hspace=0.35)\n",
    "    losses_history = []\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(f\"{config.get('loss_fn', 'Huber')} Loss\")\n",
    "    ax_loss.set_title(f\"Training Loss\")\n",
    "    ax_loss.grid(True)\n",
    "    line_loss, = ax_loss.plot([], [], 'b-')\n",
    "    # ax_vf setup will happen before each plot\n",
    "    plt.ioff() # Turn off interactive plotting for the main training figure to avoid multiple windows\n",
    "\n",
    "    global_optimizer_steps = 0\n",
    "    total_training_steps = config['num_epochs'] * len(dataloader)\n",
    "    warmup_steps = config.get('warmup_steps', int(0.1 * total_training_steps))  # Default to 10% of total steps\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    ys = f_func(train_dataset[:100][0].to(DEVICE, DTYPE))\n",
    "    min_y, max_y, delta_y = ys.min(), ys.max(), ys.max()-ys.min()\n",
    "    config['vis_S_final'] = delta_y*0.8\n",
    "    config['vis_trajectory_levelset'] = min_y+delta_y*0.01\n",
    "\n",
    "    num_initial_points_sample = 500\n",
    "    initial_points_candidate = p_distribution.sample((num_initial_points_sample*1000,)).to(DEVICE, DTYPE)\n",
    "    y_pred = (f_func(initial_points_candidate) - config.get('vis_trajectory_levelset',0))**2\n",
    "    sort_indices = torch.argsort(y_pred.view(-1))\n",
    "    initial_points_candidate = initial_points_candidate[sort_indices[:num_initial_points_sample]]\n",
    "    selected_indices = torch.randperm(num_initial_points_sample, device=DEVICE)[:config['vis_num_trajectories_final']]\n",
    "    config['x_initial_trajectories'] = initial_points_candidate[selected_indices]\n",
    "\n",
    "    try:\n",
    "        for epoch in range(config['num_epochs']):\n",
    "            epoch_loss_sum = 0.0\n",
    "            num_optimizer_steps_in_epoch = 0\n",
    "            model_V_hat.train()\n",
    "            data_iterator = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\", leave=False)\n",
    "\n",
    "            for i, (x0_batch, x1_batch, _) in enumerate(data_iterator): # u_hat_target_from_dataset not directly used for loss target here\n",
    "                if not x0_batch.numel(): continue\n",
    "\n",
    "                x0_batch = x0_batch.to(DEVICE, DTYPE)\n",
    "                x1_batch = x1_batch.to(DEVICE, DTYPE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                t_batch = torch.rand(x0_batch.shape[0]*8, 1).to(DEVICE, DTYPE)#*0.8 + 0.1\n",
    "                nstack = lambda x: torch.cat([x]*8, dim=0)\n",
    "                x0_batch = nstack(x0_batch)\n",
    "                x1_batch = nstack(x1_batch)\n",
    "                xt_batch = (1 - t_batch) * x0_batch + t_batch * x1_batch\n",
    "                xt_batch = xt_batch.view(-1, 2)\n",
    "\n",
    "                # Calculate target for w_hat(xt)\n",
    "                sigma_t_batch = x1_batch - x0_batch # Vector from x0 to x1\n",
    "                grad_f_xt = grad_f_func(xt_batch, f_func)\n",
    "                norm_grad_f_xt = torch.linalg.norm(grad_f_xt, dim=-1, keepdim=True) + 1e-9\n",
    "                normalized_grad_f_xt = grad_f_xt / norm_grad_f_xt\n",
    "                \n",
    "                # Target is component of sigma_t_batch orthogonal to normalized_grad_f_xt\n",
    "                w_hat_target_batch = sigma_t_batch - torch.sum(sigma_t_batch * normalized_grad_f_xt, dim=-1, keepdim=True) * normalized_grad_f_xt\n",
    "\n",
    "                w_hat_pred_batch = model_V_hat(xt_batch) # Assuming model takes 2D xt\n",
    "                loss = huber(w_hat_pred_batch, w_hat_target_batch)\n",
    "                loss.backward()\n",
    "\n",
    "                total_grad_norm = 0.0\n",
    "                for p in model_V_hat.parameters():\n",
    "                    if p.grad is not None: total_grad_norm += p.grad.data.norm(2).item() ** 2\n",
    "                total_grad_norm = total_grad_norm ** 0.5\n",
    "                writer.add_scalar('train/grad_norm', total_grad_norm, global_optimizer_steps)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                writer.add_scalar('train/batch_loss', loss.item(), global_optimizer_steps)\n",
    "                writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], global_optimizer_steps)\n",
    "                with torch.no_grad():\n",
    "                    writer.add_scalar('train_details/w_hat_target_norm_mean', w_hat_target_batch.norm(dim=-1).mean().item(), global_optimizer_steps)\n",
    "                    writer.add_scalar('train_details/w_hat_pred_norm_mean', w_hat_pred_batch.norm(dim=-1).mean().item(), global_optimizer_steps)\n",
    "\n",
    "                epoch_loss_sum += loss.item()\n",
    "                num_optimizer_steps_in_epoch += 1\n",
    "                global_optimizer_steps += 1\n",
    "                data_iterator.set_postfix(loss=f\"{loss.item():.4e}\", lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss_sum / num_optimizer_steps_in_epoch if num_optimizer_steps_in_epoch > 0 else 0.0\n",
    "            losses_history.append(avg_epoch_loss)\n",
    "            writer.add_scalar('train/epoch_loss', avg_epoch_loss, epoch)\n",
    "\n",
    "            for name, param in model_V_hat.named_parameters():\n",
    "                writer.add_histogram(f'model_weights/{name}', param.data, epoch)\n",
    "                if param.grad is not None: writer.add_histogram(f'model_grads/{name}', param.grad.data, epoch)\n",
    "\n",
    "            if (epoch + 1) % config.get('print_loss_every_epochs', 1) == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{config['num_epochs']}], Avg Loss: {avg_epoch_loss:.6e}, LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "            # Update and log the combined (loss + vector field) plot to TensorBoard\n",
    "            if (epoch + 1) % config.get('plot_field_every_epochs', 5) == 0 or epoch == config['num_epochs'] - 1:\n",
    "                model_V_hat.eval() # Set model to evaluation mode for plotting\n",
    "                # Update loss curve part of fig_dynamic\n",
    "                line_loss.set_data(range(1, len(losses_history) + 1), losses_history)\n",
    "                ax_loss.relim(); ax_loss.autoscale_view(tight=True)\n",
    "\n",
    "                # Update vector field part (ax_vf) of fig_dynamic\n",
    "                ax_vf.clear()\n",
    "                ax_vf.set_xlabel(\"$x_1$\"); ax_vf.set_ylabel(\"$x_2$\")\n",
    "                ax_vf.set_title(f\"Learned Field $w_{{hat}}(x)$ (Epoch {epoch+1})\")\n",
    "                ax_vf.set_aspect('equal', adjustable='box')\n",
    "                ax_vf.set_xlim(config['vis_grid_extents']); ax_vf.set_ylim(config['vis_grid_extents'])\n",
    "                ax_vf.grid(True, linestyle='--', alpha=0.5)\n",
    "                _plot_current_rescaled_field_and_contours(\n",
    "                    model_V_hat, p_distribution, f_func, grad_f_func, ax_vf,\n",
    "                    config['vis_grid_extents'], config.get('vis_grid_points_dynamic', 15),\n",
    "                    DEVICE, DTYPE, config.get('rescaling_stability_eps', 1e-4)\n",
    "                )\n",
    "                writer.add_figure('training/loss_and_vector_field', fig_dynamic, epoch)\n",
    "                model_V_hat.train() # Set model back to training mode\n",
    "\n",
    "                # Optionally, run the detailed visualization and log its plots to TensorBoard\n",
    "                if config.get('run_visualize_final_lst_flow_during_train', True):\n",
    "                    visualize_final_lst_flow(\n",
    "                        model_V_hat, p_distribution, f_func, grad_f_func,\n",
    "                        grid_extents=config['vis_grid_extents'],\n",
    "                        grid_points=config.get('vis_grid_points_final', 25),\n",
    "                        num_trajectories=config.get('vis_num_trajectories_final', 10),\n",
    "                        S_final=config.get('vis_S_final', 2.0),\n",
    "                        stability_eps=config.get('rescaling_stability_eps', 1e-4),\n",
    "                        writer=writer, current_step=epoch, # Pass writer and epoch\n",
    "                        device=DEVICE, dtype=DTYPE\n",
    "                    )\n",
    "            writer.flush() \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user.\")\n",
    "    finally:\n",
    "        writer.close()\n",
    "        plt.close(fig_dynamic) # Close the main training plot figure\n",
    "        print(\"Training finished or interrupted.\")\n",
    "    return model_V_hat\n",
    "\n",
    "\n",
    "# --- Define p(x), f(x) and grad_f(x) (from user's script) ---\n",
    "mix_probs = torch.tensor([0.8, 0.1,0.1]).to(DEVICE, DTYPE)\n",
    "means = torch.tensor([[-1.5, -1.5], [2, -.2], [-.2,2]]).to(DEVICE, DTYPE)+0.5\n",
    "std_devs = torch.tensor([0.7, 0.3, 0.3]).to(DEVICE, DTYPE)*1.9\n",
    "cov_mats_list = []\n",
    "for std in std_devs:\n",
    "    cov_mats_list.append(torch.eye(2).to(DEVICE, DTYPE) * (std**2) + torch.tensor([[0.0, (std*0.4)*2/3], [(std*0.4)*2/3, -np.random.random()* (std**2)/10]]).to(DEVICE, DTYPE))\n",
    "cov_mats = torch.stack(cov_mats_list)\n",
    "p_dist_param_device = DEVICE\n",
    "mix = torch.distributions.Categorical(mix_probs.to(p_dist_param_device))\n",
    "comp = torch.distributions.MultivariateNormal(means.to(p_dist_param_device), cov_mats.to(p_dist_param_device))\n",
    "p_distribution_base = torch.distributions.MixtureSameFamily(mix, comp)\n",
    "\n",
    "class SkewedDistribution(torch.distributions.Distribution):\n",
    "    arg_constraints = {}\n",
    "    has_rsample = False\n",
    "    def __init__(self, base_distribution, device=DEVICE, dtype=DTYPE): # Added defaults\n",
    "        self.base_distribution = base_distribution\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        super().__init__(batch_shape=base_distribution.batch_shape, event_shape=base_distribution.event_shape)\n",
    "\n",
    "    def log_prob(self, coords_tensor):\n",
    "        skewed_coords = coords_tensor + (coords_tensor @ torch.tensor([[1.],[1.]], device=self.device, dtype=self.dtype)) * torch.tensor([[-.3,.3]], device=self.device, dtype=self.dtype)\n",
    "        return self.base_distribution.log_prob(skewed_coords/torch.tensor([[1.,1.75]], device=self.device, dtype=self.dtype))\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        base_samples = self.base_distribution.sample(sample_shape)\n",
    "        coords = base_samples.clone()\n",
    "        for _ in range(3):\n",
    "            skew_term = (coords @ torch.tensor([[1.],[1.]], device=self.device, dtype=self.dtype)) * torch.tensor([[-.3,.3]], device=self.device, dtype=self.dtype)\n",
    "            skewed_coords = coords + skew_term\n",
    "            correction = base_samples - skewed_coords\n",
    "            coords = coords + 0.5 * correction\n",
    "        return coords*torch.tensor([[1.,1.75]], device=self.device, dtype=self.dtype)\n",
    "\n",
    "skewed_distribution = SkewedDistribution(p_distribution_base, device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "def f_potential_func(coords_tensor): # coords_tensor is (N, 2)\n",
    "    return torch.sum(coords_tensor, dim=-1)*2 + torch.sum(coords_tensor**2, dim=-1) / 300.0\n",
    "\n",
    "def kinked_potential_function(coords_tensor, base_slope=1.0, kink_strength=0.5, kink_hardness=2.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        coords_tensor (torch.Tensor): A tensor of shape (N, 2) representing (x, y) coordinates.\n",
    "        base_slope (float): The baseline slope for the (x+y) component.\n",
    "                            The original f(x,y)=x+y corresponds to base_slope=1, kink_strength=0.\n",
    "        kink_strength (float): Determines the magnitude of the slope variation.\n",
    "                               The slope factor will range roughly from\n",
    "                               (base_slope - kink_strength) to (base_slope + kink_strength).\n",
    "                               Should be positive. To ensure slope factor remains positive,\n",
    "                               keep base_slope > kink_strength if that's desired.\n",
    "        kink_hardness (float): Controls the sharpness of the \"kink\" (transition)\n",
    "                               around the line y = x. Higher values mean a sharper transition.\"\"\"\n",
    "    if not isinstance(coords_tensor, torch.Tensor):\n",
    "        raise TypeError(\"coords_tensor must be a PyTorch tensor.\")\n",
    "    if coords_tensor.ndim != 2 or coords_tensor.shape[-1] != 2:\n",
    "        raise ValueError(\"coords_tensor must have shape (N, 2).\")\n",
    "    if not (isinstance(base_slope, (int, float)) and\n",
    "            isinstance(kink_strength, (int, float)) and\n",
    "            isinstance(kink_hardness, (int, float))):\n",
    "        raise TypeError(\"base_slope, kink_strength, and kink_hardness must be numbers.\")\n",
    "    if kink_strength < 0:\n",
    "        # Warning or error, as negative strength inverts the intended effect\n",
    "        print(\"Warning: kink_strength is negative. The effect on level set spacing might be inverted.\")\n",
    "\n",
    "\n",
    "    x = coords_tensor[:, 0]\n",
    "    y = coords_tensor[:, 1]\n",
    "\n",
    "    x_plus_y = x + y\n",
    "    y_minus_x = y - x\n",
    "\n",
    "    # Scaling factor that depends on (y - x)\n",
    "    # tanh ranges from -1 to 1.\n",
    "    # So, scaling_factor ranges from (base_slope - kink_strength) to (base_slope + kink_strength)\n",
    "    scaling_factor = base_slope + kink_strength * torch.tanh(kink_hardness * y_minus_x)\n",
    "\n",
    "    output_potential = scaling_factor * x_plus_y\n",
    "\n",
    "    return output_potential + torch.sum(coords_tensor**2, dim=-1) / 300.0\n",
    "@torch.enable_grad()\n",
    "def grad_f_function(x_coords, f_func_ref=f_potential_func):\n",
    "    temp_x = x_coords.detach().clone().requires_grad_(True)\n",
    "    f_values = f_func_ref(temp_x)\n",
    "    grad_outputs_tensor = torch.ones_like(f_values, device=f_values.device, dtype=f_values.dtype)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=f_values, inputs=temp_x, grad_outputs=grad_outputs_tensor,\n",
    "        create_graph=False, retain_graph=False, allow_unused=False\n",
    "    )[0]\n",
    "    return gradients\n",
    "\n",
    "# --- Main Execution (Example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure f_module and VectorFieldModel2 are defined here or in a previous cell\n",
    "    # For this example, let's include simple placeholders if they weren't defined earlier\n",
    "    # In your notebook, these would already be defined.\n",
    "    class f_module(torch.nn.Module):\n",
    "        def __init__(self, f_potential_func_ref):\n",
    "            super(f_module, self).__init__()\n",
    "            self.f_potential_func = f_potential_func_ref\n",
    "        def forward(self, x):\n",
    "            return self.f_potential_func(x)\n",
    "\n",
    "\n",
    "    shared_vis_grid_extents = (-5.5, 5.0)\n",
    "    config = {\n",
    "        'dataset_pool_size': 50000,        \n",
    "        'dataset_min_f_diff': 0.001,\n",
    "        'dataset_min_norm_diff': 1e-4,\n",
    "\n",
    "        'batch_size': 512, # Actual training batch size of (x0,x1,u) triples\n",
    "        'num_epochs': 50,\n",
    "        'lr': 2e-3, # Adjusted learning rate\n",
    "        'warmup_steps': 20, # Warmup over these many optimizer steps\n",
    "        'warmup_initial_lr': 1e-6,\n",
    "\n",
    "        'model_hidden_dim': 512, # Reduced for faster example\n",
    "        'print_loss_every_epochs': 10,\n",
    "        'plot_field_every_epochs': 1, # Log combined plot to TB\n",
    "        'run_visualize_final_lst_flow_during_train': True, # Log detailed plots to TB\n",
    "\n",
    "        'vis_grid_extents': shared_vis_grid_extents,\n",
    "        'vis_grid_points_dynamic': 50, # For ax_vf plot\n",
    "        'vis_grid_points_final': 32,   # For visualize_final_lst_flow\n",
    "        'vis_num_trajectories_final': 75,\n",
    "        'vis_trajectory_levelset': 0.0,\n",
    "        'vis_S_final': 2.0,\n",
    "        'rescaling_stability_eps': 1e-3,\n",
    "        # 'tensorboard_log_dir': 'runs/lst_experiment_main',\n",
    "        'tensorboard_hist_freq': 100, # Log histograms to TB every N steps\n",
    "    }\n",
    "\n",
    "    # To run in Jupyter:\n",
    "    # %load_ext tensorboard\n",
    "    # %tensorboard --logdir runs/lst_experiment_main --reload_interval 5\n",
    "\n",
    "\n",
    "    pool_dataset = PoolDataset(\n",
    "        skewed_distribution, \n",
    "        f_potential_func, \n",
    "        pool_size=config['dataset_pool_size'],\n",
    "        device=DEVICE, # Store pool data on the main training device\n",
    "        dtype=DTYPE\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        pool_dataset,\n",
    "        batch_size=config['batch_size'] * 2 +int(config['batch_size'] * .1)*2,\n",
    "        shuffle=True, # Shuffles indices from PoolDataset before fetching\n",
    "        collate_fn=custom_collate,\n",
    "        num_workers=0, # Start with 0 workers for simplicity, then increase if I/O bound\n",
    "        drop_last=True # Ensures collate_fn always gets an even number of samples if dataloader_batch_size is even\n",
    "    )\n",
    "\n",
    "    trained_V_hat_model = train_level_set_transport_model(\n",
    "        skewed_distribution, f_potential_func, grad_f_function,\n",
    "        config,\n",
    "        (pool_dataset, train_dataloader) \n",
    "    )\n",
    "\n",
    "    if trained_V_hat_model:\n",
    "        print(\"Running final visualization after training (will show plots)...\")\n",
    "        # This call will show plots as it's outside the training loop's TB logging context\n",
    "        # unless we pass a new writer or handle it differently.\n",
    "        # For now, let it show plots directly.\n",
    "        visualize_final_lst_flow(\n",
    "            trained_V_hat_model,\n",
    "            skewed_distribution,\n",
    "            f_potential_func, grad_f_function,\n",
    "            grid_extents=shared_vis_grid_extents,\n",
    "            grid_points=30,\n",
    "            num_trajectories=15,\n",
    "            S_final=3.0,\n",
    "            stability_eps=config['rescaling_stability_eps'],\n",
    "            device=DEVICE, dtype=DTYPE\n",
    "        )\n",
    "        # To save the final model:\n",
    "        # torch.save(trained_V_hat_model.state_dict(), \"trained_lst_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard SummaryWriter initialized. Logging to: runs/lst_experiment_main\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manip39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
